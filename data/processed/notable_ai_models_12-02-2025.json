[
  {
    "model": "Claude Opus 4.5",
    "organization": "Anthropic",
    "publicationDate": "2025-11-24",
    "domain": "Language",
    "task": "Code generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://www.anthropic.com/news/claude-opus-4-5",
    "reference": "Introducing Claude Opus 4.5",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-11"
  },
  {
    "model": "Gemini 3 Pro",
    "publicationDate": "2025-11-18",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://blog.google/products/gemini/gemini-3/",
    "reference": "A new era of intelligence with Gemini 3",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "Google TPU v7 Ironwood",
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-11"
  },
  {
    "model": "GPT-5.1",
    "organization": "OpenAI",
    "publicationDate": "2025-11-13",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://openai.com/index/gpt-5-1/",
    "reference": [
      "GPT-5.1: A smarter",
      "more conversational ChatGPT"
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-11"
  },
  {
    "model": "Kimi K2 Thinking",
    "organization": "Moonshot",
    "publicationDate": "2025-11-06",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 1000000000000.0,
    "trainingComputeFlop": 4.2e+24,
    "confidence": "Likely",
    "link": "https://moonshotai.github.io/Kimi-K2/thinking",
    "reference": "Introducing Kimi K2 Thinking",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Kimi K2",
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Moonshot",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Massive (≥1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-11"
  },
  {
    "model": "Tongyi DeepResearch",
    "organization": "Alibaba",
    "publicationDate": "2025-10-28",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 30500000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2510.24701",
    "reference": "Tongyi DeepResearch Technical Report",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Qwen3-30B-A3B",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-10"
  },
  {
    "model": "MiniMax-M2",
    "organization": "MiniMax",
    "publicationDate": "2025-10-27",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 229000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://www.minimax.io/news/minimax-m2",
    "reference": "MiniMax M2 & Agent: Ingenious in Simplicity",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "MiniMax",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-10"
  },
  {
    "model": "Veo 3.1",
    "publicationDate": "2025-10-15",
    "domain": "Video",
    "task": "Image-to-video",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://blog.google/technology/ai/veo-updates-flow/",
    "reference": "Introducing Veo 3.1 and advanced capabilities in Flow",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-10"
  },
  {
    "model": "Claude Haiku 4.5",
    "organization": "Anthropic",
    "publicationDate": "2025-10-15",
    "domain": "Language",
    "task": "Chat",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://www.anthropic.com/news/claude-haiku-4-5",
    "reference": "Introducing Claude Haiku 4.5",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-10"
  },
  {
    "model": "Ling-1T",
    "organization": "Ant Group",
    "publicationDate": "2025-10-10",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 1000000000000.0,
    "trainingComputeFlop": 6.000001e+24,
    "confidence": "Confident",
    "link": "https://huggingface.co/inclusionAI/Ling-1T\nhttps://arxiv.org/pdf/2510.22115",
    "reference": "Ling-1T",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Ant Group",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Massive (≥1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-10"
  },
  {
    "model": "GPT-5 Pro",
    "organization": "OpenAI",
    "publicationDate": "2025-10-07",
    "domain": "Multimodal",
    "task": null,
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://cdn.openai.com/gpt-5-system-card.pdf",
    "reference": "GPT-5 System Card",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-10"
  },
  {
    "model": "Sora 2.0",
    "organization": "OpenAI",
    "publicationDate": "2025-09-30",
    "domain": "Video",
    "task": "Video generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://openai.com/index/sora-2/",
    "reference": "Sora 2 is here",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-09"
  },
  {
    "model": "GLM 4.6",
    "organization": [
      "Zhipu AI",
      "Tsinghua University"
    ],
    "publicationDate": "2025-09-30",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 357000000000.0,
    "trainingComputeFlop": 4.42e+24,
    "confidence": "Likely",
    "link": "https://z.ai/blog/glm-4.6",
    "reference": [
      "GLM-4.6: Advanced Agentic",
      "Reasoning and Coding Capabilities"
    ],
    "citations": null,
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Zhipu AI",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2025,
    "yearMonth": "2025-09"
  },
  {
    "model": "Claude Sonnet 4.5",
    "organization": "Anthropic",
    "publicationDate": "2025-09-29",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://www.anthropic.com/news/claude-sonnet-4-5",
    "reference": "Introducing Claude Sonnet 4.5",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-09"
  },
  {
    "model": "Gemini Robotics-ER 1.5",
    "publicationDate": "2025-09-25",
    "domain": "Vision",
    "task": "Instruction interpretation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://deepmind.google/discover/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/\n\nhttps://storage.googleapis.com/deepmind-media/gemini-robotics/Gemini-Robotics-1-5-Tech-Report.pdf",
    "reference": "Gemini Robotics 1.5 brings AI agents into the physical world",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-09"
  },
  {
    "model": "Qwen3-Omni-30B-A3B",
    "organization": "Alibaba",
    "publicationDate": "2025-09-22",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": 35300000000.0,
    "trainingComputeFlop": 3.6e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2509.17765",
    "reference": "Qwen3-Omni Technical Report",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-09"
  },
  {
    "model": "AgentFounder-30B",
    "organization": "Alibaba",
    "publicationDate": "2025-09-16",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 30000000000.0,
    "trainingComputeFlop": 6.5367e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2509.13310",
    "reference": "Scaling Agents via Continual Pre-training",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Qwen3-30B-A3B",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-09"
  },
  {
    "model": "Qwen3-Max",
    "organization": "Alibaba",
    "publicationDate": "2025-09-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 1000000000000.0,
    "trainingComputeFlop": 1.512e+25,
    "confidence": "Speculative",
    "link": "https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&url=2840914_2&modelId=qwen3-max-preview\n\nhttps://qwen.ai/blog?id=87dc93fc8a590dc718c77e1f6e84c07b474f6c5a&from=home.latest-research-list",
    "reference": [
      "Introducing Qwen3-Max-Preview (Instruct) — our biggest model yet",
      "with over 1 trillion parameters!"
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Massive (≥1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-09"
  },
  {
    "model": "LongCat-Flash",
    "organization": "Meituan Inc",
    "publicationDate": "2025-09-01",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 560000000000.0,
    "trainingComputeFlop": 3.726e+24,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2509.01322",
    "reference": "LongCat-Flash Technical Report",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meituan Inc",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-09"
  },
  {
    "model": "Gemini 2.5 Flash Image (Nano Banana)",
    "organization": "Google",
    "publicationDate": "2025-08-26",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://gemini.google/overview/image-generation/\n\nhttps://aistudio.google.com/models/gemini-2-5-flash-image",
    "reference": "Nano Banana\nImage editing in Gemini just got a major upgrade",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": null,
    "baseModel": "Gemini 2.5 Flash",
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-08"
  },
  {
    "model": "GPT-5 nano",
    "organization": "OpenAI",
    "publicationDate": "2025-08-07",
    "domain": "Multimodal",
    "task": null,
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://cdn.openai.com/gpt-5-system-card.pdf",
    "reference": "GPT-5 System Card",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-08"
  },
  {
    "model": "GPT-5 mini",
    "organization": "OpenAI",
    "publicationDate": "2025-08-07",
    "domain": "Multimodal",
    "task": null,
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://cdn.openai.com/gpt-5-system-card.pdf",
    "reference": "GPT-5 System Card",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-08"
  },
  {
    "model": "GPT-5",
    "organization": "OpenAI",
    "publicationDate": "2025-08-07",
    "domain": "Multimodal",
    "task": null,
    "parameters": null,
    "trainingComputeFlop": 6.6e+25,
    "confidence": "Speculative",
    "link": "https://cdn.openai.com/gpt-5-system-card.pdf\nhttps://openai.com/index/introducing-gpt-5/",
    "reference": "GPT-5 System Card",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-08"
  },
  {
    "model": "gpt-oss-120b",
    "organization": "OpenAI",
    "publicationDate": "2025-08-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 116830000000.0,
    "trainingComputeFlop": 4.94e+24,
    "confidence": "Confident",
    "link": "https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf",
    "reference": "gpt-oss-120b & gpt-oss-20b Model Card",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-08"
  },
  {
    "model": "gpt-oss-20b",
    "organization": "OpenAI",
    "publicationDate": "2025-08-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 20910000000.0,
    "trainingComputeFlop": 5.49e+23,
    "confidence": "Confident",
    "link": "https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf",
    "reference": "gpt-oss-120b & gpt-oss-20b Model Card",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-08"
  },
  {
    "model": "GLM 4.5",
    "organization": [
      "Zhipu AI",
      "Tsinghua University"
    ],
    "publicationDate": "2025-08-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 355000000000.0,
    "trainingComputeFlop": 4.42e+24,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2508.06471",
    "reference": [
      "GLM-4.5: Agentic",
      "Reasoning",
      "and Coding (ARC) Foundation Models"
    ],
    "citations": null,
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Zhipu AI",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2025,
    "yearMonth": "2025-08"
  },
  {
    "model": "Claude Opus 4.1",
    "organization": "Anthropic",
    "publicationDate": "2025-08-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://www.anthropic.com/news/claude-opus-4-1",
    "reference": "Claude Opus 4.1",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-08"
  },
  {
    "model": "Qwen Image",
    "organization": "Alibaba",
    "publicationDate": "2025-08-04",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 27000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/Qwen_Image.pdf",
    "reference": "Qwen-Image Technical Report",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Qwen2.5-VL-7B",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-08"
  },
  {
    "model": "Hierarchical Reasoning Model (HPM)",
    "organization": "Sapient Intelligence",
    "publicationDate": "2025-08-04",
    "domain": "Language",
    "task": "Visual puzzles",
    "parameters": 27000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2506.21734",
    "reference": "Hierarchical Reasoning Model",
    "citations": null,
    "organizationCategorization": null,
    "countryOfOrganization": "Singapore",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Sapient Intelligence",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2025,
    "yearMonth": "2025-08"
  },
  {
    "model": "MindLink-72B",
    "organization": "Kunlun Inc.",
    "publicationDate": "2025-08-01",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 72000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://github.com/SkyworkAI/MindLink/blob/main/mindlink.pdf",
    "reference": "MindLink: Adaptive Multi-step Planning for Large Language Models",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A800 PCIe 80 GB",
    "baseModel": "Qwen2.5-72B",
    "modelAccessibility": null,
    "frontierModel": null,
    "company": "Kunlun Inc.",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Unknown",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-08"
  },
  {
    "model": "Gemini 2.5 Deep Think",
    "publicationDate": "2025-08-01",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Deep-Think-Model-Card.pdf\n\nhttps://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf",
    "reference": "Gemini 2.5 Deep Think - Model Card",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-08"
  },
  {
    "model": "Qwen3-Coder-480B-A35B",
    "organization": "Alibaba",
    "publicationDate": "2025-07-22",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 480000000000.0,
    "trainingComputeFlop": 1.575e+24,
    "confidence": "Confident",
    "link": "https://qwenlm.github.io/blog/qwen3-coder/",
    "reference": "Qwen3-Coder: Agentic Coding in the World",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-07"
  },
  {
    "model": "EXAONE 4.0 (32B)",
    "organization": "LG AI Research",
    "publicationDate": "2025-07-15",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 32000000000.0,
    "trainingComputeFlop": 2.69000000000001e+24,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2507.11407",
    "reference": "EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Korea (Republic of)",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H200 SXM",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "LG AI Research",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-07"
  },
  {
    "model": "Gemini Embedding",
    "publicationDate": "2025-07-14",
    "domain": "Language",
    "task": "Semantic embedding",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/2503.07891",
    "reference": "Gemini Embedding: Generalizable Embeddings from Gemini",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-07"
  },
  {
    "model": "Kimi K2",
    "organization": "Moonshot",
    "publicationDate": "2025-07-11",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 1000000000000.0,
    "trainingComputeFlop": 2.976e+24,
    "confidence": "Confident",
    "link": "https://moonshotai.github.io/Kimi-K2/",
    "reference": "Kimi K2: Open Agentic Intelligence",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H800 SXM5",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Moonshot",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Massive (≥1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-07"
  },
  {
    "model": "Grok 4 Heavy",
    "organization": "xAI",
    "publicationDate": "2025-07-10",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://x.ai/news/grok-4",
    "reference": "Grok 4 Heavy - the most powerful version of Grok 4",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "xAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-07"
  },
  {
    "model": "Grok 4",
    "organization": "xAI",
    "publicationDate": "2025-07-09",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": 5.0000000000001e+26,
    "confidence": "Speculative",
    "link": "https://x.ai/news/grok-4",
    "reference": "Grok 4",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": "True",
    "company": "xAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-07"
  },
  {
    "model": "EXAONE Path 2.0",
    "organization": "LG AI Research",
    "publicationDate": "2025-07-09",
    "domain": "Vision",
    "task": "Cancer diagnosis",
    "parameters": 175000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2507.06639",
    "reference": "EXAONE Path 2.0: Pathology Foundation Model with End-to-End Supervision",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Korea (Republic of)",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "LG AI Research",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-07"
  },
  {
    "model": "dots.llm1",
    "organization": "Rednote",
    "publicationDate": "2025-07-06",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 142000000000.0,
    "trainingComputeFlop": 1.2164856e+24,
    "confidence": "Confident",
    "link": "https://www.arxiv.org/abs/2506.05767",
    "reference": "dots.llm1 Technical Report",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA H800 SXM5",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Rednote",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-07"
  },
  {
    "model": "FGN",
    "publicationDate": "2025-06-12",
    "domain": "Earth science",
    "task": "Weather forecasting",
    "parameters": 720000000.0,
    "trainingComputeFlop": 9.618950880000001e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2506.10772v1",
    "reference": "Skillful joint probabilistic weather forecasting from marginals",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v5p",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-06"
  },
  {
    "model": "Seed-1.6-Thinking",
    "organization": "ByteDance",
    "publicationDate": "2025-06-11",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": 230000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://seed.bytedance.com/en/blog/introduction-to-techniques-used-in-seed1-6",
    "reference": "Introduction to Techniques Used in Seed1.6",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "ByteDance",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-06"
  },
  {
    "model": "Qwen3 Embedding",
    "organization": "Alibaba",
    "publicationDate": "2025-06-05",
    "domain": "Language",
    "task": "Semantic embedding",
    "parameters": 8000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2506.05176v1",
    "reference": "Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Qwen3-8B",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-06"
  },
  {
    "model": "Claude Sonnet 4",
    "organization": "Anthropic",
    "publicationDate": "2025-05-22",
    "domain": "Language",
    "task": "Code generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://www.anthropic.com/claude/sonnet\n\nhttps://www.anthropic.com/news/claude-4",
    "reference": [
      "Hybrid reasoning model with superior intelligence for high-volume use cases",
      "and 200K context window"
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-05"
  },
  {
    "model": "Claude Opus 4",
    "organization": "Anthropic",
    "publicationDate": "2025-05-22",
    "domain": "Language",
    "task": "Code generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://www.anthropic.com/claude/opus\n\nhttps://www.anthropic.com/news/claude-4",
    "reference": [
      "Hybrid reasoning model that pushes the frontier for coding and AI agents",
      "featuring a 200K context window"
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-05"
  },
  {
    "model": "Veo 3",
    "publicationDate": "2025-05-21",
    "domain": "Video",
    "task": "Video generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://deepmind.google/models/veo/",
    "reference": "Our state-of-the-art video generation model",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-05"
  },
  {
    "model": "Imagen 4",
    "organization": "Google",
    "publicationDate": "2025-05-20",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://storage.googleapis.com/deepmind-media/Model-Cards/Imagen-4-Model-Card.pdf",
    "reference": "Imagen 4 Model Card",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-05"
  },
  {
    "model": "Seed1.5-VL",
    "organization": "ByteDance",
    "publicationDate": "2025-05-11",
    "domain": "Vision",
    "task": "Visual question answering",
    "parameters": null,
    "trainingComputeFlop": 1.388556e+24,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2505.07062",
    "reference": "Seed1.5-VL Technical Report",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA H800 SXM5",
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "ByteDance",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-05"
  },
  {
    "model": "Qwen3-235B-A22B",
    "organization": "Alibaba",
    "publicationDate": "2025-04-29",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 235000000000.0,
    "trainingComputeFlop": 4.752e+24,
    "confidence": "Likely",
    "link": "https://qwenlm.github.io/blog/qwen3/",
    "reference": [
      "Qwen3: Think Deeper",
      "Act Faster"
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-04"
  },
  {
    "model": "gpt-image-1",
    "organization": "OpenAI",
    "publicationDate": "2025-04-23",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://openai.com/index/image-generation-api/",
    "reference": "Introducing our latest image generation model in the API",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-04"
  },
  {
    "model": "Pangu Ultra",
    "organization": "Huawei",
    "publicationDate": "2025-04-10",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 135000000000.0,
    "trainingComputeFlop": 1.0692e+25,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2504.07866",
    "reference": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs",
    "citations": "12.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Huawei Ascend 910B",
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "Huawei",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-04"
  },
  {
    "model": "Llama 4 Scout",
    "publicationDate": "2025-04-05",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 109000000000.0,
    "trainingComputeFlop": 4.08e+24,
    "confidence": "Likely",
    "link": "https://ai.meta.com/blog/llama-4-multimodal-intelligence/",
    "reference": "The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-04"
  },
  {
    "model": "Llama 4 Maverick",
    "publicationDate": "2025-04-05",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 400000000000.0,
    "trainingComputeFlop": 2.244000000001e+24,
    "confidence": "Likely",
    "link": "https://ai.meta.com/blog/llama-4-multimodal-intelligence/",
    "reference": "The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-04"
  },
  {
    "model": "Llama 4 Behemoth (preview)",
    "publicationDate": "2025-04-05",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 2000000000000.0,
    "trainingComputeFlop": 5.18400000000001e+25,
    "confidence": "Likely",
    "link": "https://ai.meta.com/blog/llama-4-multimodal-intelligence/",
    "reference": "The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": "True",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Massive (≥1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-04"
  },
  {
    "model": "Gemini 2.5 Pro",
    "publicationDate": "2025-03-25",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking",
    "reference": "Gemini 2.5: Our most intelligent AI model",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-03"
  },
  {
    "model": "Diffusion Renderer",
    "organization": [
      "NVIDIA",
      "University of Toronto",
      "Vector Institute",
      "University of Illinois Urbana-Champaign (UIUC)"
    ],
    "publicationDate": "2025-03-22",
    "domain": "Video",
    "task": "Video editing",
    "parameters": 1100000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2501.18590",
    "reference": "DiffusionRenderer: Neural Inverse and Forward Rendering with Video Diffusion Models",
    "citations": null,
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Canada"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": "Stable Video Diffusion",
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2025,
    "yearMonth": "2025-03"
  },
  {
    "model": "EXAONE Deep 32B",
    "organization": "LG AI Research",
    "publicationDate": "2025-03-16",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 32000000000.0,
    "trainingComputeFlop": 1.26e+24,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2503.12524",
    "reference": "EXAONE Deep: LLMs with Enhanced Reasoning Performance",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Korea (Republic of)",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": "EXAONE 3.5 32B",
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "LG AI Research",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-03"
  },
  {
    "model": "ERNIE-4.5-VL-424B-A47B (文心大模型4.5)",
    "organization": "Baidu",
    "publicationDate": "2025-03-16",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": 424000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://www.datacamp.com/blog/ernie-4-5-x1\n\nhttps://yiyan.baidu.com/blog/publication/ERNIE_Technical_Report.pdf",
    "reference": [
      "Baidu's ERNIE 4.5 & X1: Features",
      "Access",
      "DeepSeek Comparison"
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "ERNIE-4.5-300B-A47B",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Baidu",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-03"
  },
  {
    "model": "Hunyuan-TurboS",
    "organization": "Tencent",
    "publicationDate": "2025-03-11",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 560000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://web.archive.org/web/20250408105622/https://www.dapingtime.com/article/2171.html\n\nhttps://medium.com/data-science-in-your-pocket/tencent-hunyuan-turbo-s-the-fastest-reasoning-llm-d64a02bed5c8\n\nhttps://arxiv.org/abs/2505.15431",
    "reference": [
      "Tencent HunYuan Turbo S: The fastest reasoning LLM\nAt par with DeepSeek",
      "Claude 3.5 and GPT-4o"
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Tencent",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-03"
  },
  {
    "model": "QwQ-32B",
    "organization": "Alibaba",
    "publicationDate": "2025-03-06",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 32500000000.0,
    "trainingComputeFlop": 3.51e+24,
    "confidence": "Speculative",
    "link": "https://qwenlm.github.io/blog/qwq-32b/",
    "reference": "QwQ-32B: Embracing the Power of Reinforcement Learning",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Qwen2.5-Coder (32B)",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-03"
  },
  {
    "model": "Mistral OCR",
    "organization": "Mistral AI",
    "publicationDate": "2025-03-06",
    "domain": "Multimodal",
    "task": "Character recognition (OCR)",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://mistral.ai/news/mistral-ocr",
    "reference": "Mistral OCR",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "France",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Mistral AI",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-03"
  },
  {
    "model": "Mercury",
    "organization": "Inception Labs",
    "publicationDate": "2025-02-27",
    "domain": "Language",
    "task": "Code generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://machine-learning-made-simple.medium.com/is-the-mercury-llm-the-first-of-a-new-generation-of-llms-b64de1d36029\n\nhttps://www.inceptionlabs.ai/news\n\nhttps://www.inceptionlabs.ai/introducing-mercury-our-general-chat-model",
    "reference": "Is the Mercury LLM the first of a new Generation of LLMs?",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Inception Labs",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-02"
  },
  {
    "model": "GPT-4.5",
    "organization": "OpenAI",
    "publicationDate": "2025-02-27",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": 3.8e+26,
    "confidence": "Likely",
    "link": "https://openai.com/index/introducing-gpt-4-5/",
    "reference": "Introducing GPT-4.5",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": "True",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-02"
  },
  {
    "model": "Claude 3.7 Sonnet",
    "organization": "Anthropic",
    "publicationDate": "2025-02-24",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": 3.35e+25,
    "confidence": "Likely",
    "link": "https://www.anthropic.com/news/claude-3-7-sonnet",
    "reference": "Claude 3.7 Sonnet",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-02"
  },
  {
    "model": "Grok 3",
    "organization": "xAI",
    "publicationDate": "2025-02-17",
    "domain": "Language",
    "task": "Chat",
    "parameters": null,
    "trainingComputeFlop": 3.5e+26,
    "confidence": "Likely",
    "link": "https://x.ai/blog/grok-3",
    "reference": "Grok 3 Beta — The Age of Reasoning Agents",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": "True",
    "company": "xAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-02"
  },
  {
    "model": "Eurus-2-7B-PRIME",
    "organization": [
      "Tsinghua University",
      "University of Illinois Urbana-Champaign (UIUC)",
      "Shanghai AI Lab",
      "Peking University",
      "Shanghai Jiao Tong University",
      "CUHK Shenzhen Research Institute"
    ],
    "publicationDate": "2025-02-03",
    "domain": "Mathematics",
    "task": "Mathematical reasoning",
    "parameters": 7000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/2502.01456",
    "reference": "Process Reinforcement through Implicit Rewards",
    "citations": "207.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "China",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Qwen2.5-Math-7B-Base",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2025,
    "yearMonth": "2025-02"
  },
  {
    "model": "o3-mini",
    "organization": "OpenAI",
    "publicationDate": "2025-01-31",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://openai.com/index/openai-o3-mini/",
    "reference": "Pushing the frontier of cost-effective reasoning.",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-01"
  },
  {
    "model": "Computer-Using Agent (CUA)",
    "organization": "OpenAI",
    "publicationDate": "2025-01-23",
    "domain": "Vision",
    "task": "Instruction interpretation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://openai.com/index/computer-using-agent/",
    "reference": [
      "Powering Operator with Computer-Using Agent",
      "a universal interface for AI to interact with the digital world."
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-01"
  },
  {
    "model": "Kimi k1.5",
    "organization": "Moonshot",
    "publicationDate": "2025-01-22",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/2501.12599v1",
    "reference": "Kimi k1.5: Scaling Reinforcement Learning with LLMs",
    "citations": "635.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Moonshot",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-01"
  },
  {
    "model": "Doubao-1.5-pro",
    "organization": "ByteDance",
    "publicationDate": "2025-01-22",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://team.doubao.com/zh/special/doubao_1_5_pro",
    "reference": "Doubao-1.5-pro",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "ByteDance",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-01"
  },
  {
    "model": "Eagle 2",
    "organization": [
      "NVIDIA",
      "Nanjing University",
      "Tsinghua University",
      "Hong Kong Polytechnic University",
      "Johns Hopkins University",
      "New York University (NYU)"
    ],
    "publicationDate": "2025-01-20",
    "domain": "Vision",
    "task": null,
    "parameters": 8930000000.0,
    "trainingComputeFlop": 4.7156e+22,
    "confidence": "Confident",
    "link": "arxiv.org/abs/2501.14818",
    "reference": "Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier Vision-Language Models",
    "citations": "33.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "China",
      "Hong Kong"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": "Qwen2.5-7B",
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2025,
    "yearMonth": "2025-01"
  },
  {
    "model": "DeepSeek-R1",
    "organization": "DeepSeek",
    "publicationDate": "2025-01-20",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 671000000000.0,
    "trainingComputeFlop": 4.020010000000001e+24,
    "confidence": "Confident",
    "link": "https://api-docs.deepseek.com/news/news250120",
    "reference": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": "DeepSeek-V3",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "DeepSeek",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-01"
  },
  {
    "model": "INTELLECT-MATH",
    "organization": "Prime Intellect",
    "publicationDate": "2025-01-17",
    "domain": "Mathematics",
    "task": "Mathematical reasoning",
    "parameters": 7000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://huggingface.co/PrimeIntellect/INTELLECT-MATH\nhttps://www.primeintellect.ai/blog/intellect-math",
    "reference": "INTELLECT-MATH: Frontier Mathematical Reasoning through Better Initializations for Reinforcement Learning",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Qwen2.5-Math-7B-Base",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Prime Intellect",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-01"
  },
  {
    "model": "Stable Point Aware 3D (SPAR3D)",
    "organization": [
      "Stability AI",
      "University of Illinois Urbana-Champaign (UIUC)"
    ],
    "publicationDate": "2025-01-08",
    "domain": "3D modeling",
    "task": "3D reconstruction",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/2501.04689",
    "reference": "SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single Images",
    "citations": null,
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United Kingdom",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Stability AI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2025,
    "yearMonth": "2025-01"
  },
  {
    "model": "STORM-B/8",
    "organization": [
      "University of Southern California",
      "Georgia Institute of Technology",
      "Stanford University",
      "NVIDIA"
    ],
    "publicationDate": "2024-12-31",
    "domain": "3D modeling",
    "task": "3D reconstruction",
    "parameters": 100598707.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2501.00602\nhttps://github.com/NVlabs/GaussianSTORM\nhttps://jiawei-yang.github.io/STORM",
    "reference": "STORM: Spatio-Temporal Reconstruction Model for Large-Scale Outdoor Scenes",
    "citations": "20.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": null,
    "frontierModel": null,
    "company": "University of Southern California",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Unknown",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "DeepSeek-V3",
    "organization": "DeepSeek",
    "publicationDate": "2024-12-24",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 671000000000.0,
    "trainingComputeFlop": 3.4078e+24,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2412.19437",
    "reference": "DeepSeek-V3 Technical Report",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H800 SXM5",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "DeepSeek",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "o3",
    "organization": "OpenAI",
    "publicationDate": "2024-12-20",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://openai.com/index/introducing-o3-and-o4-mini/",
    "reference": [
      "Our most powerful reasoning model with leading performance on coding",
      "math",
      "science",
      "and vision"
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "Veo 2",
    "publicationDate": "2024-12-16",
    "domain": "Video",
    "task": "Video generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://deepmind.google/technologies/veo/veo-2/",
    "reference": "Our state-of-the-art video generation model",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "Apollo 7B",
    "publicationDate": "2024-12-13",
    "domain": "Video",
    "task": "Video description",
    "parameters": 7000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2412.10360",
    "reference": "Apollo: An Exploration of Video Understanding in Large Multimodal Models",
    "citations": null,
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": [
      "Qwen2.5-7B",
      "SigLIP 400M"
    ],
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "Gemini 2.0 Pro",
    "publicationDate": "2024-12-11",
    "domain": "Language",
    "task": "Code generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://deepmind.google/technologies/gemini/pro/",
    "reference": "Our best model yet for coding performance and complex prompts",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "Sora Turbo",
    "organization": "OpenAI",
    "publicationDate": "2024-12-09",
    "domain": "Video",
    "task": "Video generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://openai.com/index/sora-is-here/",
    "reference": "Sora is here",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "EXAONE 3.5 32B",
    "organization": "LG AI Research",
    "publicationDate": "2024-12-09",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 32000000000.0,
    "trainingComputeFlop": 1.25e+24,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2412.04862",
    "reference": "EXAONE 3.5: Series of Large Language Models for Real-world Use Cases",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Korea (Republic of)",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "LG AI Research",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "Llama 3.3 70B",
    "publicationDate": "2024-12-06",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 70000000000.0,
    "trainingComputeFlop": 6.8649768e+24,
    "confidence": "Confident",
    "link": "https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/",
    "reference": "Meta Llama 3.3 multilingual large language model (LLM)",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "o1",
    "organization": "OpenAI",
    "publicationDate": "2024-12-05",
    "domain": "Language",
    "task": "Code generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://openai.com/index/introducing-chatgpt-pro/",
    "reference": "Introducing ChatGPT Pro: Broadening usage of frontier AI.",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "NVILA 15B",
    "organization": [
      "NVIDIA",
      "Massachusetts Institute of Technology (MIT)",
      "University of California (UC) Berkeley",
      "University of California San Diego",
      "University of Washington",
      "Tsinghua University"
    ],
    "publicationDate": "2024-12-05",
    "domain": "Vision",
    "task": "Visual question answering",
    "parameters": 15000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2412.04468",
    "reference": "NVILA: Efficient Frontier Visual Language Models",
    "citations": null,
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "China"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "Infinity",
    "organization": "ByteDance",
    "publicationDate": "2024-12-05",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 2000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2412.04431v1",
    "reference": "Infinity∞: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis",
    "citations": "158.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "ByteDance",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "Amazon Nova Pro",
    "organization": "Amazon",
    "publicationDate": "2024-12-03",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": 6.000010000000001e+24,
    "confidence": "Speculative",
    "link": "https://aws.amazon.com/es/blogs/aws/introducing-amazon-nova-frontier-intelligence-and-industry-leading-price-performance/\n\nhttps://assets.amazon.science/96/7d/0d3e59514abf8fdcfafcdc574300/nova-tech-report-20250317-0810.pdf",
    "reference": "Introducing Amazon Nova foundation models: Frontier intelligence and industry leading price performance",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Amazon Trainium1",
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Amazon",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "Fugatto 1",
    "organization": "NVIDIA",
    "publicationDate": "2024-11-25",
    "domain": "Multimodal",
    "task": "Audio generation",
    "parameters": 2500000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://research.nvidia.com/publication/2024-11_fugatto-1-foundational-generative-audio-transformer-opus-1",
    "reference": "Fugatto 1 - Foundational Generative Audio Transformer Opus 1",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-11"
  },
  {
    "model": "Suno v4",
    "organization": "Suno",
    "publicationDate": "2024-11-19",
    "domain": "Audio",
    "task": "Audio generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://suno.com/blog/v4",
    "reference": "Introducing v4",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "Suno",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-11"
  },
  {
    "model": "Pixtral Large",
    "organization": "Mistral AI",
    "publicationDate": "2024-11-18",
    "domain": "Multimodal",
    "task": "Vision-language generation",
    "parameters": 124000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://mistral.ai/news/pixtral-large/",
    "reference": "Pixtral Large",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "France",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": "Mistral Large 2",
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Mistral AI",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-11"
  },
  {
    "model": "k0-math",
    "organization": "Moonshot",
    "publicationDate": "2024-11-16",
    "domain": "Mathematics",
    "task": "Mathematical reasoning",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://web.archive.org/web/20250124103542/https://www.globaltimes.cn/page/202411/1323248.shtml",
    "reference": "Chinese AI start-up unveils latest reasoning model comparable to OpenAI o1 series",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Moonshot",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-11"
  },
  {
    "model": "Gemini-Exp-1114",
    "publicationDate": "2024-11-15",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://www.tomsguide.com/ai/google-gemini/google-drops-new-gemini-model-and-it-goes-straight-to-the-top-of-the-llm-leaderboard",
    "reference": "Google drops new Gemini model and it goes straight to the top of the LLM leaderboard",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-11"
  },
  {
    "model": "SeedEdit",
    "organization": "ByteDance",
    "publicationDate": "2024-11-11",
    "domain": "Vision",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://lf3-static.bytednsdoc.com/obj/eden-cn/lapzild-tss/ljhwZthlaukjlkulzlp/SeedEdit.pdf\nhttps://team.doubao.com/en/special/seededit",
    "reference": "SeedEdit: Align Image Re-Generation to Image Editing",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "ByteDance",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-11"
  },
  {
    "model": "Hunyuan-Large",
    "organization": "Tencent",
    "publicationDate": "2024-11-06",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 389000000000.0,
    "trainingComputeFlop": 3.49237e+24,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2411.02265",
    "reference": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Tencent",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-11"
  },
  {
    "model": "Doubao-pro",
    "organization": "ByteDance",
    "publicationDate": "2024-10-28",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 500000000000.0,
    "trainingComputeFlop": 2.505e+25,
    "confidence": "Likely",
    "link": "https://www.volcengine.com/docs/6360/1264663",
    "reference": "Doubao General Model Pro (Doubao-pro)",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "ByteDance",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-10"
  },
  {
    "model": "NVLM-X 72B",
    "organization": "NVIDIA",
    "publicationDate": "2024-10-22",
    "domain": "Vision",
    "task": "Language modeling/generation",
    "parameters": 72000000000.0,
    "trainingComputeFlop": 3.0398181e+24,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2409.11402",
    "reference": "NVLM: Open Frontier-Class Multimodal LLMs",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": [
      "Qwen2-72B",
      "InternViT-6B"
    ],
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-10"
  },
  {
    "model": "NVLM-H 72B",
    "organization": "NVIDIA",
    "publicationDate": "2024-10-22",
    "domain": "Vision",
    "task": "Language modeling/generation",
    "parameters": 72000000000.0,
    "trainingComputeFlop": 3.02e+24,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2409.11402",
    "reference": "NVLM: Open Frontier-Class Multimodal LLMs",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": [
      "Qwen2-72B",
      "InternViT-6B"
    ],
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-10"
  },
  {
    "model": "NVLM-D 72B",
    "organization": "NVIDIA",
    "publicationDate": "2024-10-22",
    "domain": "Vision",
    "task": "Language modeling/generation",
    "parameters": 72000000000.0,
    "trainingComputeFlop": 3.02e+24,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2409.11402",
    "reference": "NVLM: Open Frontier-Class Multimodal LLMs",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": [
      "Qwen2-72B",
      "InternViT-6B"
    ],
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-10"
  },
  {
    "model": "Yi-Lightning",
    "organization": "01.AI",
    "publicationDate": "2024-10-18",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": 1.5e+24,
    "confidence": "Confident",
    "link": "https://www.lingyiwanwu.com/en https://platform.lingyiwanwu.com/",
    "reference": "Yi-Lightning",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "01.AI",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-10"
  },
  {
    "model": "CHAI-1",
    "organization": "Chai discovery",
    "publicationDate": "2024-10-15",
    "domain": "Biology",
    "task": "Protein folding prediction",
    "parameters": null,
    "trainingComputeFlop": 7.7605724e+21,
    "confidence": "Confident",
    "link": "https://www.chaidiscovery.com/blog/introducing-chai-1\nhttps://www.biorxiv.org/content/10.1101/2024.10.10.615955v2",
    "reference": "Introducing Chai-1: Decoding the molecular interactions of life",
    "citations": "0.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Chai discovery",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-10"
  },
  {
    "model": "RDT-1B",
    "organization": "Tsinghua University",
    "publicationDate": "2024-10-10",
    "domain": "Robotics",
    "task": "Robotic manipulation",
    "parameters": 1200000000.0,
    "trainingComputeFlop": 4.06e+22,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2410.07864",
    "reference": "RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation",
    "citations": "298.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA H100 PCIe",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2024,
    "yearMonth": "2024-10"
  },
  {
    "model": "Palmyra X 004",
    "organization": "Writer",
    "publicationDate": "2024-10-09",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 150000000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "link": "https://writer.com/engineering/actions-with-palmyra-x-004/",
    "reference": "Introducing actions with Palmyra X 004",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Writer",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-10"
  },
  {
    "model": "GR-2",
    "organization": "ByteDance",
    "publicationDate": "2024-10-08",
    "domain": "Robotics",
    "task": "Video",
    "parameters": 230000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2410.06158v1",
    "reference": "GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation",
    "citations": "149.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "ByteDance",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-10"
  },
  {
    "model": "Movie Gen Video",
    "publicationDate": "2024-10-04",
    "domain": "Video",
    "task": "Video generation",
    "parameters": 30000000000.0,
    "trainingComputeFlop": 1.65e+24,
    "confidence": "Confident",
    "link": "https://ai.meta.com/static-resource/movie-gen-research-paper",
    "reference": "Movie Gen: A Cast of Media Foundation Models",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-10"
  },
  {
    "model": "PixelDance",
    "organization": "ByteDance",
    "publicationDate": "2024-09-24",
    "domain": "Video",
    "task": "Video generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://pixeldance.io/ (LINK BROKEN)",
    "reference": "PixelDance AI - Leading AI Video Generation Platform",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "ByteDance",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "Llama 3.2 11B",
    "publicationDate": "2024-09-24",
    "domain": "Multimodal",
    "task": "Visual question answering",
    "parameters": 10600000000.0,
    "trainingComputeFlop": 5.79e+23,
    "confidence": "Confident",
    "link": "https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/",
    "reference": [
      "Llama 3.2: Revolutionizing edge AI and vision with open",
      "customizable models"
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": "Llama 3.1-8B",
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "Telechat2-115B",
    "organization": "China Telecom",
    "publicationDate": "2024-09-20",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 115000000000.0,
    "trainingComputeFlop": 6.9e+24,
    "confidence": "Confident",
    "link": "https://huggingface.co/Tele-AI/TeleChat2-115B",
    "reference": "TeleChat Technical Report",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "China Telecom",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "Qwen2.5-72B",
    "organization": "Alibaba",
    "publicationDate": "2024-09-19",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 72700000000.0,
    "trainingComputeFlop": 7.8e+24,
    "confidence": "Confident",
    "link": "https://qwenlm.github.io/blog/qwen2.5/",
    "reference": "Qwen2.5: A Party of Foundation Models!",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "Qwen2.5 Instruct (72B)",
    "organization": "Alibaba",
    "publicationDate": "2024-09-19",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 72700000000.0,
    "trainingComputeFlop": 7.8516e+24,
    "confidence": "Confident",
    "link": "https://qwenlm.github.io/blog/qwen2.5/",
    "reference": "Qwen2.5: A Party of Foundation Models!",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": "Qwen2.5-72B",
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "Oryx 34B",
    "organization": [
      "Tsinghua University",
      "Tencent",
      "Nanyang Technological University"
    ],
    "publicationDate": "2024-09-19",
    "domain": "Multimodal",
    "task": "Visual question answering",
    "parameters": 34000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2409.12961v1",
    "reference": "Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary Resolution",
    "citations": "121.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "China",
      "Singapore"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A800 SXM",
    "baseModel": "Yi-1.5-34B",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "Qwen2.5-32B",
    "organization": "Alibaba",
    "publicationDate": "2024-09-17",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 32500000000.0,
    "trainingComputeFlop": 3.51e+24,
    "confidence": "Confident",
    "link": "https://qwenlm.github.io/blog/qwen2.5/",
    "reference": "Qwen2.5: A Party of Foundation Models!",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "o1-preview",
    "organization": "OpenAI",
    "publicationDate": "2024-09-12",
    "domain": "Language",
    "task": "Code generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://openai.com/index/introducing-openai-o1-preview/",
    "reference": "A new series of reasoning models for solving hard problems.",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "o1-mini",
    "organization": "OpenAI",
    "publicationDate": "2024-09-12",
    "domain": "Language",
    "task": "Code generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://openai.com/index/learning-to-reason-with-llms/",
    "reference": "Learning to reason with LLMs",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "DeepSeek-V2.5",
    "organization": "DeepSeek",
    "publicationDate": "2024-09-06",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 236000000000.0,
    "trainingComputeFlop": 1.7892e+24,
    "confidence": "Confident",
    "link": "https://huggingface.co/deepseek-ai/DeepSeek-V2.5",
    "reference": "DeepSeek-V2.5",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "DeepSeek",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "Hunyuan Turbo",
    "organization": "Tencent",
    "publicationDate": "2024-09-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://zhidx.com/p/442250.html",
    "reference": "腾讯版“GPT-4o”来了！混元Turbo首发并上线，效率翻倍价格砍半",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Tencent",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "Harrison.rad.1",
    "organization": "Harrison.ai",
    "publicationDate": "2024-09-05",
    "domain": "Vision",
    "task": "Visual question answering",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://harrison.ai/news/harrison-ai-launches-world-leading-ai-model-to-transform-healthcare/",
    "reference": "Harrison.ai launches world-leading AI model to transform healthcare",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Australia",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Harrison.ai",
    "department": null,
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "AlphaProteo",
    "publicationDate": "2024-09-05",
    "domain": "Biology",
    "task": "Protein generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaproteo-generates-novel-proteins-for-biology-and-health-research/AlphaProteo2024.pdf",
    "reference": "De novo design of high-affinity protein binders with AlphaProteo",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "Hairuo",
    "organization": "Inspur",
    "publicationDate": "2024-08-29",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://encloud.inspur.com/hairuo/cp/index.html\n\nhttps://www.eguizhou.gov.cn/guiyang/2024-08/29/c_1016827.htm",
    "reference": null,
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "Inspur",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-08"
  },
  {
    "model": "GLM-4-Plus",
    "organization": "Zhipu AI",
    "publicationDate": "2024-08-29",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "link": "https://bigmodel.cn/dev/howuse/glm-4",
    "reference": "GLM-4-Plus",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": "True",
    "company": "Zhipu AI",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-08"
  },
  {
    "model": "Jamba 1.5-Large",
    "organization": "AI21 Labs",
    "publicationDate": "2024-08-22",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 398000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2408.12570\nhttps://www.ai21.com/blog/announcing-jamba-model-family\nhttps://huggingface.co/ai21labs/AI21-Jamba-1.5-Large",
    "reference": "Jamba-1.5: Hybrid Transformer-Mamba Models at Scale",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Israel",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "AI21 Labs",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-08"
  },
  {
    "model": "Grok-2",
    "organization": "xAI",
    "publicationDate": "2024-08-13",
    "domain": "Language",
    "task": "Chat",
    "parameters": null,
    "trainingComputeFlop": 2.96e+25,
    "confidence": "Confident",
    "link": "https://x.ai/blog/grok-2",
    "reference": "Grok-2 Beta Release",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": "True",
    "company": "xAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-08"
  },
  {
    "model": "Table Tennis Agent",
    "publicationDate": "2024-08-07",
    "domain": "Robotics",
    "task": "Sports",
    "parameters": 185000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "link": "https://deepmind.google/research/publications/107741/\nhttps://arxiv.org/abs/2408.03906",
    "reference": "Achieving Human Level Competitive Robot Table Tennis",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-08"
  },
  {
    "model": "LLaVA-OV-72B",
    "organization": [
      "ByteDance",
      "Nanyang Technological University",
      "Chinese University of Hong Kong (CUHK)",
      "Hong Kong University of Science and Technology (HKUST)"
    ],
    "publicationDate": "2024-08-06",
    "domain": "Multimodal",
    "task": "Image captioning",
    "parameters": 72000000000.0,
    "trainingComputeFlop": 3.036551985824e+24,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2408.03326",
    "reference": "LLaVA-OneVision: Easy Visual Task Transfer",
    "citations": "1558.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "China",
      "Singapore",
      "Hong Kong"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Qwen2-72B",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "ByteDance",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-08"
  },
  {
    "model": "AFM-server",
    "organization": "Apple",
    "publicationDate": "2024-07-29",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": 4.3e+24,
    "confidence": "Likely",
    "link": "https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models",
    "reference": "Apple Intelligence Foundation Language Models",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "Apple",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-07"
  },
  {
    "model": "AFM-on-device",
    "organization": "Apple",
    "publicationDate": "2024-07-29",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 2730000000.0,
    "trainingComputeFlop": 4.5126e+23,
    "confidence": "Confident",
    "link": "https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models",
    "reference": "Apple Intelligence Foundation Language Models",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "Google TPU v5p",
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "Apple",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-07"
  },
  {
    "model": "Mistral Large 2",
    "organization": "Mistral AI",
    "publicationDate": "2024-07-24",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 123000000000.0,
    "trainingComputeFlop": 2.13e+25,
    "confidence": "Likely",
    "link": "https://mistral.ai/news/mistral-large-2407/",
    "reference": [
      "Top-tier reasoning for high-complexity tasks",
      "for your most sophisticated needs."
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "France",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Mistral AI",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-07"
  },
  {
    "model": "Llama 3.1-405B",
    "publicationDate": "2024-07-23",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 405000000000.0,
    "trainingComputeFlop": 3.8e+25,
    "confidence": "Confident",
    "link": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/",
    "reference": "The Llama 3 Herd of Models",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": "True",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-07"
  },
  {
    "model": "GPT-4o mini",
    "organization": "OpenAI",
    "publicationDate": "2024-07-18",
    "domain": "Language",
    "task": "Chat",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "link": "https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/",
    "reference": "GPT-4o mini: advancing cost-efficient intelligence",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-07"
  },
  {
    "model": "Mathstral",
    "organization": "Mistral AI",
    "publicationDate": "2024-07-16",
    "domain": "Mathematics",
    "task": "Mathematical reasoning",
    "parameters": 7000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://huggingface.co/mistralai/Mathstral-7B-v0.1\nhttps://mistral.ai/news/mathstral",
    "reference": "Mathstral-7B-v0.1",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "France",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Mistral 7B",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Mistral AI",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-07"
  },
  {
    "model": "DeepL LLM",
    "organization": "DeepL",
    "publicationDate": "2024-07-16",
    "domain": "Language",
    "task": "Translation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://www.deepl.com/en/blog/next-gen-language-model",
    "reference": [
      "DeepL's next-gen LLM outperforms ChatGPT-4",
      "Google",
      "and Microsoft for translation quality"
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Germany",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "DeepL",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-07"
  },
  {
    "model": "SenseChat 5.5",
    "organization": "SenseTime",
    "publicationDate": "2024-07-06",
    "domain": "Multimodal",
    "task": "Vision-language generation",
    "parameters": 600000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://www.sensetime.com/en/news-detail/51168278?categoryId=1072",
    "reference": "SenseTime Unveils SenseNova 5.5 - a Complete and Comprehensive Upgrade",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Hong Kong",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "SenseTime",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-07"
  },
  {
    "model": "Ernie 4.0 Turbo",
    "organization": "Baidu",
    "publicationDate": "2024-06-28",
    "domain": "Multimodal",
    "task": "Vision-language generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://www.reuters.com/technology/artificial-intelligence/baidu-launches-upgraded-ai-model-says-user-base-hits-300-mln-2024-06-28/",
    "reference": [
      "Baidu launches upgraded AI model",
      "says Ernie Bot hits 300 mln users"
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Baidu",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-06"
  },
  {
    "model": "ESM3 (98B)",
    "organization": [
      "EvolutionaryScale",
      "University of California (UC) Berkeley"
    ],
    "publicationDate": "2024-06-25",
    "domain": "Biology",
    "task": "Protein generation",
    "parameters": 98500000000.0,
    "trainingComputeFlop": 1.07e+24,
    "confidence": "Confident",
    "link": "https://www.evolutionaryscale.ai/blog/esm3-release",
    "reference": "ESM3: Simulating 500 million years of evolution with a language model",
    "citations": null,
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "EvolutionaryScale",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-06"
  },
  {
    "model": "Cambrian-1-34B",
    "organization": "New York University (NYU)",
    "publicationDate": "2024-06-24",
    "domain": "Multimodal",
    "task": "Image captioning",
    "parameters": 34000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2406.16860",
    "reference": [
      "Cambrian-1: A Fully Open",
      "Vision-Centric Exploration of Multimodal LLMs"
    ],
    "citations": "577.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": "Yi-34B",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "New York University (NYU)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Public",
    "year": 2024,
    "yearMonth": "2024-06"
  },
  {
    "model": "Claude 3.5 Sonnet",
    "organization": "Anthropic",
    "publicationDate": "2024-06-20",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": null,
    "trainingComputeFlop": 2.700000000000001e+25,
    "confidence": "Speculative",
    "link": "https://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_Claude_3_Addendum.pdf",
    "reference": "Claude 3.5 Sonnet",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": "True",
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-06"
  },
  {
    "model": "DeepSeek-Coder-V2 236B",
    "organization": "DeepSeek",
    "publicationDate": "2024-06-17",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 236000000000.0,
    "trainingComputeFlop": 1.2852e+24,
    "confidence": "Confident",
    "link": "https://github.com/deepseek-ai/DeepSeek-Coder-V2",
    "reference": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "DeepSeek-V2 (MoE-236B)",
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "DeepSeek",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-06"
  },
  {
    "model": "Nemotron-4 340B",
    "organization": "NVIDIA",
    "publicationDate": "2024-06-14",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 340000000000.0,
    "trainingComputeFlop": 1.8e+25,
    "confidence": "Confident",
    "link": "https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/",
    "reference": "NVIDIA Releases Open Synthetic Data Generation Pipeline for Training Large Language Models",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": "True",
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-06"
  },
  {
    "model": "OpenVLA",
    "publicationDate": "2024-06-13",
    "domain": "Robotics",
    "task": "Robotic manipulation",
    "parameters": 7188100000.0,
    "trainingComputeFlop": 1.1e+23,
    "confidence": "Confident",
    "link": "https://openvla.github.io/\nhttps://arxiv.org/abs/2406.09246",
    "reference": "OpenVLA: An Open-Source Vision-Language-Action Mode",
    "citations": "1165.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": "Llama 2-7B",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Stanford University",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-06"
  },
  {
    "model": "Llama-3.1-Nemotron-70B-Instruct",
    "publicationDate": "2024-06-12",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": 7.929e+24,
    "confidence": "Confident",
    "link": "https://catalog.ngc.nvidia.com/orgs/nim/teams/nvidia/containers/llama-3.1-nemotron-70b-instruct",
    "reference": "https://www.semanticscholar.org/paper/HelpSteer2%3A-Open-source-dataset-for-training-reward-Wang-Dong/f590d8926dd12345a3bd22253461850f5ca4b3ed",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Llama 3.1-70B",
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "NVIDIA",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-06"
  },
  {
    "model": "Qwen2-72B",
    "organization": "Alibaba",
    "publicationDate": "2024-06-07",
    "domain": "Language",
    "task": "Chat",
    "parameters": 72710000000.0,
    "trainingComputeFlop": 3.02e+24,
    "confidence": "Confident",
    "link": "https://qwenlm.github.io/blog/qwen2/ \nhttps://arxiv.org/abs/2407.10671",
    "reference": "Hello Qwen2",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-06"
  },
  {
    "model": "ALLaM adapted 70B",
    "organization": "Saudi Data and Artificial Intelligence Authority",
    "publicationDate": "2024-05-21",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 70000000000.0,
    "trainingComputeFlop": 1.062e+24,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2407.15390",
    "reference": "ALLaM: Large Language Models for Arabic and English",
    "citations": "39.0",
    "organizationCategorization": [
      "Industry",
      "Government"
    ],
    "countryOfOrganization": "Saudi Arabia",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": "Llama 2-70B",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Saudi Data and Artificial Intelligence Authority",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-05"
  },
  {
    "model": "ALLaM 7B",
    "organization": "Saudi Data and Artificial Intelligence Authority",
    "publicationDate": "2024-05-21",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 7000000000.0,
    "trainingComputeFlop": 9.04e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2407.15390",
    "reference": "ALLaM: Large Language Models for Arabic and English",
    "citations": "39.0",
    "organizationCategorization": [
      "Industry",
      "Government"
    ],
    "countryOfOrganization": "Saudi Arabia",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": "LLaMA-7B",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Saudi Data and Artificial Intelligence Authority",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-05"
  },
  {
    "model": "Octo-Base",
    "publicationDate": "2024-05-20",
    "domain": "Robotics",
    "task": "Robotic manipulation",
    "parameters": 93000000.0,
    "trainingComputeFlop": 5.85e+20,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2405.12213",
    "reference": "Octo: An Open-Source Generalist Robot Policy",
    "citations": "781.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "University of California (UC) Berkeley",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-05"
  },
  {
    "model": "GLM-4 (0520)",
    "organization": "Zhipu AI",
    "publicationDate": "2024-05-20",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2406.12793",
    "reference": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": "GLM-4 (0116)",
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Zhipu AI",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-05"
  },
  {
    "model": "Yi-Large",
    "organization": "01.AI",
    "publicationDate": "2024-05-13",
    "domain": "Language",
    "task": "Chat",
    "parameters": 100000000000.0,
    "trainingComputeFlop": 1.8e+24,
    "confidence": "Speculative",
    "link": null,
    "reference": null,
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "01.AI",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-05"
  },
  {
    "model": "GPT-4o",
    "organization": "OpenAI",
    "publicationDate": "2024-05-13",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "link": "https://openai.com/index/hello-gpt-4o/ \nhttps://openai.com/index/gpt-4o-system-card/",
    "reference": "Hello GPT-4o",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": "True",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-05"
  },
  {
    "model": "AlphaFold 3",
    "publicationDate": "2024-05-08",
    "domain": "Biology",
    "task": "Protein folding prediction",
    "parameters": null,
    "trainingComputeFlop": 4.1405645e+22,
    "confidence": "Confident",
    "link": "https://www.nature.com/articles/s41586-024-07487-w",
    "reference": "Accurate structure prediction of biomolecular interactions with AlphaFold 3",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": [
      "United States",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-05"
  },
  {
    "model": "VILA1.5-13B",
    "organization": [
      "NVIDIA",
      "Massachusetts Institute of Technology (MIT)"
    ],
    "publicationDate": "2024-05-03",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 13493916736.0,
    "trainingComputeFlop": 2.3003136e+21,
    "confidence": "Confident",
    "link": "https://huggingface.co/Efficient-Large-Model/VILA1.5-13b\nhttps://github.com/NVlabs/VILA/tree/bbc609baf326b1b49b93450b48edc516db3737fc/scripts/v1_5/release/13b\nhttps://developer.nvidia.com/blog/visual-language-models-on-nvidia-hardware-with-vila/",
    "reference": "VILA: On Pre-training for Visual Language Models",
    "citations": "622.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": [
      "SigLIP 400M",
      "Vicuna-13B-v1.5"
    ],
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-05"
  },
  {
    "model": "GenCast",
    "publicationDate": "2024-05-01",
    "domain": "Earth science",
    "task": "Weather forecasting",
    "parameters": null,
    "trainingComputeFlop": 8.169984e+20,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2312.15796",
    "reference": "GenCast: Diffusion-based ensemble forecasting for medium-range weather",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v5e",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-05"
  },
  {
    "model": "Llama 3-70B",
    "publicationDate": "2024-04-18",
    "domain": "Language",
    "task": "Chat",
    "parameters": 70000000000.0,
    "trainingComputeFlop": 7.861e+24,
    "confidence": "Confident",
    "link": "https://ai.meta.com/blog/meta-llama-3/",
    "reference": "Introducing Meta Llama 3: The most capable openly available LLM to date",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-04"
  },
  {
    "model": "Reka Core",
    "organization": "Reka AI",
    "publicationDate": "2024-04-15",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 67000000000.0,
    "trainingComputeFlop": 8.400010000000001e+24,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/2404.12387",
    "reference": [
      "Reka Core",
      "Flash",
      "and Edge: A Series of Powerful\nMultimodal Language Models"
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Reka AI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-04"
  },
  {
    "model": "ReALM",
    "organization": "Apple",
    "publicationDate": "2024-03-29",
    "domain": "Language",
    "task": "Named entity recognition (NER)",
    "parameters": 3000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2403.20329",
    "reference": "ReALM: Reference Resolution As Language Modeling",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Flan-T5 11B",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Apple",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-03"
  },
  {
    "model": "DBRX",
    "organization": "Databricks",
    "publicationDate": "2024-03-27",
    "domain": "Language",
    "task": "Chat",
    "parameters": 132000000000.0,
    "trainingComputeFlop": 2.6e+24,
    "confidence": "Confident",
    "link": "https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm",
    "reference": "Introducing DBRX: A New State-of-the-Art Open LLM",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Databricks",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-03"
  },
  {
    "model": "MM1-30B",
    "organization": "Apple",
    "publicationDate": "2024-03-14",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 30000000000.0,
    "trainingComputeFlop": 4.86e+23,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2403.09611",
    "reference": [
      "MM1: Methods",
      "Analysis & Insights from Multimodal LLM Pre-training"
    ],
    "citations": "240.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Apple",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-03"
  },
  {
    "model": "ManiGaussian",
    "organization": [
      "Tsinghua University",
      "Nanyang Technological University",
      "Carnegie Mellon University (CMU)"
    ],
    "publicationDate": "2024-03-13",
    "domain": "Robotics",
    "task": "Robotic manipulation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2403.08321\nhttps://guanxinglu.github.io/ManiGaussian/",
    "reference": "ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation",
    "citations": "100.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "China",
      "Singapore",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA GeForce RTX 4090",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2024,
    "yearMonth": "2024-03"
  },
  {
    "model": "Inflection-2.5",
    "organization": "Inflection AI",
    "publicationDate": "2024-03-07",
    "domain": "Language",
    "task": "Chat",
    "parameters": null,
    "trainingComputeFlop": 8.000001e+24,
    "confidence": "Likely",
    "link": "https://inflection.ai/inflection-2-5",
    "reference": "Inflection-2.5: meet the world's best personal AI",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "Inflection AI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-03"
  },
  {
    "model": "Claude 3 Sonnet",
    "organization": "Anthropic",
    "publicationDate": "2024-03-04",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf",
    "reference": [
      "The Claude 3 Model Family: Opus",
      "Sonnet",
      "Haiku"
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-03"
  },
  {
    "model": "Claude 3 Opus",
    "organization": "Anthropic",
    "publicationDate": "2024-03-04",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "link": "https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf",
    "reference": [
      "The Claude 3 Model Family: Opus",
      "Sonnet",
      "Haiku"
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": "True",
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-03"
  },
  {
    "model": "Aramco Metabrain AI",
    "organization": "Saudi Aramco",
    "publicationDate": "2024-03-04",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 250000000000.0,
    "trainingComputeFlop": 1.05e+25,
    "confidence": "Likely",
    "link": "https://www.offshore-technology.com/news/saudi-aramco-unveils-industry-first-generative-ai-model/",
    "reference": "Saudi Aramco unveils industry’s first generative AI model",
    "citations": null,
    "organizationCategorization": [
      "Industry",
      "Government"
    ],
    "countryOfOrganization": "Saudi Arabia",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Saudi Aramco",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-03"
  },
  {
    "model": "Mistral Large",
    "organization": "Mistral AI",
    "publicationDate": "2024-02-26",
    "domain": "Language",
    "task": "Chat",
    "parameters": null,
    "trainingComputeFlop": 1.12e+25,
    "confidence": "Likely",
    "link": "https://mistral.ai/news/mistral-large/",
    "reference": [
      "Mistral Large",
      "our new flagship model"
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "France",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Mistral AI",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-02"
  },
  {
    "model": "MegaScale (Production)",
    "organization": [
      "ByteDance",
      "Peking University"
    ],
    "publicationDate": "2024-02-23",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 530000000000.0,
    "trainingComputeFlop": 3.9e+24,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/2402.15627",
    "reference": [
      "MegaScale: Scaling Large Language Model Training to More Than 10",
      "000 GPUs"
    ],
    "citations": "225.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "ByteDance",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-02"
  },
  {
    "model": "Stable Diffusion 3",
    "organization": "Stability AI",
    "publicationDate": "2024-02-22",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 8000000000.0,
    "trainingComputeFlop": 5.0000000000000004e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2403.03206\nhttps://stability.ai/news/stable-diffusion-3",
    "reference": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United Kingdom",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Stability AI",
    "department": null,
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-02"
  },
  {
    "model": "Sora",
    "organization": "OpenAI",
    "publicationDate": "2024-02-15",
    "domain": "Video",
    "task": "Video generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://openai.com/index/video-generation-models-as-world-simulators/",
    "reference": "Video generation models as world simulators",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-02"
  },
  {
    "model": "Gemini 1.5 Pro",
    "publicationDate": "2024-02-15",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "link": "https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf",
    "reference": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": "True",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-02"
  },
  {
    "model": "Aya",
    "organization": [
      "Cohere for AI",
      "Brown University",
      "Cohere",
      "Carnegie Mellon University (CMU)",
      "Massachusetts Institute of Technology (MIT)"
    ],
    "publicationDate": "2024-02-12",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 13000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/2402.07827",
    "reference": "Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model",
    "citations": "305.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "Canada",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": "mT5-XXL",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Cohere for AI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-02"
  },
  {
    "model": "Qwen1.5-72B",
    "organization": "Alibaba",
    "publicationDate": "2024-02-04",
    "domain": "Language",
    "task": "Chat",
    "parameters": 72000000000.0,
    "trainingComputeFlop": 1.3e+24,
    "confidence": "Confident",
    "link": "https://qwenlm.github.io/blog/qwen1.5/",
    "reference": "Introducing Qwen1.5",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-02"
  },
  {
    "model": "Qwen-VL-Max",
    "organization": "Alibaba",
    "publicationDate": "2024-01-25",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 7000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://qwenlm.github.io/blog/qwen-vl/",
    "reference": "Introducing Qwen-VL",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-01"
  },
  {
    "model": "AlphaGeometry",
    "publicationDate": "2024-01-17",
    "domain": "Mathematics",
    "task": "Geometry",
    "parameters": 151000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://www.nature.com/articles/s41586-023-06747-5",
    "reference": "Solving olympiad geometry without human demonstrations",
    "citations": "546.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-01"
  },
  {
    "model": "Palmyra X 003",
    "organization": "Writer",
    "publicationDate": "2024-01-01",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 72000000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "link": "https://writer.com/llms/palmyra-x-003/",
    "reference": "Palmyra X 003 Instruct",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Writer",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-01"
  },
  {
    "model": "Kimi Explorer",
    "organization": "Moonshot",
    "publicationDate": "2024-01-01",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://www.53ai.com/news/LargeLanguageModel/2024101137012.html",
    "reference": null,
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "Moonshot",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-01"
  },
  {
    "model": "CoRe",
    "organization": "Tsinghua University",
    "publicationDate": "2023-12-29",
    "domain": "Mathematics",
    "task": "Quantitative reasoning",
    "parameters": 12400000000.0,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/2210.16257",
    "reference": "Solving Math Word Problems via Cooperative Reasoning induced Language Models",
    "citations": "103.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": "GPT-J-6B",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "GQA-8-XXL",
    "publicationDate": "2023-12-23",
    "domain": "Language",
    "task": "Text summarization",
    "parameters": 11000000000.0,
    "trainingComputeFlop": 3.4912896e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2305.13245",
    "reference": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "Google TPU v3",
    "baseModel": "T5-11B",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "nekomata-14b",
    "organization": "rinna",
    "publicationDate": "2023-12-21",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 14200000000.0,
    "trainingComputeFlop": 2.5562e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2404.01657",
    "reference": "rinna/nekomata-14b",
    "citations": "27.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Japan",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Amazon Trainium1",
    "baseModel": "Qwen-14B",
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "rinna",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "Gemini Nano-2",
    "publicationDate": "2023-12-19",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 3250000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2312.11805",
    "reference": "Gemini: A Family of Highly Capable Multimodal Models",
    "citations": "633.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "Google TPU v5e",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "Gemini Nano-1",
    "publicationDate": "2023-12-19",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 1800000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2312.11805",
    "reference": "Gemini: A Family of Highly Capable Multimodal Models",
    "citations": "633.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "Google TPU v5e",
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "FunSearch",
    "publicationDate": "2023-12-14",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 15000000000.0,
    "trainingComputeFlop": 3.87e+23,
    "confidence": "Speculative",
    "link": "https://www.nature.com/articles/s41586-023-06924-6\nhttps://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/",
    "reference": "Mathematical discoveries from program search with large language models",
    "citations": "557.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "PaLM 2",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "CogAgent",
    "organization": [
      "Tsinghua University",
      "Zhipu AI"
    ],
    "publicationDate": "2023-12-14",
    "domain": "Vision",
    "task": "Instruction interpretation",
    "parameters": 18000000000.0,
    "trainingComputeFlop": 6.707e+22,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2312.08914",
    "reference": "CogAgent: A Visual Language Model for GUI Agents",
    "citations": "553.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "CogVLM-17B",
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "VILA-13B",
    "organization": [
      "NVIDIA",
      "Massachusetts Institute of Technology (MIT)"
    ],
    "publicationDate": "2023-12-12",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 13350839296.0,
    "trainingComputeFlop": 2.3003136e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2312.07533\nhttps://huggingface.co/Efficient-Large-Model/VILA-13b",
    "reference": "VILA: On Pre-training for Visual Language Models",
    "citations": "622.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": [
      "Llama 2-13B",
      "CLIP (ViT L/14@336px)"
    ],
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "W.A.L.T",
    "publicationDate": "2023-12-11",
    "domain": "Video",
    "task": "Video generation",
    "parameters": 4719000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2312.06662",
    "reference": "Photorealistic Video Generation with Diffusion Models",
    "citations": null,
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Stanford University",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "Mixtral 8x7B",
    "organization": "Mistral AI",
    "publicationDate": "2023-12-11",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 46700000000.0,
    "trainingComputeFlop": 7.74e+23,
    "confidence": "Speculative",
    "link": [
      "https://mistral.ai/news/mixtral-of-experts/",
      "https://arxiv.org/abs/2401.04088"
    ],
    "reference": "Mixtral of experts: A high quality Sparse Mixture-of-Experts.",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "France",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Mistral AI",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "SeamlessM4T",
    "organization": [
      "Facebook",
      "INRIA",
      "University of California (UC) Berkeley"
    ],
    "publicationDate": "2023-12-08",
    "domain": "Speech",
    "task": "Translation",
    "parameters": 2300000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": [
      "https://arxiv.org/abs/2312.05187",
      "https://huggingface.co/facebook/seamless-m4t-v2-large"
    ],
    "reference": "Seamless: Multilingual Expressive and Streaming Speech Translation",
    "citations": "216.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": "W2v-BERT",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "Llama Guard",
    "publicationDate": "2023-12-07",
    "domain": "Language",
    "task": "Chat",
    "parameters": 7000000000.0,
    "trainingComputeFlop": 1.6e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2312.06674",
    "reference": "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations",
    "citations": "687.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": "Llama 2-7B",
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "Gemini 1.0 Ultra",
    "publicationDate": "2023-12-06",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": 5.0000000001e+25,
    "confidence": "Speculative",
    "link": "https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf",
    "reference": "Gemini: A Family of Highly Capable Multimodal Models",
    "citations": "633.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": "True",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "Gemini 1.0 Pro",
    "publicationDate": "2023-12-06",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "link": "https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf",
    "reference": "Gemini: A Family of Highly Capable Multimodal Models",
    "citations": "633.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "Mamba-24M (SC09)",
    "organization": [
      "Carnegie Mellon University (CMU)",
      "Princeton University"
    ],
    "publicationDate": "2023-12-01",
    "domain": "Speech",
    "task": "Audio generation",
    "parameters": 23400000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2312.00752",
    "reference": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
    "citations": "4684.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Carnegie Mellon University (CMU)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "Qwen-72B",
    "organization": "Alibaba",
    "publicationDate": "2023-11-30",
    "domain": "Language",
    "task": "Chat",
    "parameters": 72000000000.0,
    "trainingComputeFlop": 1.3e+24,
    "confidence": "Confident",
    "link": "https://huggingface.co/Qwen/Qwen-72B",
    "reference": null,
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "PPLX-70B-Online",
    "organization": "Perplexity",
    "publicationDate": "2023-11-29",
    "domain": "Language",
    "task": "Question answering",
    "parameters": 70000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "link": "https://blog.perplexity.ai/blog/introducing-pplx-online-llms",
    "reference": "Introducing PPLX Online LLMs",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": "Llama 2-70B",
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Perplexity",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "GNoME for crystal discovery",
    "publicationDate": "2023-11-29",
    "domain": "Materials science",
    "task": "Crystal discovery",
    "parameters": 16240000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "link": "https://www.nature.com/articles/s41586-023-06735-9",
    "reference": "Scaling deep learning for materials discovery",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "Inflection-2",
    "organization": "Inflection AI",
    "publicationDate": "2023-11-22",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": 1.001e+25,
    "confidence": "Confident",
    "link": "https://inflection.ai/inflection-2",
    "reference": "Inflection-2: The Next Step Up",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": "True",
    "company": "Inflection AI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "Claude 2.1",
    "organization": "Anthropic",
    "publicationDate": "2023-11-21",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://www.anthropic.com/index/claude-2-1",
    "reference": "Introducing Claude 2.1",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": "Claude 2",
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "AndesGPT",
    "organization": "Oppo Mobile Telecommunications",
    "publicationDate": "2023-11-16",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://www.oppo.com/en/newsroom/press/2023-oppo-developers-conference-odc23/",
    "reference": null,
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "Oppo Mobile Telecommunications",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "Nemotron-3-8B",
    "organization": "NVIDIA",
    "publicationDate": "2023-11-15",
    "domain": "Language",
    "task": "Chat",
    "parameters": 8000000000.0,
    "trainingComputeFlop": 1.8e+23,
    "confidence": "Confident",
    "link": "https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/\n\nhttps://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/nemotron-3-8b-base-4k",
    "reference": "NVIDIA AI Foundation Models: Build Custom Enterprise Chatbots and Co-Pilots with Production-Ready LLMs",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "Qwen-Audio-Chat",
    "organization": "Alibaba",
    "publicationDate": "2023-11-14",
    "domain": "Language",
    "task": "Audio question answering",
    "parameters": 8460000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2311.07919",
    "reference": "Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models",
    "citations": "557.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "GraphCast",
    "publicationDate": "2023-11-14",
    "domain": "Earth science",
    "task": "Weather forecasting",
    "parameters": null,
    "trainingComputeFlop": 2.1000000000000002e+22,
    "confidence": "Speculative",
    "link": "https://www.science.org/doi/epdf/10.1126/science.adi2336\n\nhttps://arxiv.org/abs/2212.12794",
    "reference": "Learning skillful medium-range globalweather forecasting",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "Volcano 13B",
    "organization": [
      "Korea University",
      "Korea Advanced Institute of Science and Technology (KAIST)",
      "LG"
    ],
    "publicationDate": "2023-11-13",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 13000000000.0,
    "trainingComputeFlop": 4.56e+22,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2311.07362",
    "reference": "Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision",
    "citations": "77.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "Korea (Republic of)",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": "LLaVA 1.5",
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Korea University",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "SPHINX (Llama 2 13B)",
    "organization": [
      "Shanghai AI Lab",
      "Chinese University of Hong Kong (CUHK)",
      "ShanghaiTech University"
    ],
    "publicationDate": "2023-11-13",
    "domain": "Vision",
    "task": "Visual question answering",
    "parameters": 19900000000.0,
    "trainingComputeFlop": 3.04e+22,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2311.07575",
    "reference": [
      "SPHINX: The Joint Mixing of Weights",
      "Tasks",
      "and Visual Embeddings for Multi-modal Large Language Models"
    ],
    "citations": null,
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "China",
      "Hong Kong"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": "Llama 2-13B",
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Shanghai AI Lab",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "RoFormer",
    "organization": "Zhuiyi Technology",
    "publicationDate": "2023-11-08",
    "domain": "Language",
    "task": "Text classification",
    "parameters": 110000000.0,
    "trainingComputeFlop": 2.162688e+18,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2104.09864",
    "reference": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Zhuiyi Technology",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "MultiBand Diffusion",
    "publicationDate": "2023-11-08",
    "domain": "Audio",
    "task": "Audio generation",
    "parameters": null,
    "trainingComputeFlop": 2.6e+19,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2308.02560",
    "reference": "From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion",
    "citations": "33.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Israel",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "OmniVec",
    "organization": "TensorTour",
    "publicationDate": "2023-11-07",
    "domain": "Multimodal",
    "task": "Image classification",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/2311.05709v1",
    "reference": "OmniVec: Learning robust representations with cross modal sharing",
    "citations": "80.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "BERT-Large",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "TensorTour",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "mPLUG-Owl2",
    "organization": "Alibaba",
    "publicationDate": "2023-11-07",
    "domain": "Vision",
    "task": "Visual question answering",
    "parameters": 7120000000.0,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/2311.04257",
    "reference": "mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Llama 2-7B",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "GPT-4 Turbo",
    "organization": "OpenAI",
    "publicationDate": "2023-11-06",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://openai.com/blog/new-models-and-developer-products-announced-at-devday",
    "reference": "New models and developer products announced at DevDay",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": "True",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "CogVLM-17B",
    "organization": [
      "Tsinghua University",
      "Zhipu AI",
      "Beihang University"
    ],
    "publicationDate": "2023-11-06",
    "domain": "Multimodal",
    "task": "Image captioning",
    "parameters": 17000000000.0,
    "trainingComputeFlop": 6.331e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2311.03079\nhttps://huggingface.co/THUDM/cogvlm-chat-hf\nhttps://github.com/THUDM/CogVLM",
    "reference": "CogVLM: Visual Expert for Pretrained Language Models",
    "citations": "675.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Vicuna-7B v0",
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "LLaVA 1.5",
    "organization": [
      "University of Wisconsin Madison",
      "Microsoft Research"
    ],
    "publicationDate": "2023-11-05",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 13000000000.0,
    "trainingComputeFlop": 7.807e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2310.03744",
    "reference": "Improved Baselines with Visual Instruction Tuning",
    "citations": "3879.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": "Vicuna-13B v0",
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "University of Wisconsin Madison",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "Grok-1",
    "organization": "xAI",
    "publicationDate": "2023-11-04",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 314000000000.0,
    "trainingComputeFlop": 2.90000000001e+24,
    "confidence": "Likely",
    "link": [
      "https://x.ai/model-card/",
      "https://x.ai/blog/grok-os"
    ],
    "reference": "Announcing Grok",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "xAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "RT-Trajectory",
    "publicationDate": "2023-11-03",
    "domain": "Robotics",
    "task": "Robotic manipulation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/2311.01977",
    "reference": "RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches",
    "citations": "105.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "BLUUMI",
    "organization": [
      "University of Turku",
      "Hugging Face"
    ],
    "publicationDate": "2023-11-03",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 176000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2311.05640",
    "reference": "FinGPT: Large Generative Models for a Small Language",
    "citations": "61.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "Finland",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "AMD Radeon Instinct MI250X",
    "baseModel": "BLOOM-176B",
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "University of Turku",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "Yi-34B",
    "organization": "01.AI",
    "publicationDate": "2023-11-02",
    "domain": "Language",
    "task": "Chat",
    "parameters": 34000000000.0,
    "trainingComputeFlop": 6.1e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2403.04652",
    "reference": "Yi: Open Foundation Models by 01.AI",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "01.AI",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "Cohere Embed",
    "organization": "Cohere",
    "publicationDate": "2023-11-02",
    "domain": "Language",
    "task": "Semantic embedding",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://txt.cohere.com/introducing-embed-v3/",
    "reference": "Cohere Command & Embed on Amazon Bedrock",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Canada",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Cohere",
    "department": null,
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "Skywork-13B",
    "organization": "Kunlun Inc.",
    "publicationDate": "2023-10-30",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 13000000000.0,
    "trainingComputeFlop": 2.5e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2310.19341",
    "reference": "Skywork: A More Open Bilingual Foundation Model",
    "citations": "118.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A800 PCIe 40 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Kunlun Inc.",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-10"
  },
  {
    "model": "ChatGLM3-6B",
    "organization": "Zhipu AI",
    "publicationDate": "2023-10-27",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 6000000000.0,
    "trainingComputeFlop": 5.04e+22,
    "confidence": "Speculative",
    "link": "https://www.zhipuai.cn/en/news/76\n\nhttps://huggingface.co/zai-org/chatglm3-6b",
    "reference": "Zhipu AI launches third-generation base model",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Zhipu AI",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-10"
  },
  {
    "model": "DiT-XL/2 + CADS",
    "organization": [
      "ETH Zurich",
      "Disney Research"
    ],
    "publicationDate": "2023-10-26",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 675000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2310.17347v2",
    "reference": "CADS: Unleashing the Diversity of Diffusion Models through Condition-Annealed Sampling",
    "citations": null,
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "Switzerland",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "DiT-XL/2",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "ETH Zurich",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-10"
  },
  {
    "model": "CODEFUSION (Python)",
    "organization": [
      "Microsoft",
      "Microsoft Research"
    ],
    "publicationDate": "2023-10-26",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 75000000.0,
    "trainingComputeFlop": 7.92e+18,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2310.17680 (was withdrawn)\n\nhttps://www.microsoft.com/en-us/research/wp-content/uploads/2023/11/CodeFusion-Revised-CameraReady.pdf",
    "reference": "CODEFUSION: A Pre-trained Diffusion Model for Code Generation",
    "citations": "41.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-10"
  },
  {
    "model": "DALL·E 3",
    "organization": "OpenAI",
    "publicationDate": "2023-10-19",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://cdn.openai.com/papers/dall-e-3.pdf",
    "reference": "Improving Image Generation with Better Captions",
    "citations": "1412.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-10"
  },
  {
    "model": "PaLI-3",
    "publicationDate": "2023-10-17",
    "domain": "Multimodal",
    "task": "Visual question answering",
    "parameters": 5000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2310.09199",
    "reference": [
      "PaLI-3 Vision Language Models: Smaller",
      "Faster",
      "Stronger"
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": [
      "United States",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "UL2",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": [
      "DeepMind",
      "Google Research"
    ],
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-10"
  },
  {
    "model": "ERNIE 4.0",
    "organization": "Baidu",
    "publicationDate": "2023-10-17",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html",
    "reference": [
      "Baidu Launches ERNIE 4.0 Foundation Model",
      "Leading a New Wave of AI-Native Applications"
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Baidu",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-10"
  },
  {
    "model": "RT-2-X",
    "publicationDate": "2023-10-13",
    "domain": "Robotics",
    "task": "Robotic manipulation",
    "parameters": 55000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2310.08864",
    "reference": "Open X-Embodiment: Robotic Learning Datasets and RT-X Models",
    "citations": "708.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "RT-2",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-10"
  },
  {
    "model": "Ferret (13B)",
    "organization": [
      "Columbia University",
      "Apple"
    ],
    "publicationDate": "2023-10-11",
    "domain": "Multimodal",
    "task": "Object recognition",
    "parameters": 13000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2310.07704",
    "reference": "Ferret: Refer and Ground Anything Anywhere at Any Granularity",
    "citations": "434.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": "Vicuna-13B v0",
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Columbia University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-10"
  },
  {
    "model": "RoseTTAFold All-Atom (RFAA)",
    "organization": [
      "University of Washington",
      "Seoul National University",
      "University of Sheffield"
    ],
    "publicationDate": "2023-10-09",
    "domain": "Biology",
    "task": "Protein folding prediction",
    "parameters": null,
    "trainingComputeFlop": 2.14e+20,
    "confidence": "Confident",
    "link": "https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1",
    "reference": "Generalized Biomolecular Modeling and Design with RoseTTAFold All-Atom",
    "citations": null,
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "United States",
      "Korea (Republic of)",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA RTX A6000",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "University of Washington",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-10"
  },
  {
    "model": "FinGPT-13B",
    "organization": [
      "University of California Los Angeles (UCLA)",
      "Columbia University",
      "New York University (NYU)"
    ],
    "publicationDate": "2023-10-07",
    "domain": "Language",
    "task": "Named entity recognition (NER)",
    "parameters": 13000000000.0,
    "trainingComputeFlop": 1.6e+23,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2310.04793; https://github.com/AI4Finance-Foundation/FinGPT",
    "reference": "FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets",
    "citations": "78.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA GeForce RTX 3090",
    "baseModel": "Llama 2-13B",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "University of California Los Angeles (UCLA)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-10"
  },
  {
    "model": "CTM (CIFAR-10)",
    "organization": [
      "Stanford University",
      "Sony"
    ],
    "publicationDate": "2023-10-01",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/2310.02279v1",
    "reference": "Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion",
    "citations": "292.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "Japan"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Stanford University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-10"
  },
  {
    "model": "Amazon Titan",
    "organization": "Amazon",
    "publicationDate": "2023-09-28",
    "domain": "Language",
    "task": "Semantic search",
    "parameters": 200000000000.0,
    "trainingComputeFlop": 4.8e+24,
    "confidence": "Likely",
    "link": "https://aws.amazon.com/bedrock/titan/",
    "reference": null,
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": "True",
    "company": "Amazon",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-09"
  },
  {
    "model": "Show-1",
    "organization": "National University of Singapore",
    "publicationDate": "2023-09-27",
    "domain": "Video",
    "task": "Video generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/2309.15818",
    "reference": "Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation",
    "citations": null,
    "organizationCategorization": "Academia",
    "countryOfOrganization": "Singapore",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "National University of Singapore",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-09"
  },
  {
    "model": "GPT-4V",
    "organization": "OpenAI",
    "publicationDate": "2023-09-25",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://cdn.openai.com/papers/GPTV_System_Card.pdf",
    "reference": "GPT-4V(ision) system card",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": "GPT-4",
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-09"
  },
  {
    "model": "AlphaMissense",
    "publicationDate": "2023-09-22",
    "domain": "Biology",
    "task": "Protein pathogenicity prediction",
    "parameters": 93000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "link": "https://www.science.org/doi/10.1126/science.adg7492",
    "reference": "Accurate proteome-wide missense variant effect prediction with AlphaMissense",
    "citations": "1157.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "AlphaFold 2",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-09"
  },
  {
    "model": "Robot Parkour",
    "organization": [
      "Shanghai Qi Zhi institute",
      "Stanford University",
      "Carnegie Mellon University (CMU)",
      "Tsinghua University"
    ],
    "publicationDate": "2023-09-12",
    "domain": "Robotics",
    "task": "Animal (human/non-human) imitation",
    "parameters": 500000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2309.05665",
    "reference": "Robot Parkour Learning",
    "citations": "211.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "China",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA GeForce RTX 3090",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Shanghai Qi Zhi institute",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-09"
  },
  {
    "model": "Falcon-180B",
    "organization": "Technology Innovation Institute",
    "publicationDate": "2023-09-06",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 180000000000.0,
    "trainingComputeFlop": 3.76e+24,
    "confidence": "Confident",
    "link": "https://falconllm.tii.ae/falcon-180b.html; https://arxiv.org/abs/2311.16867",
    "reference": "The Falcon Series of Open Language Models",
    "citations": "576.0",
    "organizationCategorization": "Government",
    "countryOfOrganization": "United Arab Emirates",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": "True",
    "company": "Technology Innovation Institute",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-09"
  },
  {
    "model": "Swift",
    "organization": "Intel Labs",
    "publicationDate": "2023-08-30",
    "domain": "Robotics",
    "task": "Helicopter driving",
    "parameters": 56804.0,
    "trainingComputeFlop": 5.337e+16,
    "confidence": "Likely",
    "link": "https://www.nature.com/articles/s41586-023-06419-4",
    "reference": "Champion-level drone racing using deep reinforcement learning",
    "citations": "101.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA GeForce RTX 3090",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Intel Labs",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-08"
  },
  {
    "model": "Jais",
    "organization": [
      "Cerebras Systems",
      "Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)",
      "Inception G42"
    ],
    "publicationDate": "2023-08-29",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 13000000000.0,
    "trainingComputeFlop": 4.8946763e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2308.16149",
    "reference": "Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models",
    "citations": "57.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "United Arab Emirates"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Cerebras CS-2",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Cerebras Systems",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-08"
  },
  {
    "model": "PeptideBERT",
    "organization": "Carnegie Mellon University (CMU)",
    "publicationDate": "2023-08-28",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": null,
    "trainingComputeFlop": 4.9e+16,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2309.03099",
    "reference": "PeptideBERT: A language Model based on Transformers for Peptide Property Prediction",
    "citations": null,
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA GeForce GTX 1080 Ti",
    "baseModel": "ProtBERT-UniRef",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Carnegie Mellon University (CMU)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-08"
  },
  {
    "model": "Qwen-VL",
    "organization": "Alibaba",
    "publicationDate": "2023-08-24",
    "domain": "Multimodal",
    "task": "Image captioning",
    "parameters": 9600000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2308.12966",
    "reference": [
      "Qwen-VL: A Versatile Vision-Language Model for Understanding",
      "Localization",
      "Text Reading",
      "and Beyond"
    ],
    "citations": "1444.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Qwen-7B",
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-08"
  },
  {
    "model": "GGNN",
    "organization": [
      "Westlake University",
      "Tsinghua University",
      "Toyota Technological Institute at Chicago"
    ],
    "publicationDate": "2023-08-05",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": null,
    "trainingComputeFlop": 7.56e+21,
    "confidence": "Confident",
    "link": "https://www.nature.com/articles/s42003-023-05133-1",
    "reference": "Integration of pre-trained protein language models into geometric deep learning networks",
    "citations": "40.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "China",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": "ESM2-650M",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Westlake University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-08"
  },
  {
    "model": "RT-2",
    "publicationDate": "2023-07-28",
    "domain": "Robotics",
    "task": "Robotic manipulation",
    "parameters": 55000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2307.15818",
    "reference": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
    "citations": "1946.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "PaLI-X",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-07"
  },
  {
    "model": "AudioLM",
    "publicationDate": "2023-07-26",
    "domain": "Audio",
    "task": "Audio generation",
    "parameters": 1500000000.0,
    "trainingComputeFlop": 3.9e+18,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/2209.03143",
    "reference": "AudioLM: a Language Modeling Approach to Audio Generation",
    "citations": "779.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-07"
  },
  {
    "model": "Llama 2-70B",
    "publicationDate": "2023-07-18",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 70000000000.0,
    "trainingComputeFlop": 8.1e+23,
    "confidence": "Confident",
    "link": "https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/\nhttps://arxiv.org/abs/2307.09288",
    "reference": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
    "citations": "14691.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-07"
  },
  {
    "model": "Llama 2-7B",
    "publicationDate": "2023-07-18",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 7000000000.0,
    "trainingComputeFlop": 8.4e+22,
    "confidence": "Confident",
    "link": "https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/",
    "reference": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
    "citations": "14691.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-07"
  },
  {
    "model": "GPT3-2.7B (FlashAttention-2)",
    "organization": [
      "Stanford University",
      "Princeton University"
    ],
    "publicationDate": "2023-07-18",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 2700000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/pdf/2307.08691",
    "reference": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning",
    "citations": "1916.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Stanford University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-07"
  },
  {
    "model": "Claude 2",
    "organization": "Anthropic",
    "publicationDate": "2023-07-11",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": 3.866e+24,
    "confidence": "Speculative",
    "link": [
      "https://www.anthropic.com/index/claude-2",
      "https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf"
    ],
    "reference": null,
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": "True",
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-07"
  },
  {
    "model": "xTrimoPGLM -100B",
    "organization": [
      "Tsinghua University",
      "BioMap Research"
    ],
    "publicationDate": "2023-07-06",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 100000000000.0,
    "trainingComputeFlop": 6.2e+23,
    "confidence": "Confident",
    "link": "https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4",
    "reference": "xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein",
    "citations": "135.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-07"
  },
  {
    "model": "InternLM",
    "organization": [
      "Shanghai AI Lab",
      "SenseTime"
    ],
    "publicationDate": "2023-07-06",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 104000000000.0,
    "trainingComputeFlop": 9.984e+23,
    "confidence": "Confident",
    "link": "https://github.com/InternLM/InternLM-techreport/blob/main/InternLM.pdf",
    "reference": null,
    "citations": null,
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "China",
      "Hong Kong"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Shanghai AI Lab",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-07"
  },
  {
    "model": "Pangu-Weather",
    "organization": "Huawei",
    "publicationDate": "2023-07-05",
    "domain": "Earth science",
    "task": "Weather forecasting",
    "parameters": 256000000.0,
    "trainingComputeFlop": 3.98e+22,
    "confidence": "Confident",
    "link": [
      "https://www.nature.com/articles/s41586-023-06185-3",
      "https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html",
      "https://www.huawei.com/en/news/2023/7/pangu-ai-model-nature-publish"
    ],
    "reference": "Accurate medium-range global weather forecasting with 3D neural networks",
    "citations": "197.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Huawei",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-07"
  },
  {
    "model": "Stable Diffusion XL (SDXL)",
    "organization": "Stability AI",
    "publicationDate": "2023-07-04",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 3400000000.0,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/2307.01952",
    "reference": "SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis",
    "citations": "3499.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United Kingdom",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Stability AI",
    "department": null,
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-07"
  },
  {
    "model": "HyenaDNA",
    "organization": [
      "Stanford University",
      "Harvard University",
      "Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",
      "University of Montreal / Université de Montréal"
    ],
    "publicationDate": "2023-06-27",
    "domain": "Biology",
    "task": "Protein or nucleotide language model (pLM/nLM)",
    "parameters": 6600000.0,
    "trainingComputeFlop": 1.811e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2306.15794",
    "reference": "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution",
    "citations": "377.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "United States",
      "Canada"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Stanford University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-06"
  },
  {
    "model": "ERNIE 3.5",
    "organization": "Baidu",
    "publicationDate": "2023-06-27",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "http://research.baidu.com/Blog/index-view?id=185",
    "reference": "Introducing ERNIE 3.5: Baidu’s Knowledge-Enhanced Foundation Model Takes a Giant Leap Forward",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "Baidu",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-06"
  },
  {
    "model": "MusicGen",
    "publicationDate": "2023-06-08",
    "domain": "Audio",
    "task": "Audio generation",
    "parameters": 3359000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2306.05284",
    "reference": "Simple and Controllable Music Generation",
    "citations": "551.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-06"
  },
  {
    "model": "LTM-1",
    "organization": "Magic",
    "publicationDate": "2023-06-06",
    "domain": "Language",
    "task": "Code generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://magic.dev/blog/ltm-1",
    "reference": [
      "LTM-1: an LLM with a 5",
      "000",
      "000 token context window"
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Magic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-06"
  },
  {
    "model": "PaLI-X",
    "publicationDate": "2023-05-29",
    "domain": "Multimodal",
    "task": "Image captioning",
    "parameters": 55000000000.0,
    "trainingComputeFlop": 5.6e+23,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2305.18565",
    "reference": "PaLI-X: On Scaling up a Multilingual Vision and Language Model",
    "citations": "241.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "UL2",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-05"
  },
  {
    "model": "DPO on Pythia-2.8B",
    "organization": [
      "Stanford University",
      "CZ Biohub Network"
    ],
    "publicationDate": "2023-05-29",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 2800000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2305.18290",
    "reference": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "citations": null,
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": null,
    "baseModel": "Pythia-2.8b",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Stanford University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-05"
  },
  {
    "model": "Goat-7B",
    "organization": "National University of Singapore",
    "publicationDate": "2023-05-23",
    "domain": "Language",
    "task": "Quantitative reasoning",
    "parameters": 7000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/2305.14201",
    "reference": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
    "citations": "98.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "Singapore",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A10 PCIe",
    "baseModel": "LLaMA-7B",
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "National University of Singapore",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-05"
  },
  {
    "model": "CodeT5+",
    "organization": "Salesforce",
    "publicationDate": "2023-05-20",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 16000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2305.07922",
    "reference": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
    "citations": "585.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Salesforce",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-05"
  },
  {
    "model": "ONE-PEACE",
    "organization": [
      "Alibaba",
      "Huazhong University of Science and Technology"
    ],
    "publicationDate": "2023-05-18",
    "domain": "Multimodal",
    "task": "Image classification",
    "parameters": 4000000000.0,
    "trainingComputeFlop": 1.8e+20,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/2305.11172v1",
    "reference": "ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities",
    "citations": "149.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-05"
  },
  {
    "model": "CoEdiT-xxl",
    "organization": [
      "University of Minnesota",
      "Grammarly"
    ],
    "publicationDate": "2023-05-17",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 11000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "link": [
      "https://arxiv.org/abs/2305.09857",
      "https://huggingface.co/grammarly/coedit-large"
    ],
    "reference": "CoEdIT: Text Editing by Task-Specific Instruction Tuning",
    "citations": "69.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": "Flan-T5 11B",
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "University of Minnesota",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-05"
  },
  {
    "model": "Med-PaLM 2",
    "publicationDate": "2023-05-16",
    "domain": "Medicine",
    "task": "Question answering",
    "parameters": 340000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2305.09617",
    "reference": "Towards Expert-Level Medical Question Answering with Large Language Models",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": [
      "United States",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "PaLM 2",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": [
      "Google Research",
      "DeepMind"
    ],
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-05"
  },
  {
    "model": "InstructBLIP",
    "organization": [
      "Salesforce Research",
      "Hong Kong University of Science and Technology (HKUST)",
      "Nanyang Technological University"
    ],
    "publicationDate": "2023-05-11",
    "domain": "Multimodal",
    "task": "Visual question answering",
    "parameters": 13000000000.0,
    "trainingComputeFlop": 1.94e+20,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2305.06500",
    "reference": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
    "citations": "2729.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Hong Kong",
      "Singapore"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": "Vicuna-13B v0",
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Salesforce Research",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-05"
  },
  {
    "model": "PaLM 2",
    "organization": "Google",
    "publicationDate": "2023-05-10",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 340000000000.0,
    "trainingComputeFlop": 7.34e+24,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2305.10403",
    "reference": "PaLM 2 Technical Report",
    "citations": "1734.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": "True",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-05"
  },
  {
    "model": "StarCoder",
    "organization": [
      "Hugging Face",
      "ServiceNow",
      "Northeastern University",
      "Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",
      "Carnegie Mellon University (CMU)",
      "Johns Hopkins University",
      "Leipzig University",
      "ScaDS.AI",
      "Queen Mary University of London",
      "Roblox",
      "Sea AI Lab",
      "Technion - Israel Institute of Technology",
      "Monash University",
      "CSIRO",
      "Data61",
      "McGill University",
      "Saama",
      "University of British Columbia (UBC)",
      "Massachusetts Institute of Technology (MIT)",
      "Technical University of Munich",
      "IBM",
      "University of Vermont",
      "UnfoldML",
      "SAP",
      "University of Notre Dame",
      "Columbia University",
      "New York University (NYU)",
      "University of Allahabad",
      "Discover Dollar",
      "Toloka",
      "Telefonica",
      "Stanford University",
      "Weizmann Institute of Science",
      "Alan Turing Institute",
      "Wellesley College",
      "EleutherAI",
      "Forschungszentrum Julich"
    ],
    "publicationDate": "2023-05-09",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 15500000000.0,
    "trainingComputeFlop": 8.46e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2305.06161",
    "reference": "StarCoder: may the source be with you!",
    "citations": "989.0",
    "organizationCategorization": [
      "Industry",
      "Academia",
      "Government",
      "Research collective"
    ],
    "countryOfOrganization": [
      "United States",
      "Canada",
      "Germany",
      "United Kingdom",
      "Singapore",
      "Israel",
      "Australia",
      "Sweden",
      "India",
      "Netherlands",
      "Spain"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Hugging Face",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-05"
  },
  {
    "model": "ImageBind",
    "publicationDate": "2023-05-09",
    "domain": "Multimodal",
    "task": "Image classification",
    "parameters": 932000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "link": [
      "https://arxiv.org/abs/2305.05665",
      "https://github.com/facebookresearch/ImageBind"
    ],
    "reference": "IMAGEBIND: One Embedding Space To Bind Them All",
    "citations": "1229.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": "ViT-Huge/14",
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-05"
  },
  {
    "model": "Agile Soccer Robot",
    "publicationDate": "2023-04-26",
    "domain": "Robotics",
    "task": "Animal (human/non-human) imitation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/2304.13653",
    "reference": "Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning",
    "citations": "213.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-04"
  },
  {
    "model": "LLaVA",
    "organization": [
      "University of Wisconsin Madison",
      "Microsoft Research",
      "Columbia University"
    ],
    "publicationDate": "2023-04-17",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 13000000000.0,
    "trainingComputeFlop": 7.8049e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2304.08485",
    "reference": "Visual Instruction Tuning",
    "citations": "6867.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": "Vicuna-13B v0",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "University of Wisconsin Madison",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-04"
  },
  {
    "model": "DINOv2",
    "publicationDate": "2023-04-14",
    "domain": "Vision",
    "task": "Image representation",
    "parameters": 1140000000.0,
    "trainingComputeFlop": 7.41851136e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2304.07193",
    "reference": "DINOv2: Learning Robust Visual Features without Supervision",
    "citations": "5425.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-04"
  },
  {
    "model": "Incoder-6.7B",
    "publicationDate": "2023-04-09",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 6700000000.0,
    "trainingComputeFlop": 3.00001e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2204.05999",
    "reference": "InCoder: A Generative Model for Code Infilling and Synthesis",
    "citations": "775.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-04"
  },
  {
    "model": "Segment Anything Model",
    "publicationDate": "2023-04-05",
    "domain": "Vision",
    "task": "Image segmentation",
    "parameters": 636000000.0,
    "trainingComputeFlop": 7.8e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2304.02643",
    "reference": "Segment Anything",
    "citations": "10321.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA A100",
    "baseModel": "ViT-Huge/14",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-04"
  },
  {
    "model": "BloombergGPT",
    "organization": [
      "Bloomberg",
      "Johns Hopkins University"
    ],
    "publicationDate": "2023-03-30",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 50558868480.0,
    "trainingComputeFlop": 2.36e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2303.17564",
    "reference": "BloombergGPT: A Large Language Model for Finance",
    "citations": "1067.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Bloomberg",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "VideoMAE V2",
    "organization": [
      "Nanjing University",
      "Shenzhen Institute of Advanced Technology",
      "Shanghai AI Lab"
    ],
    "publicationDate": "2023-03-29",
    "domain": "Video",
    "task": "Action recognition",
    "parameters": 1000000000.0,
    "trainingComputeFlop": 9.7e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2303.16727v2",
    "reference": "VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking",
    "citations": "489.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": "ViT-G/14",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Nanjing University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "SigLIP 400M",
    "publicationDate": "2023-03-27",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 400000000.0,
    "trainingComputeFlop": 4.9467301e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2303.15343",
    "reference": "Sigmoid Loss for Language Image Pre-Training",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "Firefly",
    "organization": "Adobe",
    "publicationDate": "2023-03-21",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://news.adobe.com/news/news-details/2023/Adobe-Unveils-Firefly-a-Family-of-new-Creative-Generative-AI/default.aspx",
    "reference": [
      "Adobe Unveils Firefly",
      "a Family of new Creative Generative AI"
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "Adobe",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "PanGu-Σ",
    "organization": "Huawei Noah's Ark Lab",
    "publicationDate": "2023-03-20",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 1085000000000.0,
    "trainingComputeFlop": 4.67e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2303.10845",
    "reference": "PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing",
    "citations": "78.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Huawei Ascend 910",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Huawei Noah's Ark Lab",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Massive (≥1T)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "Gen-2",
    "organization": "Runway",
    "publicationDate": "2023-03-20",
    "domain": "Video",
    "task": "Video generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://research.runwayml.com/gen2",
    "reference": [
      "Gen-2: Generate novel videos with text",
      "images or video clips"
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Runway",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "LEP-AD",
    "organization": [
      "King Abdullah University of Science and Technology (KAUST)",
      "Karolinska Institute"
    ],
    "publicationDate": "2023-03-15",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 3007381000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://www.biorxiv.org/content/10.1101/2023.03.14.532563v1.full.pdf",
    "reference": "LEP-AD: Language Embedding of Proteins and Attention to Drugs predicts Drug Target Interactions",
    "citations": "1.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "Saudi Arabia",
      "Sweden"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "ESM2-3B",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "King Abdullah University of Science and Technology (KAUST)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "GPT-4",
    "organization": "OpenAI",
    "publicationDate": "2023-03-15",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": 1800000000000.0,
    "trainingComputeFlop": 2.1e+25,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2303.08774",
    "reference": "GPT-4 Technical Report",
    "citations": "19614.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": "True",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Massive (≥1T)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "Falcon-40B",
    "organization": "Technology Innovation Institute",
    "publicationDate": "2023-03-15",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 40000000000.0,
    "trainingComputeFlop": 2.4e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2311.16867; https://www.tii.ae/news/abu-dhabi-based-technology-innovation-institute-introduces-falcon-llm-foundational-large",
    "reference": "Abu Dhabi-based Technology Innovation Institute Introduces Falcon LLM: Foundational Large Language Model (LLM) outperforms GPT-3 with 40 Billion Parameters",
    "citations": "0.0",
    "organizationCategorization": "Government",
    "countryOfOrganization": "United Arab Emirates",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Technology Innovation Institute",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "Claude",
    "organization": "Anthropic",
    "publicationDate": "2023-03-14",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://www.anthropic.com/index/introducing-claude",
    "reference": "Introducing Claude",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "PaLM-E",
    "organization": [
      "Google",
      "TU Berlin"
    ],
    "publicationDate": "2023-03-06",
    "domain": "Robotics",
    "task": "Visual question answering",
    "parameters": 562000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2303.03378",
    "reference": "PaLM-E: An Embodied Multimodal Language Model",
    "citations": "2104.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Germany"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "PaLM (540B)",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "AudioGen",
    "publicationDate": "2023-03-05",
    "domain": "Audio",
    "task": "Audio generation",
    "parameters": 1000000000.0,
    "trainingComputeFlop": 9.5e+21,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2209.15352",
    "reference": "AudioGen: Textually Guided Audio Generation",
    "citations": "378.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Israel"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "DiT-XL/2",
    "organization": [
      "New York University (NYU)",
      "University of California (UC) Berkeley"
    ],
    "publicationDate": "2023-03-02",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 675000000.0,
    "trainingComputeFlop": 6e+20,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2212.09748",
    "reference": "Scalable Diffusion Models with Transformers",
    "citations": "3729.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": "Stable Diffusion (LDM-KL-8-G)",
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "New York University (NYU)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "LLaMA-65B",
    "publicationDate": "2023-02-24",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 65200000000.0,
    "trainingComputeFlop": 5.5e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2302.13971",
    "reference": "LLaMA: Open and Efficient Foundation Language Models",
    "citations": "16922.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-02"
  },
  {
    "model": "BASIC-L + Lion",
    "organization": [
      "Google",
      "University of California Los Angeles (UCLA)"
    ],
    "publicationDate": "2023-02-13",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 3070000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2302.06675v4",
    "reference": "Symbolic Discovery of Optimization Algorithms",
    "citations": "485.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-02"
  },
  {
    "model": "ViT-22B",
    "organization": "Google",
    "publicationDate": "2023-02-10",
    "domain": "Vision",
    "task": "Object detection",
    "parameters": 21743000000.0,
    "trainingComputeFlop": 1.93248e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2302.05442v1",
    "reference": "Scaling Vision Transformers to 22 Billion Parameters",
    "citations": "727.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-02"
  },
  {
    "model": "ProteinDT",
    "organization": [
      "University of California (UC) Berkeley",
      "California Institute of Technology",
      "University of Toronto",
      "University of Wisconsin Madison",
      "Texas A&M",
      "NVIDIA",
      "Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)"
    ],
    "publicationDate": "2023-02-09",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2302.04611",
    "reference": "A Text-guided Protein Design Framework",
    "citations": null,
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "Canada"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "SciBERT",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "University of California (UC) Berkeley",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-02"
  },
  {
    "model": "Gen-1",
    "organization": "Runway",
    "publicationDate": "2023-02-06",
    "domain": "Video",
    "task": "Video generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/2302.03011",
    "reference": "Structure and Content-Guided Video Synthesis with Diffusion Models",
    "citations": "645.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Runway",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-02"
  },
  {
    "model": "BLIP-2 (Q-Former)",
    "organization": "Salesforce Research",
    "publicationDate": "2023-01-30",
    "domain": "Vision",
    "task": "Visual question answering",
    "parameters": 1480000000.0,
    "trainingComputeFlop": 1.20000000001e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2301.12597",
    "reference": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
    "citations": "6235.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Salesforce Research",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-01"
  },
  {
    "model": "DDPM-IP (CelebA)",
    "organization": "Utrecht University",
    "publicationDate": "2023-01-27",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 295000000.0,
    "trainingComputeFlop": 3.5e+20,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2301.11706v3",
    "reference": "Input Perturbation Reduces Exposure Bias in Diffusion Models",
    "citations": "80.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "Netherlands",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Utrecht University",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-01"
  },
  {
    "model": "MusicLM",
    "organization": "Google",
    "publicationDate": "2023-01-26",
    "domain": "Audio",
    "task": "Audio generation",
    "parameters": 860000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2301.11325",
    "reference": "MusicLM: Generating Music From Text",
    "citations": "582.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "W2v-BERT",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-01"
  },
  {
    "model": "Ankh_large",
    "organization": [
      "Technical University of Munich",
      "Columbia University"
    ],
    "publicationDate": "2023-01-16",
    "domain": "Biology",
    "task": "Protein generation",
    "parameters": 1900000000.0,
    "trainingComputeFlop": 6.5e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2301.06568",
    "reference": "Ankh: Optimized Protein Language Model Unlocks General-Purpose Modelling",
    "citations": "139.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "Germany",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Technical University of Munich",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-01"
  },
  {
    "model": "Nucleotide Transformer",
    "organization": [
      "NVIDIA",
      "Technical University of Munich",
      "InstaDeep"
    ],
    "publicationDate": "2023-01-15",
    "domain": "Biology",
    "task": "Protein or nucleotide language model (pLM/nLM)",
    "parameters": 2500000000.0,
    "trainingComputeFlop": 8.08e+21,
    "confidence": "Likely",
    "link": "https://www.biorxiv.org/content/10.1101/2023.01.11.523679v1.full.pdf",
    "reference": "The Nucleotide Transformer: Building and Evaluating Robust\nFoundation Models for Human Genomics",
    "citations": "22.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Germany",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-01"
  },
  {
    "model": "DreamerV3",
    "publicationDate": "2023-01-10",
    "domain": "Games",
    "task": "Open ended play",
    "parameters": 200000000.0,
    "trainingComputeFlop": 2.2032e+20,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2301.04104v1",
    "reference": "Mastering Diverse Domains through World Models",
    "citations": "797.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United Kingdom",
      "Canada"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-01"
  },
  {
    "model": "VALL-E",
    "organization": "Microsoft",
    "publicationDate": "2023-01-05",
    "domain": "Audio",
    "task": "Speech synthesis",
    "parameters": 353000000.0,
    "trainingComputeFlop": 1.01e+19,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/2301.02111",
    "reference": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers",
    "citations": "974.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-01"
  },
  {
    "model": "Hybrid H3-2.7B",
    "organization": [
      "Stanford University",
      "University at Buffalo"
    ],
    "publicationDate": "2022-12-28",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 2700000000.0,
    "trainingComputeFlop": 6.48e+21,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2212.14052",
    "reference": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models",
    "citations": "537.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Stanford University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-12"
  },
  {
    "model": "CaLM",
    "organization": "University of Oxford",
    "publicationDate": "2022-12-19",
    "domain": "Biology",
    "task": "Protein or nucleotide language model (pLM/nLM)",
    "parameters": 86000000.0,
    "trainingComputeFlop": 2.9e+19,
    "confidence": "Likely",
    "link": "https://www.biorxiv.org/content/10.1101/2022.12.15.519894v1.full.pdf",
    "reference": "Codon language embeddings provide strong signals for protein engineering",
    "citations": "32.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United Kingdom",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Quadro RTX 4000",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "University of Oxford",
    "department": null,
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-12"
  },
  {
    "model": "RT-1",
    "organization": "Google",
    "publicationDate": "2022-12-13",
    "domain": "Robotics",
    "task": "Robotic manipulation",
    "parameters": 35000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2212.06817",
    "reference": "RT-1: Robotics Transformer for Real-World Control at Scale",
    "citations": "1591.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-12"
  },
  {
    "model": "TranceptEve",
    "organization": [
      "University of Oxford",
      "Harvard Medical School"
    ],
    "publicationDate": "2022-12-10",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1",
    "reference": "TranceptEVE: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction",
    "citations": null,
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "United Kingdom",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Tranception",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "University of Oxford",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-12"
  },
  {
    "model": "Vega v2",
    "organization": [
      "Wuhan University",
      "JD Explore Academy",
      "Shanghai AI Lab",
      "Nanyang Technological University",
      "Washington University in St Louis",
      "Chongqing University of Posts and Telecommunications",
      "University of Sydney"
    ],
    "publicationDate": "2022-12-04",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 6000000000.0,
    "trainingComputeFlop": 7.76e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2212.01853",
    "reference": "Toward Efficient Language Model Pretraining and Downstream Adaptation via Self-Evolution: A Case Study on SuperGLUE",
    "citations": "39.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "China",
      "Singapore",
      "United States",
      "Australia"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Wuhan University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-12"
  },
  {
    "model": "DeepNash",
    "publicationDate": "2022-12-01",
    "domain": "Games",
    "task": "Stratego",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://www.science.org/stoken/author-tokens/ST-887/full",
    "reference": "Mastering the game of Stratego with model-free multiagent reinforcement learning",
    "citations": "239.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-12"
  },
  {
    "model": "GPT-3.5 Turbo",
    "organization": "OpenAI",
    "publicationDate": "2022-11-30",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 20000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "link": "https://platform.openai.com/docs/models/gpt-3.5-turbo",
    "reference": [
      "A fast",
      "inexpensive model for simple tasks"
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "GPT-3.5",
    "organization": "OpenAI",
    "publicationDate": "2022-11-28",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": 2.578e+24,
    "confidence": "Speculative",
    "link": "https://platform.openai.com/docs/models/gpt-3-5",
    "reference": null,
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": "True",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "DiT-XL/2 + Discriminator Guidance",
    "organization": [
      "Korea Advanced Institute of Science and Technology (KAIST)",
      "NAVER"
    ],
    "publicationDate": "2022-11-28",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/2211.17091v4",
    "reference": "Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models",
    "citations": "100.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "Korea (Republic of)",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": "DiT-XL/2",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Korea Advanced Institute of Science and Technology (KAIST)",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "Discriminator Guidance",
    "organization": [
      "Korea Advanced Institute of Science and Technology (KAIST)",
      "NAVER"
    ],
    "publicationDate": "2022-11-28",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": 2.1570000001e+20,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2211.17091v4",
    "reference": "Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models",
    "citations": "100.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "Korea (Republic of)",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 PCIe",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Korea Advanced Institute of Science and Technology (KAIST)",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "ALM 1.0",
    "organization": "Beijing Academy of Artificial Intelligence / BAAI",
    "publicationDate": "2022-11-28",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 335000000.0,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "link": "https://github.com/FlagAI-Open/FlagAI/blob/master/examples/ALM/README.md",
    "reference": "ALM 1.0",
    "citations": null,
    "organizationCategorization": "Academia",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "Beijing Academy of Artificial Intelligence / BAAI",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "CICERO",
    "publicationDate": "2022-11-22",
    "domain": "Games",
    "task": "Diplomacy",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://www.science.org/doi/10.1126/science.ade9097",
    "reference": "Human-level play in the game of Diplomacy by combining language models with strategic reasoning",
    "citations": "438.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "AR-LDM",
    "organization": [
      "Alibaba",
      "University of Waterloo",
      "Vector Institute"
    ],
    "publicationDate": "2022-11-20",
    "domain": "Image generation",
    "task": "Text-to-image",
    "parameters": 1500000000.0,
    "trainingComputeFlop": 5.1e+20,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2211.10950",
    "reference": "Synthesizing Coherent Story with Auto-Regressive Latent Diffusion Models",
    "citations": "76.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "China",
      "Canada"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": "Stable Diffusion (LDM-KL-8-G)",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "Fusion in Encoder",
    "organization": "Samsung",
    "publicationDate": "2022-11-18",
    "domain": "Language",
    "task": "Question answering",
    "parameters": 330000000.0,
    "trainingComputeFlop": 1.3e+20,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2211.10147",
    "reference": "FiE: Building a Global Probability Space by Leveraging Early Fusion in Encoder for Open-Domain Question Answering",
    "citations": "11.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Korea (Republic of)",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Samsung",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "Galactica",
    "publicationDate": "2022-11-16",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 120000000000.0,
    "trainingComputeFlop": 3.24e+23,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2211.09085",
    "reference": "Galactica: A Large Language Model for Science",
    "citations": "903.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "EVA-01",
    "organization": [
      "Beijing Academy of Artificial Intelligence / BAAI",
      "Huazhong University of Science and Technology",
      "Zhejiang University (ZJU)",
      "Beijing Institute of Technology"
    ],
    "publicationDate": "2022-11-14",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 1011000000.0,
    "trainingComputeFlop": 1.501e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2211.07636",
    "reference": "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale",
    "citations": "865.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Beijing Academy of Artificial Intelligence / BAAI",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "AltCLIP_M9",
    "organization": "Beijing Academy of Artificial Intelligence / BAAI",
    "publicationDate": "2022-11-12",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/2211.06679",
    "reference": "AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities",
    "citations": "103.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "CLIP (ViT L/14@336px)",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Beijing Academy of Artificial Intelligence / BAAI",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "InternImage",
    "organization": [
      "Shanghai AI Lab",
      "Tsinghua University",
      "Nanjing University",
      "SenseTime",
      "Chinese University of Hong Kong (CUHK)"
    ],
    "publicationDate": "2022-11-10",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 1080000000.0,
    "trainingComputeFlop": 2.408e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2211.05778",
    "reference": "InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions",
    "citations": "913.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "China",
      "Hong Kong"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Shanghai AI Lab",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "mT0-13B",
    "organization": [
      "Hugging Face",
      "BigScience"
    ],
    "publicationDate": "2022-11-03",
    "domain": "Language",
    "task": "Translation",
    "parameters": 13000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2211.01786",
    "reference": "Crosslingual Generalization through Multitask Finetuning",
    "citations": "242.0",
    "organizationCategorization": [
      "Industry",
      "Research collective"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "mT5-XXL",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Hugging Face",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "Mogrifier RLSTM (WT2)",
    "publicationDate": "2022-11-03",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 35000000.0,
    "trainingComputeFlop": 1.4e+17,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2211.01848",
    "reference": "Circling Back to Recurrent Models of Language",
    "citations": "0.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "eDiff-I",
    "organization": "NVIDIA",
    "publicationDate": "2022-11-02",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 9100000000.0,
    "trainingComputeFlop": 5.46e+19,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2211.01324",
    "reference": "eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers",
    "citations": "954.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "EnCodec",
    "publicationDate": "2022-10-24",
    "domain": "Audio",
    "task": "Audio generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/2210.13438",
    "reference": "High Fidelity Neural Audio Compression",
    "citations": "938.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-10"
  },
  {
    "model": "U-PaLM (540B)",
    "organization": "Google",
    "publicationDate": "2022-10-20",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 540000000000.0,
    "trainingComputeFlop": 2.53e+24,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2210.11399",
    "reference": "Transcending Scaling Laws with 0.1% Extra Compute",
    "citations": "73.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": "PaLM (540B)",
    "modelAccessibility": "Unreleased",
    "frontierModel": "True",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-10"
  },
  {
    "model": "LMSI-Palm",
    "organization": [
      "Google",
      "University of Illinois Urbana-Champaign (UIUC)"
    ],
    "publicationDate": "2022-10-20",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 540000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2210.11610",
    "reference": "Large Language Models Can Self-Improve",
    "citations": "728.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "PaLM (540B)",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-10"
  },
  {
    "model": "Flan-PaLM 540B",
    "organization": "Google",
    "publicationDate": "2022-10-20",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 540000000000.0,
    "trainingComputeFlop": 2.540000000001e+24,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2210.11416",
    "reference": "Scaling Instruction-Finetuned Language Models",
    "citations": "3703.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v4",
    "baseModel": "PaLM (540B)",
    "modelAccessibility": "Unreleased",
    "frontierModel": "True",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-10"
  },
  {
    "model": "GenSLM",
    "organization": [
      "University of Chicago",
      "NVIDIA",
      "Harvard University",
      "Cerebras Systems",
      "Technical University of Munich",
      "California Institute of Technology"
    ],
    "publicationDate": "2022-10-11",
    "domain": "Biology",
    "task": "Protein or nucleotide language model (pLM/nLM)",
    "parameters": 25000000000.0,
    "trainingComputeFlop": 1.42e+21,
    "confidence": "Confident",
    "link": "https://www.biorxiv.org/content/biorxiv/early/2022/10/11/2022.10.10.511571.full.pdf",
    "reference": "GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics",
    "citations": "113.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "Germany"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "University of Chicago",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-10"
  },
  {
    "model": "Diplodocus",
    "publicationDate": "2022-10-11",
    "domain": "Games",
    "task": "Diplomacy",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/2210.05492",
    "reference": "Mastering the Game of No-Press Diplomacy via Human-Regularized Reinforcement Learning and Planning",
    "citations": "53.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-10"
  },
  {
    "model": "Phenaki",
    "publicationDate": "2022-10-05",
    "domain": "Video",
    "task": "Video generation",
    "parameters": 1800000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "link": "https://arxiv.org/abs/2210.02399",
    "reference": "Phenaki: Variable Length Video Generation From Open Domain Textual Description",
    "citations": "289.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United Kingdom",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "University College London (UCL)",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-10"
  },
  {
    "model": "AlphaTensor",
    "publicationDate": "2022-10-05",
    "domain": "Other",
    "task": null,
    "parameters": null,
    "trainingComputeFlop": 7.1414784e+20,
    "confidence": "Confident",
    "link": "https://www.nature.com/articles/s41586-022-05172-4",
    "reference": "Discovering faster matrix multiplication algorithms with reinforcement learning",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-10"
  },
  {
    "model": "DiffDock",
    "organization": "Massachusetts Institute of Technology (MIT)",
    "publicationDate": "2022-10-04",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 20240000.0,
    "trainingComputeFlop": 7.2e+19,
    "confidence": "Likely",
    "link": [
      "https://arxiv.org/abs/2210.01776",
      "https://docs.nvidia.com/bionemo-framework/latest/models/diffdock.html"
    ],
    "reference": [
      "DiffDock: Diffusion Steps",
      "Twists",
      "and Turns for Molecular Docking"
    ],
    "citations": "587.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA RTX A6000",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Massachusetts Institute of Technology (MIT)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-10"
  },
  {
    "model": "Make-A-Video",
    "publicationDate": "2022-09-29",
    "domain": "Video",
    "task": "Video generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/2209.14792",
    "reference": "Make-A-Video: Text-to-Video Generation without Text-Video Data",
    "citations": "1717.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-09"
  },
  {
    "model": "Whisper",
    "organization": "OpenAI",
    "publicationDate": "2022-09-21",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": 1550000000.0,
    "trainingComputeFlop": 4.2072663e+21,
    "confidence": "Likely",
    "link": "https://cdn.openai.com/papers/whisper.pdf\n\nhttps://arxiv.org/abs/2212.04356",
    "reference": "Robust Speech Recognition via Large-Scale Weak Supervision",
    "citations": "5363.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-09"
  },
  {
    "model": "SauTech",
    "organization": [
      "Saudi Data and Artificial Intelligence Authority",
      "Saudi Company for Artificial Intelligence"
    ],
    "publicationDate": "2022-09-14",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://globalaisummit.org/en/News/Pages/NewsDetails.aspx?NewsId=73\nhttps://saudigazette.com.sa/article/624984/SAUDI-ARABIA/SDAIA-SCAI-unveil-SauTech-speech-to-text-software",
    "reference": "SDAIA and SCAI Unveil \"SauTech\" Speech-to-text Software",
    "citations": null,
    "organizationCategorization": [
      "Industry",
      "Government"
    ],
    "countryOfOrganization": "Saudi Arabia",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "Saudi Data and Artificial Intelligence Authority",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-09"
  },
  {
    "model": "PaLI",
    "organization": "Google",
    "publicationDate": "2022-09-14",
    "domain": "Language",
    "task": "Visual question answering",
    "parameters": 16900000000.0,
    "trainingComputeFlop": 1.69e+23,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2209.06794v4",
    "reference": "PaLI: A Jointly-Scaled Multilingual Language-Image Model",
    "citations": "879.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-09"
  },
  {
    "model": "BEIT-3",
    "organization": "Microsoft",
    "publicationDate": "2022-08-22",
    "domain": "Multimodal",
    "task": "Object detection",
    "parameters": 1900000000.0,
    "trainingComputeFlop": 7e+19,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2208.10442",
    "reference": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks",
    "citations": "699.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-08"
  },
  {
    "model": "BlenderBot 3",
    "publicationDate": "2022-08-10",
    "domain": "Language",
    "task": "Chat",
    "parameters": 175000000000.0,
    "trainingComputeFlop": 4.3e+23,
    "confidence": "Likely",
    "link": [
      "https://arxiv.org/abs/2208.03188",
      "https://github.com/facebookresearch/ParlAI/blob/main/parlai/zoo/bb3/model_card.md\n\ntraining code: https://parl.ai/projects/bb3/"
    ],
    "reference": "BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage",
    "citations": "265.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "Canada",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": "OPT-175B",
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "McGill University",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-08"
  },
  {
    "model": "GLM-130B",
    "organization": "Tsinghua University",
    "publicationDate": "2022-08-04",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 130000000000.0,
    "trainingComputeFlop": 3.5490054945e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2210.02414",
    "reference": "GLM-130B: An Open Bilingual Pre-trained Model",
    "citations": "1197.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-08"
  },
  {
    "model": "AlexaTM 20B",
    "organization": "Amazon",
    "publicationDate": "2022-08-02",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 19750000000.0,
    "trainingComputeFlop": 2.04374016e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2208.01448",
    "reference": "AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model",
    "citations": "89.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Amazon",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-08"
  },
  {
    "model": "OmegaPLM",
    "organization": [
      "Massachusetts Institute of Technology (MIT)",
      "Westlake University"
    ],
    "publicationDate": "2022-07-22",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 670000000.0,
    "trainingComputeFlop": 1.03514112e+22,
    "confidence": "Confident",
    "link": "https://www.biorxiv.org/content/10.1101/2022.07.21.500999v1",
    "reference": "High-resolution de novo structure prediction from primary sequence",
    "citations": "435.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "United States",
      "China"
    ],
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Massachusetts Institute of Technology (MIT)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-07"
  },
  {
    "model": "ESM2-15B",
    "publicationDate": "2022-07-21",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 15000000000.0,
    "trainingComputeFlop": 7.35000000001e+22,
    "confidence": "Confident",
    "link": "https://www.science.org/doi/abs/10.1126/science.ade2574\nhttps://www.biorxiv.org/content/10.1101/2022.07.20.500902v2",
    "reference": "Evolutionary-scale prediction of atomic-level protein structure with a language model",
    "citations": "636.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-07"
  },
  {
    "model": "BLOOM-176B",
    "organization": [
      "Hugging Face",
      "BigScience"
    ],
    "publicationDate": "2022-07-11",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 176247271424.0,
    "trainingComputeFlop": 3.65664e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2211.05100",
    "reference": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
    "citations": "2686.0",
    "organizationCategorization": [
      "Industry",
      "Research collective"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "Hugging Face",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-07"
  },
  {
    "model": "NLLB",
    "publicationDate": "2022-07-06",
    "domain": "Language",
    "task": "Translation",
    "parameters": 54500000000.0,
    "trainingComputeFlop": 1.751113728e+22,
    "confidence": "Confident",
    "link": "https://research.facebook.com/publications/no-language-left-behind/",
    "reference": "No Language Left Behind: Scaling Human-Centered Machine Translation",
    "citations": "1532.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-07"
  },
  {
    "model": "CodeT5-large",
    "organization": "Salesforce",
    "publicationDate": "2022-07-05",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 770000000.0,
    "trainingComputeFlop": 2.72e+21,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2207.01780",
    "reference": "CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning",
    "citations": "362.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Salesforce",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-07"
  },
  {
    "model": "Minerva (540B)",
    "organization": "Google",
    "publicationDate": "2022-06-29",
    "domain": "Language",
    "task": "Quantitative reasoning",
    "parameters": 540350000000.0,
    "trainingComputeFlop": 2.7415e+24,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2206.14858",
    "reference": "Solving Quantitative Reasoning Problems with Language Models",
    "citations": "1218.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": "PaLM (540B)",
    "modelAccessibility": "Unreleased",
    "frontierModel": "True",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-06"
  },
  {
    "model": "ProGen2-xlarge",
    "organization": [
      "Salesforce Research",
      "Columbia University",
      "Johns Hopkins University"
    ],
    "publicationDate": "2022-06-27",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 6400000000.0,
    "trainingComputeFlop": 1.35e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2206.13517",
    "reference": "ProGen2: Exploring the Boundaries of Protein Language Models",
    "citations": "396.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Salesforce Research",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-06"
  },
  {
    "model": "Parti",
    "publicationDate": "2022-06-22",
    "domain": "Image generation",
    "task": "Text-to-image",
    "parameters": 20000000000.0,
    "trainingComputeFlop": 5.09607936e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2206.10789v1",
    "reference": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation",
    "citations": "1316.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-06"
  },
  {
    "model": "CoCa",
    "publicationDate": "2022-06-14",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 2100000000.0,
    "trainingComputeFlop": 7.3e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2205.01917v2",
    "reference": "CoCa: Contrastive Captioners are Image-Text Foundation Models",
    "citations": "1556.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-06"
  },
  {
    "model": "MetaLM",
    "organization": "Microsoft Research",
    "publicationDate": "2022-06-13",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/2206.06336v1",
    "reference": "Language Models are General-Purpose Interfaces",
    "citations": "107.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Microsoft Research",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-06"
  },
  {
    "model": "DITTO",
    "organization": [
      "Tsinghua University",
      "Apple",
      "Westlake University",
      "Chinese University of Hong Kong (CUHK)"
    ],
    "publicationDate": "2022-06-06",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 750000000.0,
    "trainingComputeFlop": 3.31776e+18,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2206.02369",
    "reference": "Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation",
    "citations": "90.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "China",
      "United States",
      "Hong Kong"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-06"
  },
  {
    "model": "Diffusion-GAN",
    "organization": [
      "UT Austin",
      "Microsoft"
    ],
    "publicationDate": "2022-06-05",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/2206.02262v4",
    "reference": "Diffusion-GAN: Training GANs with Diffusion",
    "citations": "286.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "UT Austin",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-06"
  },
  {
    "model": "CogVideo",
    "organization": [
      "Tsinghua University",
      "Beijing Academy of Artificial Intelligence / BAAI"
    ],
    "publicationDate": "2022-05-29",
    "domain": "Video",
    "task": "Video generation",
    "parameters": 9400000000.0,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/2205.15868",
    "reference": "CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers",
    "citations": "821.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": null,
    "baseModel": "CogView2",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-05"
  },
  {
    "model": "Tranception",
    "organization": [
      "University of Oxford",
      "Harvard Medical School",
      "Cohere"
    ],
    "publicationDate": "2022-05-27",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 700000000.0,
    "trainingComputeFlop": 7.24e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2205.13760",
    "reference": "Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval",
    "citations": "223.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United Kingdom",
      "United States",
      "Canada"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "University of Oxford",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-05"
  },
  {
    "model": "GPT-2 Medium (FlashAttention)",
    "organization": [
      "Stanford University",
      "University at Buffalo"
    ],
    "publicationDate": "2022-05-27",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 355000000.0,
    "trainingComputeFlop": 8.9280922e+20,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2205.14135",
    "reference": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
    "citations": "3079.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Stanford University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-05"
  },
  {
    "model": "Imagen",
    "publicationDate": "2022-05-23",
    "domain": "Image generation",
    "task": "Text-to-image",
    "parameters": 7762000000.0,
    "trainingComputeFlop": 1.46e+22,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2205.11487",
    "reference": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
    "citations": "7213.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "Google",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-05"
  },
  {
    "model": "SimCSE",
    "organization": [
      "Princeton University",
      "Tsinghua University"
    ],
    "publicationDate": "2022-05-18",
    "domain": "Language",
    "task": "Semantic embedding",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2104.08821",
    "reference": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
    "citations": "3896.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "United States",
      "China"
    ],
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": "RoBERTa Large",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Princeton University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-05"
  },
  {
    "model": "Gato",
    "publicationDate": "2022-05-12",
    "domain": "Multimodal",
    "task": "Atari",
    "parameters": 1180000000.0,
    "trainingComputeFlop": 4.02e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2205.06175",
    "reference": "A Generalist Agent",
    "citations": "946.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-05"
  },
  {
    "model": "UL2",
    "publicationDate": "2022-05-10",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 20000000000.0,
    "trainingComputeFlop": 1.2e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2205.05131v1",
    "reference": "Unifying Language Learning Paradigms",
    "citations": "354.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Google",
    "department": [
      "Google Research",
      "Google Brain"
    ],
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-05"
  },
  {
    "model": "DeBERTaV3large + KEAR",
    "organization": "Microsoft",
    "publicationDate": "2022-05-04",
    "domain": "Language",
    "task": "Question answering",
    "parameters": 418000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2112.03254v3",
    "reference": "Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention",
    "citations": "62.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "DeBERTaV3large",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-05"
  },
  {
    "model": "OPT-175B",
    "publicationDate": "2022-05-02",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 175000000000.0,
    "trainingComputeFlop": 4.3e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2205.01068",
    "reference": "OPT: Open Pre-trained Transformer Language Models",
    "citations": "4260.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-05"
  },
  {
    "model": "Flamingo",
    "publicationDate": "2022-04-29",
    "domain": "Multimodal",
    "task": "Visual question answering",
    "parameters": 80000000000.0,
    "trainingComputeFlop": 2.18972000000001e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2204.14198",
    "reference": "Flamingo: a Visual Language Model for Few-Shot Learning",
    "citations": "4563.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": "Chinchilla",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-04"
  },
  {
    "model": "XMC-GAN",
    "publicationDate": "2022-04-14",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/2101.04702",
    "reference": "Cross-Modal Contrastive Learning for Text-to-Image Generation",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-04"
  },
  {
    "model": "Sparse all-MLP",
    "publicationDate": "2022-04-14",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 9410000000.0,
    "trainingComputeFlop": 5.32224e+20,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2203.06850",
    "reference": "Efficient Language Modeling with Sparse all-MLP",
    "citations": "15.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-04"
  },
  {
    "model": "Stable Diffusion (LDM-KL-8-G)",
    "organization": [
      "Runway",
      "Ludwig Maximilian University of Munich",
      "Heidelberg University"
    ],
    "publicationDate": "2022-04-13",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 1450000000.0,
    "trainingComputeFlop": 5.0000000000000004e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2112.10752",
    "reference": "High-Resolution Image Synthesis with Latent Diffusion Models",
    "citations": "19912.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Germany"
    ],
    "notabilityCriteria": "Significant use",
    "trainingHardware": "NVIDIA A100",
    "baseModel": "Stable Diffusion 1.2",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Runway",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-04"
  },
  {
    "model": "BERT-RBP",
    "organization": "Waseda University",
    "publicationDate": "2022-04-07",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 110000000.0,
    "trainingComputeFlop": 1.4e+20,
    "confidence": "Confident",
    "link": "https://academic.oup.com/bioinformaticsadvances/article/2/1/vbac023/6564689",
    "reference": "Prediction of RNA–protein interactions using a nucleotide language model",
    "citations": "65.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "Japan",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "DNABERT",
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Waseda University",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-04"
  },
  {
    "model": "DALL·E 2",
    "organization": "OpenAI",
    "publicationDate": "2022-04-06",
    "domain": "Image generation",
    "task": "Text-to-image",
    "parameters": 3500000000.0,
    "trainingComputeFlop": 3.3695784e+23,
    "confidence": "Speculative",
    "link": "https://cdn.openai.com/papers/dall-e-2.pdf",
    "reference": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
    "citations": "8041.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-04"
  },
  {
    "model": "PaLM (540B)",
    "publicationDate": "2022-04-04",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 540350000000.0,
    "trainingComputeFlop": 2.5272e+24,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2204.02311",
    "reference": "PaLM: Scaling Language Modeling with Pathways",
    "citations": "7224.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": "True",
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-04"
  },
  {
    "model": "Chinchilla",
    "publicationDate": "2022-03-29",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 70000000000.0,
    "trainingComputeFlop": 5.76e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2203.15556",
    "reference": "Training Compute-Optimal Large Language Models",
    "citations": "2494.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-03"
  },
  {
    "model": "Make-A-Scene",
    "publicationDate": "2022-03-24",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 4000000000.0,
    "trainingComputeFlop": 6.4172851e+21,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2203.13131",
    "reference": "Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-03"
  },
  {
    "model": [
      "Segatron-XL large",
      "M=384 + HCP"
    ],
    "organization": [
      "Microsoft Research",
      "University of Waterloo"
    ],
    "publicationDate": "2022-03-21",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 256999999.99999997,
    "trainingComputeFlop": 2.65e+19,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2203.10692",
    "reference": "Better Language Model with Hypernym Class Prediction",
    "citations": "16.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Canada"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Microsoft Research",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-03"
  },
  {
    "model": "ViT-G (model soup)",
    "publicationDate": "2022-03-10",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 1843000000.0,
    "trainingComputeFlop": 3.4e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2203.05482v3",
    "reference": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
    "citations": "1231.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "Israel"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "University of Washington",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-03"
  },
  {
    "model": "MegaSyn",
    "organization": "Collaborations Pharmaceuticals",
    "publicationDate": "2022-03-07",
    "domain": "Medicine",
    "task": "Drug discovery",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9544280/",
    "reference": "Dual Use of Artificial Intelligence-powered Drug Discovery",
    "citations": "148.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "Collaborations Pharmaceuticals",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-03"
  },
  {
    "model": "Statement Curriculum Learning",
    "organization": "OpenAI",
    "publicationDate": "2022-03-02",
    "domain": "Language",
    "task": "Automated theorem proving",
    "parameters": 774000000.0,
    "trainingComputeFlop": 1.7901648e+22,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2202.01344",
    "reference": "Formal Mathematics Statement Curriculum Learning",
    "citations": "142.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-03"
  },
  {
    "model": "DeepNet",
    "organization": "Microsoft Research",
    "publicationDate": "2022-03-01",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 3200000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2203.00555",
    "reference": [
      "DeepNet: Scaling Transformers to 1",
      "000 Layers"
    ],
    "citations": "188.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Microsoft Research",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-03"
  },
  {
    "model": "PolyCoder",
    "organization": "Carnegie Mellon University (CMU)",
    "publicationDate": "2022-02-26",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 2700000000.0,
    "trainingComputeFlop": 1.1e+21,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2202.13169",
    "reference": "A Systematic Evaluation of Large Language Models of Code",
    "citations": "766.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Quadro RTX 8000",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Carnegie Mellon University (CMU)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-02"
  },
  {
    "model": "FourCastNet",
    "organization": [
      "NVIDIA",
      "NERSC",
      "Lawrence Berkeley National Laboratory",
      "University of Michigan",
      "Rice University",
      "California Institute of Technology",
      "Purdue University"
    ],
    "publicationDate": "2022-02-22",
    "domain": "Earth science",
    "task": "Weather forecasting",
    "parameters": null,
    "trainingComputeFlop": 3.4504704e+20,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2202.11214",
    "reference": "FourCastNet: A Global Data-driven High-resolution Weather Model using Adaptive Fourier Neural Operators",
    "citations": null,
    "organizationCategorization": [
      "Industry",
      "Government",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-02"
  },
  {
    "model": "ST-MoE",
    "publicationDate": "2022-02-17",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 269000000000.0,
    "trainingComputeFlop": 2.9e+23,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2202.08906v2",
    "reference": "ST-MoE: Designing Stable and Transferable Sparse Expert Models",
    "citations": "287.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": [
      "Google Brain",
      "Google Research"
    ],
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-02"
  },
  {
    "model": "Midjourney V1",
    "organization": "Midjourney",
    "publicationDate": "2022-02-15",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": null,
    "reference": null,
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "Midjourney",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-02"
  },
  {
    "model": "ProteinBERT",
    "organization": [
      "Hebrew University of Jerusalem",
      "Ben-Gurion University of the Negev",
      "Deep Trading"
    ],
    "publicationDate": "2022-02-10",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 16000000.0,
    "trainingComputeFlop": 6.5e+19,
    "confidence": "Confident",
    "link": "https://academic.oup.com/bioinformatics/article/38/8/2102/6502274",
    "reference": "ProteinBERT: a universal deep-learning model of protein sequence and function",
    "citations": "747.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "Israel",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Quadro RTX 5000",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Hebrew University of Jerusalem",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-02"
  },
  {
    "model": "LaMDA",
    "organization": "Google",
    "publicationDate": "2022-02-10",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 137000000000.0,
    "trainingComputeFlop": 3.55e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2201.08239",
    "reference": "LaMDA: Language Models for Dialog Applications",
    "citations": "1755.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-02"
  },
  {
    "model": "GPT-NeoX-20B",
    "organization": "EleutherAI",
    "publicationDate": "2022-02-09",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 20000000000.0,
    "trainingComputeFlop": 9.31627008e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2204.06745",
    "reference": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model",
    "citations": "924.0",
    "organizationCategorization": "Research collective",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "EleutherAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-02"
  },
  {
    "model": "MaskGIT (ImageNet)",
    "publicationDate": "2022-02-08",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 227000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2202.04200",
    "reference": "MaskGIT: Masked Generative Image Transformer",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": null,
    "frontierModel": null,
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Unknown",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-02"
  },
  {
    "model": "RETRO-7B",
    "publicationDate": "2022-02-07",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 7500000000.0,
    "trainingComputeFlop": 1.68e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2112.04426",
    "reference": "Improving language models by retrieving from trillions of tokens",
    "citations": "1357.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-02"
  },
  {
    "model": "AlphaCode",
    "publicationDate": "2022-02-02",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 41100000000.0,
    "trainingComputeFlop": 2.38010000000001e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2203.07814",
    "reference": "Competition-Level Code Generation with AlphaCode",
    "citations": "1758.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-02"
  },
  {
    "model": "InstructGPT 175B",
    "organization": "OpenAI",
    "publicationDate": "2022-01-27",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 175000000000.0,
    "trainingComputeFlop": 3.19181e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/pdf/2203.02155",
    "reference": "Training language models to follow instructions with human feedback",
    "citations": "16617.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": null,
    "baseModel": "GPT-3 175B (davinci)",
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-01"
  },
  {
    "model": "InstructGPT 6B",
    "organization": "OpenAI",
    "publicationDate": "2022-01-27",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 6000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2203.02155",
    "reference": "Training language models to follow instructions with human feedback",
    "citations": "16617.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": null,
    "baseModel": "GPT-3 6.7B",
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-01"
  },
  {
    "model": "InstructGPT 1.3B",
    "organization": "OpenAI",
    "publicationDate": "2022-01-27",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 1300000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2203.02155",
    "reference": "Training language models to follow instructions with human feedback",
    "citations": "16617.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": null,
    "baseModel": "GPT-3 XL",
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-01"
  },
  {
    "model": "OntoProtein",
    "organization": "Zhejiang University (ZJU)",
    "publicationDate": "2022-01-23",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 420000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "link": "https://openreview.net/pdf?id=yfe1VMYAXa4",
    "reference": "ONTOPROTEIN: PROTEIN PRETRAINING WITH GENE ONTOLOGY EMBEDDING",
    "citations": null,
    "organizationCategorization": "Academia",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "ProtBERT-BFD",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Zhejiang University (ZJU)",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-01"
  },
  {
    "model": "AbLang (heavy sequences)",
    "organization": "University of Oxford",
    "publicationDate": "2022-01-22",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 355000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://academic.oup.com/bioinformaticsadvances/article/2/1/vbac046/6609807",
    "reference": "AbLang: an antibody language model for completing antibody sequences",
    "citations": "171.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United Kingdom",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "University of Oxford",
    "department": null,
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-01"
  },
  {
    "model": "data2vec (vision)",
    "publicationDate": "2022-01-20",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 705134592.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "link": "https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language/",
    "reference": [
      "Data2vec: A General Framework for Self-supervised Learning in Speech",
      "Vision and Language"
    ],
    "citations": "1000.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-01"
  },
  {
    "model": "data2vec (speech)",
    "publicationDate": "2022-01-20",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": 705134592.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "link": "https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language/",
    "reference": [
      "Data2vec: A General Framework for Self-supervised Learning in Speech",
      "Vision and Language"
    ],
    "citations": "1000.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-01"
  },
  {
    "model": "data2vec (language)",
    "publicationDate": "2022-01-20",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 705134592.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "link": "https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language/",
    "reference": [
      "Data2vec: A General Framework for Self-supervised Learning in Speech",
      "Vision and Language"
    ],
    "citations": "1000.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-01"
  },
  {
    "model": "Detic",
    "publicationDate": "2022-01-07",
    "domain": "Vision",
    "task": "Object detection",
    "parameters": 88000000.0,
    "trainingComputeFlop": 2.34399744e+19,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/2201.02605",
    "reference": "Detecting Twenty-thousand Classes using Image-level Supervision",
    "citations": "730.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-01"
  },
  {
    "model": "ERNIE-ViLG",
    "organization": "Baidu",
    "publicationDate": "2021-12-31",
    "domain": "Multimodal",
    "task": "Vision-language generation",
    "parameters": 10000000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "link": "https://arxiv.org/abs/2112.15283",
    "reference": "ERNIE-ViLG: Unified Generative Pre-training for Bidirectional Vision-Language Generation",
    "citations": "54.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Baidu",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-12"
  },
  {
    "model": "ERNIE 3.0 Titan",
    "organization": [
      "Baidu",
      "Peng Cheng Laboratory"
    ],
    "publicationDate": "2021-12-23",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 260000000000.0,
    "trainingComputeFlop": 1.0421e+24,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2112.12731",
    "reference": "ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation",
    "citations": "84.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 32 GB",
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": "True",
    "company": "Baidu",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-12"
  },
  {
    "model": "XGLM-7.5B",
    "publicationDate": "2021-12-20",
    "domain": "Language",
    "task": "Translation",
    "parameters": 7500000000.0,
    "trainingComputeFlop": 2.25e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2112.10668",
    "reference": "Few-shot Learning with Multilingual Language Models",
    "citations": "350.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Meta",
    "department": [
      "Meta AI",
      "Facebook AI Research"
    ],
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-12"
  },
  {
    "model": "LDM-1.45B",
    "organization": [
      "Heidelberg University",
      "Runway"
    ],
    "publicationDate": "2021-12-20",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 1450000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2112.10752",
    "reference": "High-Resolution Image Synthesis with Latent Diffusion Models",
    "citations": "19912.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "Germany",
      "United States"
    ],
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Heidelberg University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-12"
  },
  {
    "model": "Contriever",
    "publicationDate": "2021-12-16",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 110000000.0,
    "trainingComputeFlop": 1.57e+20,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2112.09118",
    "reference": "Unsupervised Dense Information Retrieval with Contrastive Learning",
    "citations": "1184.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "United Kingdom",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "BERT-Large",
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-12"
  },
  {
    "model": "LongT5",
    "publicationDate": "2021-12-15",
    "domain": "Language",
    "task": "Text summarization",
    "parameters": 3000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2112.07916",
    "reference": "LongT5: Efficient Text-To-Text Transformer for Long Sequences",
    "citations": "356.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-12"
  },
  {
    "model": "GLaM",
    "organization": "Google",
    "publicationDate": "2021-12-13",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 1200000000000.0,
    "trainingComputeFlop": 3.6363112434e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2112.06905",
    "reference": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts",
    "citations": "1005.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Massive (≥1T)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-12"
  },
  {
    "model": "Gopher (280B)",
    "publicationDate": "2021-12-08",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 280000000000.0,
    "trainingComputeFlop": 6.31e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2112.11446",
    "reference": [
      "\"Scaling Language Models: Methods",
      "Analysis & Insights from Training Gopher\""
    ],
    "citations": "1481.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": "True",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-12"
  },
  {
    "model": "Student of Games",
    "publicationDate": "2021-12-06",
    "domain": "Games",
    "task": "Chess",
    "parameters": null,
    "trainingComputeFlop": 3.6679273004682866e+22,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/2112.03178",
    "reference": "Player of Games",
    "citations": "28.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-12"
  },
  {
    "model": "T-NLRv5 XXL",
    "organization": "Microsoft",
    "publicationDate": "2021-12-03",
    "domain": "Language",
    "task": null,
    "parameters": 5400000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://www.marketscreener.com/quote/stock/MICROSOFT-CORPORATION-4835/news/Microsoft-Turing-NLRv5-achieves-new-performance-milestones-37207301/\nhttps://www.microsoft.com/en-us/research/blog/efficiently-and-effectively-scaling-up-language-model-pretraining-for-best-language-representation-model-on-glue-and-superglue/",
    "reference": "Microsoft : Turing-NLRv5 achieves new performance milestones",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-12"
  },
  {
    "model": "NÜWA",
    "organization": [
      "Microsoft Research",
      "Peking University"
    ],
    "publicationDate": "2021-11-24",
    "domain": "Multimodal",
    "task": "Image generation",
    "parameters": 870000000.0,
    "trainingComputeFlop": 7.24598784e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2111.12417",
    "reference": "NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion",
    "citations": "335.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "China"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Microsoft Research",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-11"
  },
  {
    "model": "Florence",
    "organization": "Microsoft",
    "publicationDate": "2021-11-22",
    "domain": "Vision",
    "task": "Image captioning",
    "parameters": 893000000.0,
    "trainingComputeFlop": 4.831e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2111.11432v1",
    "reference": "Florence: A New Foundation Model for Computer Vision",
    "citations": "1027.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-11"
  },
  {
    "model": "BASIC-L",
    "organization": "Google",
    "publicationDate": "2021-11-19",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 3070000000.0,
    "trainingComputeFlop": 4.12e+22,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2111.10050",
    "reference": "Combined Scaling for Zero-shot Transfer Learning",
    "citations": "223.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-11"
  },
  {
    "model": "Swin Transformer V2 (SwinV2-G)",
    "organization": "Microsoft Research Asia",
    "publicationDate": "2021-11-18",
    "domain": "Vision",
    "task": "Action recognition",
    "parameters": 3000000000.0,
    "trainingComputeFlop": 1.1e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2111.09883v2",
    "reference": "Swin Transformer V2: Scaling Up Capacity and Resolution",
    "citations": "2297.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Microsoft Research Asia",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-11"
  },
  {
    "model": "ViT-G/14 (LiT)",
    "publicationDate": "2021-11-15",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 3005000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2111.07991v3",
    "reference": "Zero-Shot Transfer with Locked-image Text Tuning",
    "citations": "644.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": "ViT-G/14",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-11"
  },
  {
    "model": "Masked Autoencoders ViT-H",
    "publicationDate": "2021-11-11",
    "domain": "Vision",
    "task": "Semantic segmentation",
    "parameters": 632000000.0,
    "trainingComputeFlop": 4.6e+20,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/2111.06377",
    "reference": "Masked Autoencoders Are Scalable Vision Learners",
    "citations": "9607.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": "ViT-Huge/14",
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-11"
  },
  {
    "model": "Projected GAN",
    "organization": "Heidelberg University",
    "publicationDate": "2021-11-01",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": 1.05e+19,
    "confidence": "Confident",
    "link": "https://proceedings.neurips.cc/paper/2021/hash/9219adc5c42107c4911e249155320648-Abstract.html",
    "reference": "Projected GANs Converge Faster",
    "citations": "275.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "Germany",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Heidelberg University",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2021,
    "yearMonth": "2021-11"
  },
  {
    "model": "CodeT5-base",
    "organization": [
      "Salesforce",
      "Nanyang Technological University"
    ],
    "publicationDate": "2021-11-01",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 220000000.0,
    "trainingComputeFlop": 1.56e+21,
    "confidence": "Likely",
    "link": "https://aclanthology.org/2021.emnlp-main.685/",
    "reference": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",
    "citations": "1903.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Singapore"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Salesforce",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-11"
  },
  {
    "model": "S4",
    "organization": "Stanford University",
    "publicationDate": "2021-10-31",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 249000000.00000003,
    "trainingComputeFlop": 7.8328627e+19,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2111.00396",
    "reference": "Efficiently Modeling Long Sequences with Structured State Spaces",
    "citations": "2647.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Stanford University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2021,
    "yearMonth": "2021-10"
  },
  {
    "model": "EfficientZero",
    "organization": [
      "Tsinghua University",
      "University of California (UC) Berkeley",
      "Shanghai Qi Zhi institute"
    ],
    "publicationDate": "2021-10-30",
    "domain": "Games",
    "task": "Atari",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/2111.00210",
    "reference": "Mastering Atari Games with Limited Data",
    "citations": "291.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "China",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2021,
    "yearMonth": "2021-10"
  },
  {
    "model": "Eve",
    "organization": [
      "Harvard Medical School",
      "University of Oxford"
    ],
    "publicationDate": "2021-10-27",
    "domain": "Biology",
    "task": "Protein pathogenicity prediction",
    "parameters": 15010300.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "link": "https://www.nature.com/articles/s41586-021-04043-8#change-history",
    "reference": "Disease variant prediction with deep generative models of evolutionary data",
    "citations": "618.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "United States",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Harvard Medical School",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2021,
    "yearMonth": "2021-10"
  },
  {
    "model": "base LM+GNN+kNN",
    "organization": [
      "Shannon.AI",
      "Nanjing University",
      "Nanyang Technological University",
      "Zhejiang University (ZJU)"
    ],
    "publicationDate": "2021-10-17",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 274000000.0,
    "trainingComputeFlop": 5.2587456e+19,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2110.08743",
    "reference": "GNN-LM: Language Modeling based on Global Contexts via GNN",
    "citations": "44.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "China",
      "Singapore"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Transformer (Adaptive Input Embeddings) WT103",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Shannon.AI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-10"
  },
  {
    "model": "Yuan 1.0",
    "organization": "Inspur",
    "publicationDate": "2021-10-12",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 245730000000.0,
    "trainingComputeFlop": 3.5380000000001e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2110.04725",
    "reference": "Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning",
    "citations": "64.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": "True",
    "company": "Inspur",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-10"
  },
  {
    "model": "Megatron-Turing NLG 530B",
    "organization": [
      "Microsoft",
      "NVIDIA"
    ],
    "publicationDate": "2021-10-11",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 530000000000.0,
    "trainingComputeFlop": 8.586e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2201.11990",
    "reference": [
      "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B",
      "A Large-Scale Generative Language Model"
    ],
    "citations": "800.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": "True",
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-10"
  },
  {
    "model": "AlphaFold-Multimer",
    "publicationDate": "2021-10-04",
    "domain": "Biology",
    "task": "Protein folding prediction",
    "parameters": null,
    "trainingComputeFlop": 4.35e+21,
    "confidence": "Confident",
    "link": "https://www.biorxiv.org/content/10.1101/2021.10.04.463034v1",
    "reference": "Protein complex prediction with AlphaFold-Multimer",
    "citations": "2639.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": "AlphaFold 2",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-10"
  },
  {
    "model": "Turing ULRv5",
    "organization": "Microsoft",
    "publicationDate": "2021-09-28",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 2200000000.0,
    "trainingComputeFlop": 2.8983951e+22,
    "confidence": "Confident",
    "link": "https://www.microsoft.com/en-us/research/blog/microsoft-turing-universal-language-representation-model-t-ulrv5-tops-xtreme-leaderboard-and-trains-100x-faster/",
    "reference": [
      "Microsoft Turing Universal Language Representation model",
      "T-ULRv5",
      "tops XTREME leaderboard and trains 100x faster"
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-09"
  },
  {
    "model": "TrOCR",
    "organization": [
      "Beihang University",
      "Microsoft Research Asia"
    ],
    "publicationDate": "2021-09-21",
    "domain": "Vision",
    "task": "Character recognition (OCR)",
    "parameters": 558000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2109.10282",
    "reference": "TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models",
    "citations": "462.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Beihang University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-09"
  },
  {
    "model": "PLATO-XL",
    "organization": "Baidu",
    "publicationDate": "2021-09-20",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 11000000000.0,
    "trainingComputeFlop": 9.9e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2109.09519",
    "reference": "PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation",
    "citations": "69.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Baidu",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-09"
  },
  {
    "model": "HyperCLOVA 204B",
    "organization": "NAVER",
    "publicationDate": "2021-09-10",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 204000000000.0,
    "trainingComputeFlop": 2.0000000001e+23,
    "confidence": "Speculative",
    "link": "https://aibusiness.com/nlp/south-korea-s-naver-unveils-hyperscale-ai-platform-language-model-with-more-parameters-than-gpt-3",
    "reference": [
      "South Korea's Naver unveils 'hyperscale AI' platform",
      "language model with more parameters than GPT-3"
    ],
    "citations": "92.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Korea (Republic of)",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": "True",
    "company": "NAVER",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-09"
  },
  {
    "model": "PermuteFormer",
    "organization": "Peking University",
    "publicationDate": "2021-09-06",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 149697024.0,
    "trainingComputeFlop": 2.775e+18,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/2109.02377",
    "reference": "PermuteFormer: Efficient Relative Position Encoding for Long Sequences",
    "citations": "22.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Peking University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2021,
    "yearMonth": "2021-09"
  },
  {
    "model": "MEB",
    "organization": "Microsoft",
    "publicationDate": "2021-09-04",
    "domain": "Search",
    "task": "Search",
    "parameters": 135000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://www.microsoft.com/en-us/research/blog/make-every-feature-binary-a-135b-parameter-sparse-neural-network-for-massively-improved-search-relevance/",
    "reference": "Make Every feature Binary: A 135B parameter sparse neural network for massively improved search relevance",
    "citations": "26.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-09"
  },
  {
    "model": "FLAN 137B",
    "publicationDate": "2021-09-03",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 137000000000.0,
    "trainingComputeFlop": 2.047e+24,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2109.01652",
    "reference": "Finetuned Language Models Are Zero-Shot Learners",
    "citations": "4461.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": "LaMDA",
    "modelAccessibility": "Unreleased",
    "frontierModel": "True",
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-09"
  },
  {
    "model": "XLMR-XXL",
    "publicationDate": "2021-08-17",
    "domain": "Language",
    "task": "Translation",
    "parameters": 10700000000.0,
    "trainingComputeFlop": 3.366e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2105.00572",
    "reference": "Larger-Scale Transformers for Multilingual Masked Language Modeling",
    "citations": "142.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-08"
  },
  {
    "model": "DNABERT",
    "organization": "Northeastern University",
    "publicationDate": "2021-08-15",
    "domain": "Biology",
    "task": "Protein or nucleotide language model (pLM/nLM)",
    "parameters": 110000000.0,
    "trainingComputeFlop": 1.07e+20,
    "confidence": "Confident",
    "link": "https://academic.oup.com/bioinformatics/article/37/15/2112/6128680",
    "reference": "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome",
    "citations": "923.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA GeForce RTX 2080 Ti 11GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Northeastern University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2021,
    "yearMonth": "2021-08"
  },
  {
    "model": "Zidong Taichu",
    "organization": [
      "Chinese Academy of Sciences",
      "Wuhan AI Computing Center"
    ],
    "publicationDate": "2021-08-11",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": 3200000000.0,
    "trainingComputeFlop": 8.016e+20,
    "confidence": "Confident",
    "link": "https://gitee.com/zidongtaichu/multi-modal-models",
    "reference": "Zidong Ancestral multi-modal large model",
    "citations": null,
    "organizationCategorization": [
      "Academia",
      "Government"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Chinese Academy of Sciences",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-08"
  },
  {
    "model": "Jurassic-1-Jumbo",
    "organization": "AI21 Labs",
    "publicationDate": "2021-08-11",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 178000000000.0,
    "trainingComputeFlop": 3.7e+23,
    "confidence": "Confident",
    "link": "https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf",
    "reference": "Jurassic-1: Technical Details and Evaluation",
    "citations": "55.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Israel",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": "True",
    "company": "AI21 Labs",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-08"
  },
  {
    "model": "W2v-BERT",
    "publicationDate": "2021-08-07",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": 1000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2108.06209v2",
    "reference": "W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training",
    "citations": "485.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-08"
  },
  {
    "model": "YOLOX-X",
    "organization": "Megvii Inc",
    "publicationDate": "2021-08-06",
    "domain": "Vision",
    "task": "Object detection",
    "parameters": 99100000.0,
    "trainingComputeFlop": 6.34275e+20,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2107.08430",
    "reference": "YOLOX: Exceeding YOLO Series in 2021",
    "citations": "5123.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Megvii Inc",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-08"
  },
  {
    "model": "6-Act Tether",
    "publicationDate": "2021-08-03",
    "domain": "Robotics",
    "task": "Object detection",
    "parameters": 5000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://openaccess.thecvf.com/content/ICCV2021/html/Ye_Auxiliary_Tasks_and_Exploration_Enable_ObjectGoal_Navigation_ICCV_2021_paper.html\nhttps://arxiv.org/abs/2104.04112",
    "reference": "Auxiliary Tasks and Exploration Enable ObjectGoal Navigation",
    "citations": "114.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-08"
  },
  {
    "model": "SEER",
    "publicationDate": "2021-07-29",
    "domain": "Vision",
    "task": "Image embedding",
    "parameters": 1300000000.0,
    "trainingComputeFlop": 1.8e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2103.01988",
    "reference": "Self-supervised Pretraining of Visual Features in the Wild",
    "citations": "294.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-07"
  },
  {
    "model": "HuBERT",
    "publicationDate": "2021-07-27",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": 1000000000.0,
    "trainingComputeFlop": 5.54e+21,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/2106.07447",
    "reference": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
    "citations": "3813.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-07"
  },
  {
    "model": "GOAT",
    "publicationDate": "2021-07-27",
    "domain": "Games",
    "task": "Open ended play",
    "parameters": 3472816.0,
    "trainingComputeFlop": 2.412e+22,
    "confidence": "Speculative",
    "link": "https://deepmind.com/blog/article/generally-capable-agents-emerge-from-open-ended-play\n\nhttps://arxiv.org/abs/2107.12808",
    "reference": "Open-Ended Learning Leads to Generally Capable Agents",
    "citations": "214.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-07"
  },
  {
    "model": "Codex",
    "organization": "OpenAI",
    "publicationDate": "2021-07-07",
    "domain": "Language",
    "task": "Code autocompletion",
    "parameters": 12000000000.0,
    "trainingComputeFlop": 7.344e+22,
    "confidence": "Likely",
    "link": "https://openai.com/blog/openai-codex/\nhttps://arxiv.org/abs/2107.03374",
    "reference": "Evaluating Large Language Models Trained on Code",
    "citations": "7283.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": "GPT-3 13B",
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-07"
  },
  {
    "model": "ERNIE 3.0",
    "organization": "Baidu",
    "publicationDate": "2021-07-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 10000000000.0,
    "trainingComputeFlop": 2.25e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2107.02137",
    "reference": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation",
    "citations": "548.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Baidu",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-07"
  },
  {
    "model": "Adaptive Input Transformer + RD",
    "organization": [
      "Microsoft Research Asia",
      "Soochow University"
    ],
    "publicationDate": "2021-06-28",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 247000000.00000003,
    "trainingComputeFlop": 8.58e+19,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2106.14448",
    "reference": "R-Drop: Regularized Dropout for Neural Networks",
    "citations": "499.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "China",
      "Taiwan"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Microsoft Research Asia",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-06"
  },
  {
    "model": "Fold2Seq",
    "organization": [
      "IBM",
      "Texas A&M"
    ],
    "publicationDate": "2021-06-24",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 12427904.0,
    "trainingComputeFlop": 1.4e+17,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/2106.13058",
    "reference": "Fold2Seq: A Joint Sequence(1D)-Fold(3D) Embedding-based Generative Model for Protein Design",
    "citations": "52.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Tesla K80",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "IBM",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-06"
  },
  {
    "model": "EfficientNetV2-XL",
    "publicationDate": "2021-06-23",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 208000000.0,
    "trainingComputeFlop": 9.56e+19,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2104.00298",
    "reference": "EfficientNetV2: Smaller Models and Faster Training",
    "citations": "3479.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Google",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-06"
  },
  {
    "model": "StyleGAN3-T",
    "organization": [
      "NVIDIA",
      "Aalto University"
    ],
    "publicationDate": "2021-06-21",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 2230000.0,
    "trainingComputeFlop": 1.70208e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2106.12423",
    "reference": "Alias-Free Generative Adversarial Networks",
    "citations": "2031.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Finland"
    ],
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-06"
  },
  {
    "model": "StyleGAN3-R",
    "organization": [
      "NVIDIA",
      "Aalto University"
    ],
    "publicationDate": "2021-06-21",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 1580000.0,
    "trainingComputeFlop": 2.42784e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/pdf/2106.12423",
    "reference": "Alias-Free Generative Adversarial Networks",
    "citations": "2031.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Finland"
    ],
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-06"
  },
  {
    "model": "Denoising Diffusion Probabilistic Models (LSUN Bedroom)",
    "organization": "University of California (UC) Berkeley",
    "publicationDate": "2021-06-11",
    "domain": "Vision",
    "task": "Image generation",
    "parameters": 256000000.0,
    "trainingComputeFlop": 7.840125000000001e+19,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2006.11239",
    "reference": "Denoising Diffusion Probabilistic Models",
    "citations": "24183.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "University of California (UC) Berkeley",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2021,
    "yearMonth": "2021-06"
  },
  {
    "model": "ALIGN",
    "publicationDate": "2021-06-11",
    "domain": "Multimodal",
    "task": "Representation learning",
    "parameters": 820000000.0,
    "trainingComputeFlop": 2.598670000001e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2102.05918",
    "reference": "Scaling up visual and vision-language representation learning with noisy text supervision",
    "citations": "4691.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-06"
  },
  {
    "model": "DeBERTa",
    "organization": "Microsoft",
    "publicationDate": "2021-06-10",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 1500000000.0,
    "trainingComputeFlop": 2.588e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2006.03654",
    "reference": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
    "citations": "3247.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-06"
  },
  {
    "model": "EMDR",
    "publicationDate": "2021-06-09",
    "domain": "Language",
    "task": "Question answering",
    "parameters": 440000000.0,
    "trainingComputeFlop": 1.91e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2106.05346v2",
    "reference": "End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering",
    "citations": "179.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "Canada",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-06"
  },
  {
    "model": "CoAtNet",
    "publicationDate": "2021-06-09",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 2440000000.0,
    "trainingComputeFlop": 4.27e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2106.04803v2",
    "reference": "CoAtNet: Marrying Convolution and Attention\nfor All Data Sizes",
    "citations": "1422.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": [
      "Google Research",
      "Google Brain"
    ],
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-06"
  },
  {
    "model": "ViT-G/14",
    "publicationDate": "2021-06-08",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 1843000000.0,
    "trainingComputeFlop": 5.85e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2106.04560",
    "reference": "Scaling Vision Transformers",
    "citations": "1266.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": [
      "Google Brain",
      "Google Research"
    ],
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-06"
  },
  {
    "model": "ByT5-XXL",
    "publicationDate": "2021-05-28",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 12900000000.0,
    "trainingComputeFlop": 8.1e+22,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2105.13626",
    "reference": "ByT5: Towards a token-free future with pre-trained byte-to-byte models",
    "citations": "588.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-05"
  },
  {
    "model": "Transformer local-attention (NesT-B)",
    "publicationDate": "2021-05-26",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 90100000.0,
    "trainingComputeFlop": 2.40576e+19,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2105.12723v4",
    "reference": [
      "Nested Hierarchical Transformer: Towards Accurate",
      "Data-Efficient and Interpretable Visual Understanding"
    ],
    "citations": "5734.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Google Cloud",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-05"
  },
  {
    "model": "CogView",
    "organization": [
      "Tsinghua University",
      "Alibaba DAMO Academy"
    ],
    "publicationDate": "2021-05-26",
    "domain": "Image generation",
    "task": "Text-to-image",
    "parameters": 4000000000.0,
    "trainingComputeFlop": 2.68e+22,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2105.13290",
    "reference": "CogView: Mastering Text-to-Image Generation via Transformers",
    "citations": "896.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 16 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-05"
  },
  {
    "model": "MedBERT",
    "organization": [
      "Peng Cheng Laboratory",
      "University of Texas at Houston"
    ],
    "publicationDate": "2021-05-20",
    "domain": "Medicine",
    "task": "Medical diagnosis",
    "parameters": 17000000.0,
    "trainingComputeFlop": 9.47e+18,
    "confidence": "Likely",
    "link": "https://www.nature.com/articles/s41746-021-00455-y",
    "reference": "Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction",
    "citations": "813.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "China",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 32 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Peng Cheng Laboratory",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2021,
    "yearMonth": "2021-05"
  },
  {
    "model": "ADM",
    "organization": "OpenAI",
    "publicationDate": "2021-05-11",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 559000000.0,
    "trainingComputeFlop": 6.2e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2105.05233",
    "reference": "Diffusion Models Beat GANs on Image Synthesis",
    "citations": "9833.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-05"
  },
  {
    "model": "ProtT5-XL-U50",
    "organization": [
      "Technical University of Munich",
      "Med AI Technology",
      "NVIDIA",
      "Oak Ridge National Laboratory",
      "Google",
      "Seoul National University"
    ],
    "publicationDate": "2021-05-04",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 3000000000.0,
    "trainingComputeFlop": 1.8704498688e+22,
    "confidence": "Confident",
    "link": "https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3.full.pdf",
    "reference": "ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Learning",
    "citations": null,
    "organizationCategorization": [
      "Academia",
      "Industry",
      "Government"
    ],
    "countryOfOrganization": [
      "Germany",
      "China",
      "United States",
      "Korea (Republic of)"
    ],
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Technical University of Munich",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-05"
  },
  {
    "model": "ProtBERT-BFD",
    "organization": [
      "Technical University of Munich",
      "NVIDIA",
      "Seoul National University",
      "Google",
      "Oak Ridge National Laboratory",
      "Med AI Technology"
    ],
    "publicationDate": "2021-05-04",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 420000000.0,
    "trainingComputeFlop": 3.9e+22,
    "confidence": "Confident",
    "link": "https://www.biorxiv.org/content/10.1101/2020.07.12.199554v3 or \nhttps://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9477085",
    "reference": "ProtTrans:Towards Cracking the Language of Life's Code Through Self-Supervised Learning",
    "citations": null,
    "organizationCategorization": [
      "Academia",
      "Industry",
      "Government"
    ],
    "countryOfOrganization": [
      "Germany",
      "United States",
      "Korea (Republic of)",
      "China"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Technical University of Munich",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-05"
  },
  {
    "model": "PLUG",
    "organization": "Alibaba",
    "publicationDate": "2021-04-19",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 27000000000.0,
    "trainingComputeFlop": 3.5997696e+22,
    "confidence": "Confident",
    "link": "https://mp.weixin.qq.com/s/DAQomIkDa52Sef-ruyH5qg",
    "reference": null,
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-04"
  },
  {
    "model": "Unicorn",
    "organization": "Allen Institute for AI",
    "publicationDate": "2021-03-24",
    "domain": "Language",
    "task": "Question answering",
    "parameters": 11000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2103.13009",
    "reference": "UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark",
    "citations": null,
    "organizationCategorization": "Research collective",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "T5-11B",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Allen Institute for AI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-03"
  },
  {
    "model": "M6-T",
    "organization": "Alibaba",
    "publicationDate": "2021-03-05",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 1002700000000.0,
    "trainingComputeFlop": 5.5e+21,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2105.15082",
    "reference": "M6-T: Exploring Sparse Expert Models and Beyond",
    "citations": "76.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 32 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Massive (≥1T)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-03"
  },
  {
    "model": "Generative BST",
    "publicationDate": "2021-03-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 9431810048.0,
    "trainingComputeFlop": 1.449e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2004.13637",
    "reference": "Recipes for building an open-domain chatbot",
    "citations": "1072.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-03"
  },
  {
    "model": "Meta Pseudo Labels",
    "publicationDate": "2021-03-01",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 480000000.0,
    "trainingComputeFlop": 4.79e+22,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2003.10580",
    "reference": "Meta pseudo labels",
    "citations": "725.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-03"
  },
  {
    "model": "SRU++ Large",
    "organization": "ASAPP",
    "publicationDate": "2021-02-24",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 234000000.0,
    "trainingComputeFlop": 2.1173704e+19,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2102.12459",
    "reference": "When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute",
    "citations": "54.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "ASAPP",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-02"
  },
  {
    "model": "Rational DQN Average",
    "organization": "TU Darmstadt",
    "publicationDate": "2021-02-18",
    "domain": "Games",
    "task": "Atari",
    "parameters": 1683456.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "link": "https://arxiv.org/abs/2102.09407v3",
    "reference": "Recurrent Rational Networks",
    "citations": "8.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "Germany",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "TU Darmstadt",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2021,
    "yearMonth": "2021-02"
  },
  {
    "model": "MSA Transformer",
    "publicationDate": "2021-02-13",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 100000000.0,
    "trainingComputeFlop": 5.49e+21,
    "confidence": "Likely",
    "link": "https://proceedings.mlr.press/v139/rao21a/rao21a.pdf",
    "reference": "MSA Transformer",
    "citations": "630.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 32 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-02"
  },
  {
    "model": "top-down frozen classifier",
    "organization": [
      "University of Edinburgh",
      "Toshiba Cambridge Research Laboratory"
    ],
    "publicationDate": "2021-02-09",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/2102.04697",
    "reference": "Train your classifier first: Cascade Neural Networks Training from upper layers to lower layers",
    "citations": "2.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United Kingdom",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "University of Edinburgh",
    "department": null,
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-02"
  },
  {
    "model": "DLWP",
    "organization": [
      "University of Washington",
      "Microsoft Research"
    ],
    "publicationDate": "2021-02-09",
    "domain": "Earth science",
    "task": "Weather forecasting",
    "parameters": 2676376.0,
    "trainingComputeFlop": 5.6845152e+18,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2102.05107",
    "reference": "Sub-seasonal forecasting with a large ensemble of deep-learning weather prediction models",
    "citations": null,
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 32 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "University of Washington",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-02"
  },
  {
    "model": "DeiT-B",
    "publicationDate": "2021-01-15",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 86000000.0,
    "trainingComputeFlop": 7.884e+19,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2012.12877",
    "reference": "Training data-efficient image transformers & distillation through attention",
    "citations": "8013.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-01"
  },
  {
    "model": "Switch",
    "organization": "Google",
    "publicationDate": "2021-01-11",
    "domain": "Language",
    "task": "Text autocompletion",
    "parameters": 1571000000000.0,
    "trainingComputeFlop": 8.22e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2101.03961",
    "reference": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
    "citations": "2895.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": "True",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Massive (≥1T)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-01"
  },
  {
    "model": "BigSSL",
    "organization": [
      "Google",
      "Apple"
    ],
    "publicationDate": "2021-01-10",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": 8000000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "link": "https://arxiv.org/abs/2109.13226",
    "reference": "BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition",
    "citations": "151.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-01"
  },
  {
    "model": "DALL-E",
    "organization": "OpenAI",
    "publicationDate": "2021-01-05",
    "domain": "Image generation",
    "task": "Text-to-image",
    "parameters": 12000000000.0,
    "trainingComputeFlop": 4.7e+22,
    "confidence": "Likely",
    "link": "https://openai.com/blog/dall-e/\n\nhttps://arxiv.org/abs/2102.12092",
    "reference": "Zero-Shot Text-to-Image Generation",
    "citations": "5794.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 16 GB",
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-01"
  },
  {
    "model": "CLIP (ViT L/14@336px)",
    "organization": "OpenAI",
    "publicationDate": "2021-01-05",
    "domain": "Multimodal",
    "task": "Zero-shot image classification",
    "parameters": 370000000.0,
    "trainingComputeFlop": 1.05e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2103.00020\nhttps://huggingface.co/openai/clip-vit-large-patch14-336",
    "reference": "Learning Transferable Visual Models From Natural Language Supervision",
    "citations": "38980.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-01"
  },
  {
    "model": "CLIP (ResNet-50)",
    "organization": "OpenAI",
    "publicationDate": "2021-01-05",
    "domain": "Multimodal",
    "task": "Zero-shot image classification",
    "parameters": 88600000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2103.00020",
    "reference": "Learning Transferable Visual Models From Natural Language Supervision",
    "citations": "38980.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-01"
  },
  {
    "model": "ERNIE-Doc (247M)",
    "organization": "Baidu",
    "publicationDate": "2020-12-31",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 247000000.00000003,
    "trainingComputeFlop": 3.0302798e+19,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2012.15688",
    "reference": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer",
    "citations": "62.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Baidu",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-12"
  },
  {
    "model": "CT-MoS (WT2)",
    "organization": [
      "Google",
      "National Tsing Hua University"
    ],
    "publicationDate": "2020-12-25",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 45000000.0,
    "trainingComputeFlop": 5.4e+17,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2012.13575",
    "reference": "Contextual Temperature for Language Modeling",
    "citations": "33.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Taiwan"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA GeForce GTX 1080 Ti",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-12"
  },
  {
    "model": "DensePhrases",
    "organization": [
      "Korea University",
      "Princeton University"
    ],
    "publicationDate": "2020-12-23",
    "domain": "Language",
    "task": "Question answering",
    "parameters": null,
    "trainingComputeFlop": 2.09952e+18,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/2012.12624v3",
    "reference": "Learning Dense Representations of Phrases at Scale",
    "citations": "125.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "Korea (Republic of)",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA TITAN Xp",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Korea University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2020,
    "yearMonth": "2020-12"
  },
  {
    "model": "VQGAN + CLIP",
    "organization": "Heidelberg University",
    "publicationDate": "2020-12-17",
    "domain": "Image generation",
    "task": "Text-to-image",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/2012.09841",
    "reference": "Taming Transformers for High-Resolution Image Synthesis",
    "citations": "3623.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "Germany",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Heidelberg University",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2020,
    "yearMonth": "2020-12"
  },
  {
    "model": "ESM1b",
    "publicationDate": "2020-12-15",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 652400000.0,
    "trainingComputeFlop": 5.1e+21,
    "confidence": "Confident",
    "link": "https://www.pnas.org/doi/abs/10.1073/pnas.2016239118",
    "reference": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
    "citations": "2556.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-12"
  },
  {
    "model": "CPM-Large",
    "organization": [
      "Tsinghua University",
      "Beijing Academy of Artificial Intelligence / BAAI"
    ],
    "publicationDate": "2020-12-01",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 2600000000.0,
    "trainingComputeFlop": 2.6052e+20,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2012.00413",
    "reference": "CPM: A Large-scale Generative Chinese Pre-trained Language Model",
    "citations": "127.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2020,
    "yearMonth": "2020-12"
  },
  {
    "model": "AlphaFold 2",
    "publicationDate": "2020-11-30",
    "domain": "Biology",
    "task": "Protein folding prediction",
    "parameters": 93000000.0,
    "trainingComputeFlop": 2.99e+21,
    "confidence": "Likely",
    "link": "https://www.nature.com/articles/s41586-021-03819-2",
    "reference": "Highly accurate protein structure prediction with AlphaFold",
    "citations": "31052.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-11"
  },
  {
    "model": "KEPLER",
    "organization": [
      "Tsinghua University",
      "Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",
      "HEC",
      "CIFAR AI Research",
      "Princeton University",
      "University of Montreal / Université de Montréal"
    ],
    "publicationDate": "2020-11-23",
    "domain": "Language",
    "task": "Relation extraction",
    "parameters": 125000000.0,
    "trainingComputeFlop": 1.66e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1911.06136",
    "reference": "KEPLER: A Unified Model for Knowledge Embedding and Pre- trained Language Representation.",
    "citations": "753.0",
    "organizationCategorization": [
      "Academia",
      "Research collective"
    ],
    "countryOfOrganization": [
      "China",
      "Canada",
      "France",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "RoBERTa Base",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-11"
  },
  {
    "model": "wave2vec 2.0 LARGE",
    "organization": "Facebook",
    "publicationDate": "2020-10-22",
    "domain": "Speech",
    "task": "Speech completion",
    "parameters": 317000000.0,
    "trainingComputeFlop": 3.87072e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2006.11477",
    "reference": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
    "citations": "7116.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 32 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-10"
  },
  {
    "model": "ViT-Huge/14",
    "publicationDate": "2020-10-22",
    "domain": "Vision",
    "task": "Image representation",
    "parameters": 632000000.0,
    "trainingComputeFlop": 4.262e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2010.11929",
    "reference": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "citations": "52499.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Google",
    "department": [
      "Google Brain",
      "Google Research"
    ],
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-10"
  },
  {
    "model": "ViT-Base/32",
    "publicationDate": "2020-10-22",
    "domain": "Vision",
    "task": "Image representation",
    "parameters": 86000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "link": "https://arxiv.org/abs/2010.11929",
    "reference": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "citations": "52499.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Google",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-10"
  },
  {
    "model": "German ELECTRA Large",
    "organization": [
      "deepset",
      "Bayerische Staatsbibliothek Muenchen"
    ],
    "publicationDate": "2020-10-21",
    "domain": "Language",
    "task": "Document classification",
    "parameters": 335000000.0,
    "trainingComputeFlop": 1.42829568e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2010.10906",
    "reference": "German's Next Language Model",
    "citations": "304.0",
    "organizationCategorization": [
      "Industry",
      "Government"
    ],
    "countryOfOrganization": "Germany",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "deepset",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-10"
  },
  {
    "model": "mT5-XXL",
    "publicationDate": "2020-10-20",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 13000000000.0,
    "trainingComputeFlop": 8.2e+22,
    "confidence": "Confident",
    "link": "https://aclanthology.org/2021.naacl-main.41/",
    "reference": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
    "citations": "2843.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": "True",
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-10"
  },
  {
    "model": "Conformer + Wav2vec 2.0 + Noisy Student",
    "publicationDate": "2020-10-20",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": 1000000000.0,
    "trainingComputeFlop": 7.6e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2010.10504v2",
    "reference": "Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition",
    "citations": "324.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": [
      "Google Research",
      "Google Brain"
    ],
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-10"
  },
  {
    "model": "LUKE",
    "organization": [
      "University of Washington",
      "National Institute of Informatics"
    ],
    "publicationDate": "2020-10-02",
    "domain": "Language",
    "task": "Question answering",
    "parameters": 483000000.0,
    "trainingComputeFlop": 1.8144e+22,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2010.01057v1",
    "reference": "LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention",
    "citations": "713.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "United States",
      "Japan"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": "RoBERTa Large",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "University of Washington",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2020,
    "yearMonth": "2020-10"
  },
  {
    "model": "ProBERTa",
    "organization": [
      "University of Illinois Urbana-Champaign (UIUC)",
      "Reed College"
    ],
    "publicationDate": "2020-09-01",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 44000000.0,
    "trainingComputeFlop": 9.72e+18,
    "confidence": "Confident",
    "link": "https://dl.acm.org/doi/10.1145/3388440.3412467",
    "reference": "Transforming the Language of Life: Transformer Neural Networks for Protein Prediction Tasks",
    "citations": "97.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "University of Illinois Urbana-Champaign (UIUC)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2020,
    "yearMonth": "2020-09"
  },
  {
    "model": "ERNIE-GEN (large)",
    "organization": "Baidu",
    "publicationDate": "2020-08-06",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 340000000.0,
    "trainingComputeFlop": 2e+20,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/2001.11314",
    "reference": "ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation",
    "citations": "132.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Baidu",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-08"
  },
  {
    "model": "DeLighT",
    "publicationDate": "2020-08-03",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 99000000.0,
    "trainingComputeFlop": 3.8016e+18,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2008.00623",
    "reference": "DeLighT: Deep and Light-weight Transformer",
    "citations": "98.0",
    "organizationCategorization": [
      "Academia",
      "Research collective",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "University of Washington",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-08"
  },
  {
    "model": "EfficientDet",
    "publicationDate": "2020-07-27",
    "domain": "Vision",
    "task": "Object detection",
    "parameters": 77000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "link": "https://arxiv.org/abs/1911.09070",
    "reference": "EfficientDet: Scalable and Efficient Object Detection",
    "citations": "4701.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Google",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-07"
  },
  {
    "model": "Hopfield Networks (2020)",
    "organization": [
      "Johannes Kepler University Linz",
      "Institute of Advanced Research in Artificial Intelligence",
      "University of Oslo"
    ],
    "publicationDate": "2020-07-16",
    "domain": "Biology",
    "task": "Drug discovery",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/2008.02217",
    "reference": "Hopfield Networks is All You Need",
    "citations": "517.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "Austria",
      "Norway"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Johannes Kepler University Linz",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2020,
    "yearMonth": "2020-07"
  },
  {
    "model": "SemExp",
    "publicationDate": "2020-07-02",
    "domain": "Robotics",
    "task": "Object detection",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://proceedings.neurips.cc/paper/2020/file/2c75cf2681788adaca63aa95ae028b22-Paper.pdf\nhttps://arxiv.org/abs/2007.00643",
    "reference": "Object Goal Navigation using Goal-Oriented Semantic Exploration",
    "citations": "619.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Carnegie Mellon University (CMU)",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-07"
  },
  {
    "model": "GShard (dense)",
    "organization": "Google",
    "publicationDate": "2020-06-30",
    "domain": "Language",
    "task": "Translation",
    "parameters": 2300000000.0,
    "trainingComputeFlop": 4.765e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2006.16668",
    "reference": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
    "citations": "1525.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": "True",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-06"
  },
  {
    "model": "GPT-3 175B (davinci)",
    "organization": "OpenAI",
    "publicationDate": "2020-05-28",
    "domain": "Language",
    "task": "Text autocompletion",
    "parameters": 174600000000.0,
    "trainingComputeFlop": 3.14e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2005.14165",
    "reference": "Language Models are Few-Shot Learners",
    "citations": "50444.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 32 GB",
    "baseModel": null,
    "modelAccessibility": "API access",
    "frontierModel": "True",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-05"
  },
  {
    "model": "DETR",
    "organization": "Facebook",
    "publicationDate": "2020-05-26",
    "domain": "Vision",
    "task": "Object detection",
    "parameters": 60000000.0,
    "trainingComputeFlop": 4e+20,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2005.12872",
    "reference": "End-to-End Object Detection with Transformers",
    "citations": "15847.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-05"
  },
  {
    "model": "Retrieval-Augmented Generator",
    "organization": [
      "Facebook",
      "New York University (NYU)",
      "University College London (UCL)"
    ],
    "publicationDate": "2020-05-22",
    "domain": "Language",
    "task": "Question answering",
    "parameters": 626000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2005.11401v4",
    "reference": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
    "citations": "9507.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "United Kingdom"
    ],
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA Tesla V100 PCIe 32 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-05"
  },
  {
    "model": "Conformer",
    "organization": "Google",
    "publicationDate": "2020-05-16",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": 118800000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2005.08100v1",
    "reference": "Conformer: Convolution-augmented Transformer for Speech Recognition",
    "citations": "3678.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-05"
  },
  {
    "model": "ContextNet",
    "organization": "Google",
    "publicationDate": "2020-05-07",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": 112700000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2005.03191v3",
    "reference": "ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context",
    "citations": "289.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-05"
  },
  {
    "model": "NAS+ESS (23M)",
    "organization": [
      "Northeastern University (China)",
      "NiuTrans Research",
      "Kingsoft"
    ],
    "publicationDate": "2020-05-06",
    "domain": "Language",
    "task": "Neural Architecture Search - NAS",
    "parameters": 23000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/pdf/2005.02593",
    "reference": "Learning Architectures from an Extended Search Space for Language Modeling",
    "citations": null,
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA GeForce GTX 1080 Ti",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Northeastern University (China)",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-05"
  },
  {
    "model": "UnifiedQA",
    "organization": [
      "Allen Institute for AI",
      "University of Washington"
    ],
    "publicationDate": "2020-05-02",
    "domain": "Language",
    "task": "Question answering",
    "parameters": 11000000000.0,
    "trainingComputeFlop": 1.65e+19,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2005.00700v3",
    "reference": "UnifiedQA: Crossing Format Boundaries With a Single QA System",
    "citations": "783.0",
    "organizationCategorization": [
      "Research collective",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": "T5-11B",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Allen Institute for AI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-05"
  },
  {
    "model": "Once for All",
    "organization": [
      "MIT-IBM Watson AI Lab",
      "Massachusetts Institute of Technology (MIT)",
      "IBM"
    ],
    "publicationDate": "2020-04-29",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 7700000.0,
    "trainingComputeFlop": 6.237e+20,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1908.09791",
    "reference": "Once for all: Train one network and specialize it for efficient deployment.",
    "citations": "1401.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "MIT-IBM Watson AI Lab",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-04"
  },
  {
    "model": "Go-explore",
    "organization": [
      "Uber AI",
      "OpenAI"
    ],
    "publicationDate": "2020-04-27",
    "domain": "Games",
    "task": "Atari",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/2004.12919",
    "reference": [
      "First return",
      "then explore"
    ],
    "citations": "398.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Uber AI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-04"
  },
  {
    "model": "CURL",
    "organization": "University of California (UC) Berkeley",
    "publicationDate": "2020-04-08",
    "domain": "Games",
    "task": "Atari",
    "parameters": 907264.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "link": "https://arxiv.org/abs/2004.04136v4",
    "reference": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning",
    "citations": "978.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "University of California (UC) Berkeley",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2020,
    "yearMonth": "2020-04"
  },
  {
    "model": "Agent57",
    "publicationDate": "2020-03-30",
    "domain": "Games",
    "task": "Atari",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/2003.13350",
    "reference": "Agent57: Outperforming the Atari Human Benchmark",
    "citations": "554.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-03"
  },
  {
    "model": "MetNet",
    "organization": "Google",
    "publicationDate": "2020-03-24",
    "domain": "Earth science",
    "task": "Weather forecasting",
    "parameters": 225000000.0,
    "trainingComputeFlop": 9.510912e+18,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/2003.12140",
    "reference": "MetNet: A Neural Weather Model for Precipitation Forecasting",
    "citations": "313.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-03"
  },
  {
    "model": "ELECTRA",
    "publicationDate": "2020-03-23",
    "domain": "Language",
    "task": "Text autocompletion",
    "parameters": 335000000.0,
    "trainingComputeFlop": 3.1e+21,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2003.10555v1",
    "reference": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators",
    "citations": "2968.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Stanford University",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-03"
  },
  {
    "model": "Tensor-Transformer(1core)+PN (WT103)",
    "organization": "University of California (UC) Berkeley",
    "publicationDate": "2020-03-17",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 85300000.0,
    "trainingComputeFlop": 1.58e+18,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2003.07845",
    "reference": "PowerNorm: Rethinking Batch Normalization in Transformers",
    "citations": "60.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "University of California (UC) Berkeley",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2020,
    "yearMonth": "2020-03"
  },
  {
    "model": "Routing Transformer (WT-103)",
    "publicationDate": "2020-03-12",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 79500000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2003.05997",
    "reference": "Efficient Content-Based Sparse Attention with Routing Transformers",
    "citations": "665.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-03"
  },
  {
    "model": "TransformerXL + spectrum control",
    "organization": [
      "University of California Los Angeles (UCLA)",
      "JD.com"
    ],
    "publicationDate": "2020-03-11",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 151000000.0,
    "trainingComputeFlop": 2.6289761e+19,
    "confidence": "Speculative",
    "link": "https://openreview.net/forum?id=ByxY8CNtvr",
    "reference": "Improving Neural Language Generation with Spectrum Control",
    "citations": "90.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "China"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "University of California Los Angeles (UCLA)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-03"
  },
  {
    "model": "TCAN (WT2)",
    "organization": [
      "Nanjing University",
      "Ant Group"
    ],
    "publicationDate": "2020-02-28",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 33000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2002.12530",
    "reference": "Temporal Convolutional Attention-based Network For Sequence Modeling",
    "citations": "45.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Nanjing University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-02"
  },
  {
    "model": "Feedback Transformer",
    "publicationDate": "2020-02-21",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 126000000.0,
    "trainingComputeFlop": 7.690547e+18,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/2002.09402",
    "reference": "Addressing Some Limitations of Transformers with Feedback Memory",
    "citations": "41.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "France",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "LORIA",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-02"
  },
  {
    "model": "FFN SwiGLU",
    "organization": "Google",
    "publicationDate": "2020-02-14",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 220000000.0,
    "trainingComputeFlop": 3.3687317e+19,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2002.05202",
    "reference": "GLU Variants Improve Transformer",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "Google TPU v2",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-02"
  },
  {
    "model": "Turing-NLG",
    "organization": "Microsoft",
    "publicationDate": "2020-02-13",
    "domain": "Language",
    "task": "Text autocompletion",
    "parameters": 17000000000.0,
    "trainingComputeFlop": 1.57e+22,
    "confidence": "Likely",
    "link": "https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/",
    "reference": "Turing-NLG: A 17-billion-parameter language model by Microsoft",
    "citations": "114.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 32 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-02"
  },
  {
    "model": "SimCLR",
    "publicationDate": "2020-02-13",
    "domain": "Vision",
    "task": "Image completion",
    "parameters": 375000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "link": "https://arxiv.org/abs/2002.05709",
    "reference": "A Simple Framework for Contrastive Learning of Visual Representations",
    "citations": "16017.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Google",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-02"
  },
  {
    "model": "ALBERT-xxlarge",
    "organization": [
      "Toyota Technological Institute at Chicago",
      "Google"
    ],
    "publicationDate": "2020-02-09",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 235000000.0,
    "trainingComputeFlop": 2.39e+21,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/1909.11942",
    "reference": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.",
    "citations": "6988.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Toyota Technological Institute at Chicago",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-02"
  },
  {
    "model": "TaLK Convolution",
    "organization": "Carleton University",
    "publicationDate": "2020-02-08",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 240000000.0,
    "trainingComputeFlop": 2.6990346e+19,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/2002.03184",
    "reference": "Time-aware Large Kernel Convolutions",
    "citations": "30.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "Canada",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA GeForce RTX 2080 Ti 11GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Carleton University",
    "department": null,
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2020,
    "yearMonth": "2020-02"
  },
  {
    "model": "Perceiver IO (optical flow)",
    "publicationDate": "2020-02-08",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": 27900000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2107.14795",
    "reference": "Perceiver IO: A General Architecture for Structured Inputs & Outputs",
    "citations": "699.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-02"
  },
  {
    "model": "Theseus 6/768",
    "organization": [
      "University of California San Diego",
      "Beihang University",
      "Microsoft"
    ],
    "publicationDate": "2020-02-07",
    "domain": "Language",
    "task": "Text autocompletion",
    "parameters": 66000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2002.02925",
    "reference": "BERT-of-Theseus: Compressing BERT by Progressive Module Replacing",
    "citations": "214.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "China"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": "BERT-Large",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "University of California San Diego",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-02"
  },
  {
    "model": "Meena",
    "publicationDate": "2020-01-28",
    "domain": "Language",
    "task": "Text autocompletion",
    "parameters": 2600000000.0,
    "trainingComputeFlop": 1.12e+23,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2001.09977",
    "reference": "Towards a Human-like Open-Domain Chatbot",
    "citations": "987.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": "True",
    "company": "Google",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-01"
  },
  {
    "model": "ContextNet + Noisy Student",
    "organization": "Google",
    "publicationDate": "2020-01-19",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": null,
    "trainingComputeFlop": 8.16e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/2005.09629v2",
    "reference": "Improved Noisy Student Training for Automatic Speech Recognition",
    "citations": "256.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-01"
  },
  {
    "model": "AlphaFold",
    "publicationDate": "2020-01-15",
    "domain": "Biology",
    "task": "Protein folding prediction",
    "parameters": 16340840.0,
    "trainingComputeFlop": 1e+20,
    "confidence": "Speculative",
    "link": "https://www.nature.com/articles/s41586-019-1923-7",
    "reference": "Improved protein structure prediction using potentials from deep learning",
    "citations": "2773.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-01"
  },
  {
    "model": "Big Transfer (BiT-L)",
    "publicationDate": "2019-12-24",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 928000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "link": "https://arxiv.org/abs/1912.11370",
    "reference": "Big Transfer (BiT): General Visual Representation Learning",
    "citations": "1291.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-12"
  },
  {
    "model": "DD-PPO",
    "publicationDate": "2019-12-19",
    "domain": "Robotics",
    "task": "Object detection",
    "parameters": null,
    "trainingComputeFlop": 7.8e+20,
    "confidence": "Likely",
    "link": "https://openreview.net/forum?id=H1gX8C4YPr\nhttps://arxiv.org/abs/1911.00357",
    "reference": "DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames",
    "citations": "536.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "France",
      "Canada"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Georgia Institute of Technology",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-12"
  },
  {
    "model": "OpenAI Five Rerun",
    "organization": "OpenAI",
    "publicationDate": "2019-12-13",
    "domain": "Games",
    "task": "Dota 2",
    "parameters": 159000000.0,
    "trainingComputeFlop": 1.3e+22,
    "confidence": "Confident",
    "link": "https://cdn.openai.com/dota-2.pdf",
    "reference": "Dota 2 with Large Scale Deep Reinforcement Learning",
    "citations": "1994.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA P100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-12"
  },
  {
    "model": "OpenAI Five",
    "organization": "OpenAI",
    "publicationDate": "2019-12-13",
    "domain": "Games",
    "task": "Dota 2",
    "parameters": 159000000.0,
    "trainingComputeFlop": 6.7e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1912.06680",
    "reference": "Dota 2 with Large Scale Deep Reinforcement Learning",
    "citations": "1994.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA P100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": "True",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-12"
  },
  {
    "model": "MMLSTM (WT-2)",
    "organization": [
      "Beijing University of Posts and Telecommunications",
      "University of West London"
    ],
    "publicationDate": "2019-12-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 32300000.0,
    "trainingComputeFlop": 1.938e+17,
    "confidence": "Likely",
    "link": "http://repository.uwl.ac.uk/id/eprint/6490/1/Loo_etal_IEEE_TNNLS_2019_Major-minor_long_short-term_memory_for_word-level_language_model.pdf",
    "reference": "Major–Minor Long Short-Term Memory for Word-Level Language Model",
    "citations": "19.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "China",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Beijing University of Posts and Telecommunications",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2019,
    "yearMonth": "2019-12"
  },
  {
    "model": "MMLSTM (PTB)",
    "organization": [
      "Beijing University of Posts and Telecommunications",
      "University of West London"
    ],
    "publicationDate": "2019-12-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 21300000.0,
    "trainingComputeFlop": 5.8298782e+16,
    "confidence": "Likely",
    "link": "http://repository.uwl.ac.uk/id/eprint/6490/1/Loo_etal_IEEE_TNNLS_2019_Major-minor_long_short-term_memory_for_word-level_language_model.pdf",
    "reference": "Major–Minor Long Short-Term Memory for Word-Level Language Model",
    "citations": "19.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "China",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Beijing University of Posts and Telecommunications",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2019,
    "yearMonth": "2019-12"
  },
  {
    "model": "StarGAN v2",
    "organization": [
      "NAVER",
      "Yonsei University",
      "Swiss Federal Institute of Technology"
    ],
    "publicationDate": "2019-12-04",
    "domain": "Vision",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/1912.01865",
    "reference": "StarGAN v2: Diverse Image Synthesis for Multiple Domains",
    "citations": "1921.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "Korea (Republic of)",
      "Switzerland"
    ],
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "NAVER",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-12"
  },
  {
    "model": "StyleGAN2",
    "organization": [
      "NVIDIA",
      "Aalto University"
    ],
    "publicationDate": "2019-12-03",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 30000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/pdf/1912.04958",
    "reference": "Analyzing and Improving the Image Quality of StyleGAN",
    "citations": "7908.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Finland"
    ],
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "frontierModel": null,
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-12"
  },
  {
    "model": "Transformer-XL DeFINE (141M)",
    "organization": [
      "University of Washington",
      "Allen Institute for AI"
    ],
    "publicationDate": "2019-11-27",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 141000000.0,
    "trainingComputeFlop": 1.74276e+18,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/1911.12385",
    "reference": "DeFINE: DEep Factorized INput Token Embeddings for Neural Sequence Modeling",
    "citations": "26.0",
    "organizationCategorization": [
      "Academia",
      "Research collective"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "University of Washington",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-11"
  },
  {
    "model": "Photo-Geometric Autoencoder",
    "organization": "University of Oxford",
    "publicationDate": "2019-11-25",
    "domain": "3D modeling",
    "task": "3D reconstruction",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/1911.11130",
    "reference": "Unsupervised Learning of Probably Symmetric Deformable 3D Objects From Images in the Wild",
    "citations": "323.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United Kingdom",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "University of Oxford",
    "department": null,
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2019,
    "yearMonth": "2019-11"
  },
  {
    "model": "Transformer - LibriVox + Decoding/Rescoring",
    "organization": "Facebook",
    "publicationDate": "2019-11-19",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": 296000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1911.08460v3",
    "reference": "End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern Architectures",
    "citations": "260.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-11"
  },
  {
    "model": "MuZero",
    "publicationDate": "2019-11-19",
    "domain": "Games",
    "task": "Atari",
    "parameters": 36864000.0,
    "trainingComputeFlop": 4.8e+19,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/1911.08265v2",
    "reference": "Mastering Atari Go Chess and Shogi by Planning with a Learned Model",
    "citations": "2251.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-11"
  },
  {
    "model": "MoCo",
    "publicationDate": "2019-11-13",
    "domain": "Vision",
    "task": "Image completion",
    "parameters": 375000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "link": "https://arxiv.org/abs/1911.05722",
    "reference": "Momentum Contrast for Unsupervised Visual Representation Learning",
    "citations": "10711.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Facebook AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-11"
  },
  {
    "model": "Noisy Student (L2)",
    "organization": [
      "Carnegie Mellon University (CMU)",
      "Google"
    ],
    "publicationDate": "2019-11-11",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 480000000.0,
    "trainingComputeFlop": 2.612e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1911.04252v4",
    "reference": "Self-training with Noisy Student improves ImageNet classification",
    "citations": "2572.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": "True",
    "company": "Carnegie Mellon University (CMU)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-11"
  },
  {
    "model": "Sandwich Transformer",
    "publicationDate": "2019-11-10",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 209000000.0,
    "trainingComputeFlop": 2.3504093e+19,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/1911.03864",
    "reference": "Improving Transformer Models by Reordering their Sublayers",
    "citations": "93.0",
    "organizationCategorization": [
      "Research collective",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Allen Institute for AI",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-11"
  },
  {
    "model": "CamemBERT",
    "organization": [
      "Facebook",
      "INRIA",
      "Sorbonne University"
    ],
    "publicationDate": "2019-11-10",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 335000000.0,
    "trainingComputeFlop": 8.3e+20,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1911.03894",
    "reference": "CamemBERT: a Tasty French Language Model",
    "citations": "1036.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-11"
  },
  {
    "model": "XLM-RoBERTa",
    "publicationDate": "2019-11-05",
    "domain": "Language",
    "task": "Named entity recognition (NER)",
    "parameters": 550000000.0,
    "trainingComputeFlop": 2.076e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1911.02116",
    "reference": "Unsupervised Cross-lingual Representation Learning at Scale",
    "citations": "7386.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 32 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": "True",
    "company": "Meta",
    "department": "Facebook AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-11"
  },
  {
    "model": "Base LM + kNN LM + Continuous Cache",
    "publicationDate": "2019-11-01",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 247000000.00000003,
    "trainingComputeFlop": 3.05292e+19,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/1911.00172",
    "reference": "Generalization through Memorization: Nearest Neighbor Language Models",
    "citations": "947.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Stanford University",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-11"
  },
  {
    "model": "AlphaStar",
    "publicationDate": "2019-10-30",
    "domain": "Games",
    "task": "StarCraft",
    "parameters": 139000000.0,
    "trainingComputeFlop": 1.0773400001e+23,
    "confidence": "Confident",
    "link": "https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning",
    "reference": "Grandmaster level in StarCraft II using multi-agent reinforcement learning",
    "citations": "4037.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": "True",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-10"
  },
  {
    "model": "BART-large",
    "publicationDate": "2019-10-29",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 406291456.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/1910.13461",
    "reference": [
      "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation",
      "Translation",
      "and Comprehension"
    ],
    "citations": "11831.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Facebook AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-10"
  },
  {
    "model": "T5-11B",
    "organization": "Google",
    "publicationDate": "2019-10-23",
    "domain": "Language",
    "task": "Text autocompletion",
    "parameters": 11000000000.0,
    "trainingComputeFlop": 3.3e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1910.10683",
    "reference": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
    "citations": "23135.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": "True",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-10"
  },
  {
    "model": "T5-3B",
    "organization": "Google",
    "publicationDate": "2019-10-23",
    "domain": "Language",
    "task": "Text autocompletion",
    "parameters": 2800000000.0,
    "trainingComputeFlop": 9.0000000001e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1910.10683",
    "reference": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
    "citations": "23135.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": "True",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-10"
  },
  {
    "model": "M4-50B",
    "organization": "Google",
    "publicationDate": "2019-10-11",
    "domain": "Language",
    "task": "Translation",
    "parameters": 50000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://blog.research.google/2019/10/exploring-massively-multilingual.html",
    "reference": [
      "Exploring Massively Multilingual",
      "Massive Neural Machine Translation"
    ],
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-10"
  },
  {
    "model": "DistilBERT",
    "organization": "Hugging Face",
    "publicationDate": "2019-10-02",
    "domain": "Language",
    "task": "Text autocompletion",
    "parameters": 66000000.0,
    "trainingComputeFlop": 1.24416e+19,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1910.01108",
    "reference": [
      "DistilBERT",
      "a distilled version of BERT: smaller",
      "faster",
      "cheaper and lighter"
    ],
    "citations": "8601.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 16 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Hugging Face",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-10"
  },
  {
    "model": "AlphaX-1",
    "publicationDate": "2019-10-02",
    "domain": "Vision",
    "task": "Neural architecture search for computer vision",
    "parameters": 5400000.0,
    "trainingComputeFlop": 8.89344e+17,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1903.11059",
    "reference": "AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search",
    "citations": "97.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA GeForce GTX 1080 Ti",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-10"
  },
  {
    "model": "ALBERT",
    "publicationDate": "2019-09-26",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 18000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1909.11942",
    "reference": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
    "citations": "6988.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Toyota Technological Institute at Chicago",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-09"
  },
  {
    "model": "Adaptive Inputs + LayerDrop",
    "publicationDate": "2019-09-25",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 423000000.00000006,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1909.11556",
    "reference": "Reducing Transformer Depth on Demand with Structured Dropout",
    "citations": "645.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-09"
  },
  {
    "model": "Megatron-LM (8.3B)",
    "organization": "NVIDIA",
    "publicationDate": "2019-09-17",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 8300000000.0,
    "trainingComputeFlop": 9.1e+21,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/1909.08053",
    "reference": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
    "citations": "2306.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 32 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": "True",
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-09"
  },
  {
    "model": "Megatron-LM (1.2B)",
    "organization": "NVIDIA",
    "publicationDate": "2019-09-17",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 1200000000.0,
    "trainingComputeFlop": 1.13e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1909.08053",
    "reference": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
    "citations": "2306.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA Tesla V100 SXM3 32 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-09"
  },
  {
    "model": "Megatron-BERT",
    "organization": "NVIDIA",
    "publicationDate": "2019-09-17",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 3900000000.0,
    "trainingComputeFlop": 2.2e+22,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1909.08053",
    "reference": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
    "citations": "2306.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA Tesla V100S PCIe 32 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": "True",
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-09"
  },
  {
    "model": "UDSMProt",
    "organization": "Fraunhofer Heinrich Hertz Institute",
    "publicationDate": "2019-09-04",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 28303800.0,
    "trainingComputeFlop": 6.37e+17,
    "confidence": "Likely",
    "link": "https://www.biorxiv.org/content/10.1101/704874v2.full.pdf",
    "reference": "UDSMProt: Universal Deep Sequence Models for Protein Classification",
    "citations": null,
    "organizationCategorization": "Research collective",
    "countryOfOrganization": "Germany",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Fraunhofer Heinrich Hertz Institute",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-09"
  },
  {
    "model": [
      "Mogrifier (d2",
      "MoS2",
      "MC) + dynamic eval"
    ],
    "publicationDate": "2019-09-04",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 35000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1909.01792",
    "reference": "Mogrifier LSTM",
    "citations": "109.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United Kingdom",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-09"
  },
  {
    "model": "trRosetta",
    "organization": [
      "Nankai University",
      "University of Washington",
      "Tianjin University",
      "Harvard University"
    ],
    "publicationDate": "2019-08-22",
    "domain": "Biology",
    "task": "Protein folding prediction",
    "parameters": null,
    "trainingComputeFlop": 3.8047968e+19,
    "confidence": "Confident",
    "link": "https://www.pnas.org/doi/10.1073/pnas.1914677117",
    "reference": "Improved protein structure prediction using predictedinterresidue orientations",
    "citations": null,
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "China",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA TITAN RTX",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Nankai University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2019,
    "yearMonth": "2019-08"
  },
  {
    "model": "EN^2AS with performance reward",
    "organization": [
      "Beijing Institute of Technology",
      "University of Technology Sydney",
      "Monash University"
    ],
    "publicationDate": "2019-07-22",
    "domain": "Language",
    "task": "Neural Architecture Search - NAS",
    "parameters": 23000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "link": "https://arxiv.org/abs/1907.09109",
    "reference": "Efficient Novelty-Driven Neural Architecture Search",
    "citations": "1.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "China",
      "Australia"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Beijing Institute of Technology",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2019,
    "yearMonth": "2019-07"
  },
  {
    "model": "Pluribus",
    "publicationDate": "2019-07-11",
    "domain": "Games",
    "task": "Poker",
    "parameters": null,
    "trainingComputeFlop": 6.6e+16,
    "confidence": "Likely",
    "link": "https://www.science.org/cms/asset/910714a7-ee2a-486e-9970-42fb893b08d9/pap.pdf",
    "reference": "Superhuman AI for multiplayer poker",
    "citations": "786.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-07"
  },
  {
    "model": "BigBiGAN",
    "organization": "Google",
    "publicationDate": "2019-07-04",
    "domain": "Vision",
    "task": "Image completion",
    "parameters": 86000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "link": "https://arxiv.org/abs/1907.02544",
    "reference": "Large Scale Adversarial Representation Learning",
    "citations": "525.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-07"
  },
  {
    "model": "RoBERTa Large",
    "organization": [
      "Facebook",
      "University of Washington"
    ],
    "publicationDate": "2019-07-01",
    "domain": "Language",
    "task": "Question answering",
    "parameters": 355000000.0,
    "trainingComputeFlop": 8.5067e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1907.11692",
    "reference": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
    "citations": "27245.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 32 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": "True",
    "company": "Meta",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-07"
  },
  {
    "model": "Walking Minotaur robot",
    "publicationDate": "2019-06-19",
    "domain": "Robotics",
    "task": "Animal (human/non-human) imitation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/1812.11103",
    "reference": "Learning to Walk via Deep Reinforcement Learning",
    "citations": "470.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "University of California (UC) Berkeley",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-06"
  },
  {
    "model": "LaNet-L (CIFAR-10)",
    "organization": [
      "Brown University",
      "Facebook"
    ],
    "publicationDate": "2019-06-17",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 44100000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1906.06832",
    "reference": "Sample-Efficient Neural Architecture Search by Learning Action Space",
    "citations": "47.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Brown University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-06"
  },
  {
    "model": "PG-SWGAN",
    "organization": "ETH Zurich",
    "publicationDate": "2019-06-15",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://openaccess.thecvf.com/content_CVPR_2019/html/Wu_Sliced_Wasserstein_Generative_Models_CVPR_2019_paper.html",
    "reference": "Sliced Wasserstein Generative Models",
    "citations": "136.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "Switzerland",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "ETH Zurich",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2019,
    "yearMonth": "2019-06"
  },
  {
    "model": "FixRes ResNeXt-101 WSL",
    "publicationDate": "2019-06-14",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 829000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "link": "https://arxiv.org/abs/1906.06423",
    "reference": "Fixing the train-test resolution discrepancy",
    "citations": "405.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Facebook AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-06"
  },
  {
    "model": "Char-CNN-BiLSTM",
    "organization": "Capital One",
    "publicationDate": "2019-06-13",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1906.05678",
    "reference": "Telephonetic: Making Neural Language Models Robust to ASR and Semantic Noise",
    "citations": "2.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Capital One",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-06"
  },
  {
    "model": "AWD-LSTM + MoS + Partial Shuffled",
    "organization": "University of Texas at Austin",
    "publicationDate": "2019-06-10",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 35000000.0,
    "trainingComputeFlop": 3.15e+17,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1906.03805",
    "reference": "Improving Neural Language Modeling via Adversarial Training",
    "citations": "122.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "University of Texas at Austin",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2019,
    "yearMonth": "2019-06"
  },
  {
    "model": "Transformer-XL Large + Phrase Induction",
    "organization": [
      "Massachusetts Institute of Technology (MIT)",
      "University of Illinois Urbana-Champaign (UIUC)"
    ],
    "publicationDate": "2019-06-04",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 256999999.99999997,
    "trainingComputeFlop": 3.7848651e+20,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/1906.01702",
    "reference": [
      "Improving Neural Language Models by Segmenting",
      "Attending",
      "and Predicting the Future"
    ],
    "citations": "14.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Transformer-XL (257M)",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Massachusetts Institute of Technology (MIT)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2019,
    "yearMonth": "2019-06"
  },
  {
    "model": "XLNet",
    "publicationDate": "2019-06-01",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 340000000.0,
    "trainingComputeFlop": 6.19e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1906.08237",
    "reference": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
    "citations": "8964.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": "True",
    "company": "Carnegie Mellon University (CMU)",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-06"
  },
  {
    "model": "XLM",
    "organization": "Facebook",
    "publicationDate": "2019-06-01",
    "domain": "Language",
    "task": "Translation",
    "parameters": 665000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "link": "https://arxiv.org/abs/1901.07291",
    "reference": "Cross-lingual Language Model Pretraining",
    "citations": "2588.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Meta",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-06"
  },
  {
    "model": "DLRM-2020",
    "publicationDate": "2019-05-31",
    "domain": "Recommendation",
    "task": "Recommender system",
    "parameters": 100000000000.0,
    "trainingComputeFlop": 4e+18,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1906.00091",
    "reference": "Deep Learning Recommendation Model for Personalization and Recommendation Systems",
    "citations": "816.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Meta",
    "department": "Facebook AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-05"
  },
  {
    "model": "EfficientNet-L2",
    "organization": "Google",
    "publicationDate": "2019-05-28",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 480000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "link": "https://arxiv.org/abs/1905.11946",
    "reference": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
    "citations": "15327.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-05"
  },
  {
    "model": "CPC v2",
    "publicationDate": "2019-05-22",
    "domain": "Vision",
    "task": "Image completion",
    "parameters": 303000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "link": "https://arxiv.org/abs/1905.09272",
    "reference": "Data-Efficient Image Recognition with Contrastive Predictive Coding",
    "citations": "491.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United Kingdom",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-05"
  },
  {
    "model": "AWD-LSTM-DRILL + dynamic evaluation† (WT2)",
    "organization": "IDIAP",
    "publicationDate": "2019-05-14",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 34000000.0,
    "trainingComputeFlop": 4.08e+17,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/1905.05513",
    "reference": "Deep Residual Output Layers for Neural Language Generation",
    "citations": "7.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "Switzerland",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "IDIAP",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2019,
    "yearMonth": "2019-05"
  },
  {
    "model": "ResNeXt-101 Billion-scale",
    "publicationDate": "2019-05-02",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 193000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "link": "https://arxiv.org/abs/1905.00546",
    "reference": "Billion-scale semi-supervised learning for image classification",
    "citations": "436.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Facebook AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-05"
  },
  {
    "model": "RaptorX-Contact",
    "organization": "Toyota Technological Institute at Chicago",
    "publicationDate": "2019-05-02",
    "domain": "Biology",
    "task": "Protein folding prediction",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://www.biorxiv.org/content/biorxiv/early/2019/05/02/624460.full.pdf",
    "reference": "Analysis of distance-based protein structure prediction by deep learning in CASP13",
    "citations": null,
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Toyota Technological Institute at Chicago",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2019,
    "yearMonth": "2019-05"
  },
  {
    "model": "Neuro-Symbolic Concept Learner",
    "publicationDate": "2019-04-26",
    "domain": "Vision",
    "task": "Visual question answering",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/1904.12584",
    "reference": [
      "The Neuro-Symbolic Concept Learner: Interpreting Scenes",
      "Words",
      "and Sentences From Natural Supervision"
    ],
    "citations": "761.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "China",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Massachusetts Institute of Technology (MIT)",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-04"
  },
  {
    "model": "MuseNet",
    "organization": "OpenAI",
    "publicationDate": "2019-04-25",
    "domain": "Audio",
    "task": "Audio generation",
    "parameters": 2038431744.0,
    "trainingComputeFlop": 2.208301056e+20,
    "confidence": "Likely",
    "link": "https://openai.com/index/musenet/",
    "reference": "MuseNet",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-04"
  },
  {
    "model": "BERT-Large-CAS (PTB+WT2+WT103)",
    "organization": "Amazon",
    "publicationDate": "2019-04-20",
    "domain": "Language",
    "task": "Neural Architecture Search - NAS",
    "parameters": 395000000.0,
    "trainingComputeFlop": 1.5405e+20,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/1904.09408",
    "reference": "Language Models with Transformers",
    "citations": "129.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Amazon",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-04"
  },
  {
    "model": "Transformer-XL + RMS dynamic eval",
    "organization": "University of Edinburgh",
    "publicationDate": "2019-04-17",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 256999999.99999997,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1904.08378",
    "reference": "Dynamic Evaluation of Transformer Language Models",
    "citations": "45.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United Kingdom",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "University of Edinburgh",
    "department": null,
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2019,
    "yearMonth": "2019-04"
  },
  {
    "model": "WeNet (Penn Treebank)",
    "organization": "Amazon",
    "publicationDate": "2019-04-08",
    "domain": "Language",
    "task": "Neural Architecture Search - NAS",
    "parameters": 23000000.0,
    "trainingComputeFlop": 7.30000001e+17,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/1904.03819",
    "reference": "WeNet: Weighted Networks for Recurrent Network Architecture Search",
    "citations": "5.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Amazon",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-04"
  },
  {
    "model": "True-Regularization+Finetune+Dynamic-Eval",
    "organization": [
      "Mobvoi",
      "Williams College"
    ],
    "publicationDate": "2019-04-08",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 7000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1904.04163",
    "reference": "Knowledge Distillation For Recurrent Neural Network Language Modeling With Trust Regularization",
    "citations": "25.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "China",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Mobvoi",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-04"
  },
  {
    "model": "Cross-lingual alignment",
    "organization": [
      "Tel Aviv University",
      "Massachusetts Institute of Technology (MIT)"
    ],
    "publicationDate": "2019-04-04",
    "domain": "Language",
    "task": "Translation",
    "parameters": null,
    "trainingComputeFlop": 2.56e+18,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/1902.09492",
    "reference": [
      "Cross-lingual alignment of contextual word embeddings",
      "with applications to zero- shot dependency parsing."
    ],
    "citations": "218.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "Israel",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA GeForce GTX 1080 Ti",
    "baseModel": "ELMo",
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Tel Aviv University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2019,
    "yearMonth": "2019-04"
  },
  {
    "model": "SciBERT",
    "organization": "Allen Institute for AI",
    "publicationDate": "2019-03-26",
    "domain": "Language",
    "task": "Relation extraction",
    "parameters": 110000000.0,
    "trainingComputeFlop": 8.926848e+19,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1903.10676",
    "reference": "SciBERT: A Pretrained Language Model for Scientific Text",
    "citations": "3386.0",
    "organizationCategorization": "Research collective",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Allen Institute for AI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-03"
  },
  {
    "model": "NMT Transformer 437M",
    "organization": [
      "Google",
      "Bar-Ilan University"
    ],
    "publicationDate": "2019-02-28",
    "domain": "Language",
    "task": "Translation",
    "parameters": 437700000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1903.00089",
    "reference": "Massively Multilingual Neural Machine Translation",
    "citations": "520.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Israel"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-02"
  },
  {
    "model": "KataGo",
    "organization": "Jane Street",
    "publicationDate": "2019-02-27",
    "domain": "Games",
    "task": "Go",
    "parameters": 2500000.0,
    "trainingComputeFlop": 2.32e+19,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/1902.10565",
    "reference": "Accelerating Self-Play Learning in Go",
    "citations": "103.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 16 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Jane Street",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-02"
  },
  {
    "model": "GPT-2 (1.5B)",
    "organization": "OpenAI",
    "publicationDate": "2019-02-14",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 1500000000.0,
    "trainingComputeFlop": 1.920000000001e+21,
    "confidence": "Speculative",
    "link": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf",
    "reference": "Language Models are Unsupervised Multitask Learners",
    "citations": "26008.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-02"
  },
  {
    "model": "SDE",
    "publicationDate": "2019-02-09",
    "domain": "Language",
    "task": "Translation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/1902.03499",
    "reference": "Multilingual Neural Machine Translation With Soft Decoupled Encoding",
    "citations": null,
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "Australia"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Carnegie Mellon University (CMU)",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-02"
  },
  {
    "model": "Hanabi 4 player",
    "publicationDate": "2019-02-01",
    "domain": "Games",
    "task": "Hanabi",
    "parameters": 764000.0,
    "trainingComputeFlop": 4.3e+18,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1902.00506",
    "reference": "The Hanabi Challenge: A New Frontier for AI Research",
    "citations": "229.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United Kingdom",
      "United States"
    ],
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": [
      "DeepMind",
      "Google Brain"
    ],
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-02"
  },
  {
    "model": "MT-DNN",
    "organization": "Microsoft",
    "publicationDate": "2019-01-31",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 330000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1901.11504",
    "reference": "Multi-Task Deep Neural Networks for Natural Language Understanding",
    "citations": "1318.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-01"
  },
  {
    "model": "Transformer-XL (257M)",
    "publicationDate": "2019-01-09",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 256999999.99999997,
    "trainingComputeFlop": 3.7832771e+20,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/1901.02860",
    "reference": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context",
    "citations": "4051.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Carnegie Mellon University (CMU)",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-01"
  },
  {
    "model": "Transformer ELMo",
    "organization": [
      "Allen Institute for AI",
      "University of Washington"
    ],
    "publicationDate": "2019-01-01",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 56000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://www.semanticscholar.org/paper/Dissecting-Contextual-Word-Embeddings%3A-Architecture-Peters-Neumann/ac11062f1f368d97f4c826c317bf50dcc13fdb59",
    "reference": "Dissecting Contextual Word Embeddings: Architecture and Representation",
    "citations": "455.0",
    "organizationCategorization": [
      "Research collective",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Allen Institute for AI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-01"
  },
  {
    "model": "StyleGAN",
    "organization": "NVIDIA",
    "publicationDate": "2018-12-12",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 26200000.0,
    "trainingComputeFlop": 3.93e+16,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1812.04948",
    "reference": "A Style-Based Generator Architecture for Generative Adversarial Networks",
    "citations": "14496.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": null,
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-12"
  },
  {
    "model": "SPN (ImageNet 128)",
    "publicationDate": "2018-12-04",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 250000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1812.01608",
    "reference": "Generating High Fidelity Images with Subscale Pixel Networks and Multidimensional Upscaling",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": [
      "United States",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": [
      "Google Brain",
      "DeepMind"
    ],
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-12"
  },
  {
    "model": "GPipe (Transformer)",
    "organization": "Google",
    "publicationDate": "2018-11-16",
    "domain": "Language",
    "task": "Translation",
    "parameters": 6000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/1811.06965",
    "reference": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism",
    "citations": "1218.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": null,
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Unknown",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-11"
  },
  {
    "model": "Multi-cell LSTM",
    "organization": "University of Hyderabad",
    "publicationDate": "2018-11-15",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 7200000.0,
    "trainingComputeFlop": 2006640000000000.0,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/1811.06477",
    "reference": "Multi-cell LSTM Based Neural Language Model",
    "citations": "6.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "India",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "University of Hyderabad",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2018,
    "yearMonth": "2018-11"
  },
  {
    "model": "Fine-tuned-AWD-LSTM-DOC (fin)",
    "organization": "Samsung R&D Institute Russia",
    "publicationDate": "2018-11-12",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 46000000.0,
    "trainingComputeFlop": 5.188e+16,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1811.04623",
    "reference": "Fine-tuning of Language Models with Discriminator",
    "citations": "2.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Russia",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "AWD-LSTM-DOC (fin) (23M)",
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Samsung R&D Institute Russia",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-11"
  },
  {
    "model": "Mesh-TensorFlow Transformer 4.9B (language)",
    "publicationDate": "2018-11-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 4900000000.0,
    "trainingComputeFlop": 1.617408e+20,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1811.02084",
    "reference": "Mesh-TensorFlow: Deep Learning for Supercomputers",
    "citations": "417.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v2",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-11"
  },
  {
    "model": "Mesh-TensorFlow Transformer 2.9B (translation)",
    "publicationDate": "2018-11-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 2900000000.0,
    "trainingComputeFlop": 6.84288e+19,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/1811.02084",
    "reference": "Mesh-TensorFlow: Deep Learning for Supercomputers",
    "citations": "417.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v2",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-11"
  },
  {
    "model": "MemoReader",
    "organization": [
      "Samsung",
      "Korea University"
    ],
    "publicationDate": "2018-10-31",
    "domain": "Language",
    "task": "Question answering",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://aclanthology.org/D18-1237/",
    "reference": "MemoReader: Large-Scale Reading Comprehension through Neural Memory Controller",
    "citations": "17.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "Korea (Republic of)",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA M40",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Samsung",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2018,
    "yearMonth": "2018-10"
  },
  {
    "model": "TrellisNet",
    "organization": [
      "Carnegie Mellon University (CMU)",
      "Bosch Center for Artificial Intelligence",
      "Intel Labs"
    ],
    "publicationDate": "2018-10-15",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 180000000.0,
    "trainingComputeFlop": 2.78e+18,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1810.06682",
    "reference": "Trellis Networks for Sequence Modeling",
    "citations": "153.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "Germany"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Carnegie Mellon University (CMU)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2018,
    "yearMonth": "2018-10"
  },
  {
    "model": "MetaMimic",
    "organization": "Google",
    "publicationDate": "2018-10-11",
    "domain": "Robotics",
    "task": "Robotic manipulation",
    "parameters": 22000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "link": "https://arxiv.org/abs/1810.05017",
    "reference": "One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL",
    "citations": "26.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": null,
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Unknown",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-10"
  },
  {
    "model": "BERT-Large",
    "organization": "Google",
    "publicationDate": "2018-10-11",
    "domain": "Language",
    "task": "Question answering",
    "parameters": 340000000.0,
    "trainingComputeFlop": 2.85e+20,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1810.04805",
    "reference": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "citations": "105720.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v2",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-10"
  },
  {
    "model": "Transformer (Adaptive Input Embeddings) WT103",
    "publicationDate": "2018-09-28",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 247000000.0,
    "trainingComputeFlop": 4.47e+19,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1809.10853",
    "reference": "Adaptive Input Representations for Neural Language Modeling",
    "citations": "420.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-09"
  },
  {
    "model": "LSTM+NeuralCache",
    "organization": [
      "KU Leuven",
      "ESAT - PSI",
      "Apple"
    ],
    "publicationDate": "2018-09-24",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 2100000.0,
    "trainingComputeFlop": 982800000000000.0,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/1809.08826",
    "reference": "Information-Weighted Neural Cache Language Models for ASR",
    "citations": "3.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "Belgium",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "KU Leuven",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2018,
    "yearMonth": "2018-09"
  },
  {
    "model": [
      "AWD-LSTM-MoS + dynamic evaluation (WT2",
      "2018)"
    ],
    "organization": [
      "Peking University",
      "Microsoft Research Asia"
    ],
    "publicationDate": "2018-09-18",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 35000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1809.06858",
    "reference": "FRAGE: Frequency-Agnostic Word Representation",
    "citations": "152.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Peking University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2018,
    "yearMonth": "2018-09"
  },
  {
    "model": "Transformer + Simple Recurrent Unit",
    "organization": [
      "ASAPP",
      "Cornell University",
      "Google",
      "Princeton University"
    ],
    "publicationDate": "2018-09-17",
    "domain": "Language",
    "task": "Translation",
    "parameters": 90000000.0,
    "trainingComputeFlop": 1.1e+19,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1709.02755v5",
    "reference": "Simple Recurrent Units for Highly Parallelizable Recurrence",
    "citations": "294.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "ASAPP",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2018,
    "yearMonth": "2018-09"
  },
  {
    "model": "NetSurfP-2.0",
    "organization": [
      "Technical University of Denmark",
      "University of Copenhagen",
      "Universidad Nacional de San Martín",
      "AIMST University"
    ],
    "publicationDate": "2018-09-10",
    "domain": "Biology",
    "task": "Protein folding prediction",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://www.biorxiv.org/content/10.1101/311209v3.full",
    "reference": "NetSurfP-2.0: improved prediction of protein structural features by integrated deep learning",
    "citations": null,
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "Denmark",
      "Argentina",
      "Malaysia"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "frontierModel": null,
    "company": "Technical University of Denmark",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2018,
    "yearMonth": "2018-09"
  },
  {
    "model": "(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2)",
    "organization": [
      "NTT Communication Science Laboratories",
      "Tohoku University"
    ],
    "publicationDate": "2018-08-30",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 185000000.0,
    "trainingComputeFlop": 6.66e+17,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1808.10143",
    "reference": "Direct Output Connection for a High-Rank Language Model",
    "citations": "37.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "Japan",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "NTT Communication Science Laboratories",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2018,
    "yearMonth": "2018-08"
  },
  {
    "model": "Big Transformer for Back-Translation",
    "publicationDate": "2018-08-28",
    "domain": "Language",
    "task": "Translation",
    "parameters": null,
    "trainingComputeFlop": 4.7808e+20,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/1808.09381",
    "reference": "Understanding Back-Translation at Scale",
    "citations": "1155.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 16 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "Meta",
    "department": [
      "Facebook AI Research",
      "Google Brain"
    ],
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-08"
  },
  {
    "model": "AWD-LSTM-MoS+PDR + dynamic evaluation (WT2)",
    "organization": "IBM",
    "publicationDate": "2018-08-14",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 35000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1808.05908",
    "reference": "Improved Language Modeling by Decoding the Past",
    "citations": "6.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "IBM",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-08"
  },
  {
    "model": "Big-Little Net (speech)",
    "organization": "IBM",
    "publicationDate": "2018-07-10",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": 3320000.0,
    "trainingComputeFlop": 4.290048e+17,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/1807.03848",
    "reference": "Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition",
    "citations": "99.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "IBM",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-07"
  },
  {
    "model": "Big-Little Net",
    "organization": "IBM",
    "publicationDate": "2018-07-10",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 77360000.0,
    "trainingComputeFlop": 2.46048e+17,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/1807.03848",
    "reference": "Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition",
    "citations": "99.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Tesla K80",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "IBM",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-07"
  },
  {
    "model": "FTW (For The Win)",
    "publicationDate": "2018-07-03",
    "domain": "Games",
    "task": "Capture the flag",
    "parameters": 126001330.0,
    "trainingComputeFlop": 3.49e+19,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/1807.01281",
    "reference": "Human-level performance in first-person multiplayer games with population-based deep reinforcement learning",
    "citations": "761.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-07"
  },
  {
    "model": "MobileNetV2",
    "organization": "Google",
    "publicationDate": "2018-06-18",
    "domain": "Vision",
    "task": "Object detection",
    "parameters": 3400000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "link": "https://ieeexplore.ieee.org/document/8578572",
    "reference": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
    "citations": "16899.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": null,
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Unknown",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-06"
  },
  {
    "model": "Relational Memory Core",
    "publicationDate": "2018-06-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1806.01822",
    "reference": "Relational recurrent neural networks",
    "citations": "235.0",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United Kingdom",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2018,
    "yearMonth": "2018-06"
  },
  {
    "model": "GPT-1",
    "organization": "OpenAI",
    "publicationDate": "2018-06-01",
    "domain": "Language",
    "task": "Question answering",
    "parameters": 117000000.0,
    "trainingComputeFlop": 1.7578125e+19,
    "confidence": "Likely",
    "link": "https://openai.com/blog/language-unsupervised/",
    "reference": "Improving Language Understanding by Generative Pre-Training",
    "citations": "13586.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA Quadro P600",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "frontierModel": null,
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-06"
  },
  {
    "model": "aLSTM(depth-2)+RecurrentPolicy (WT2)",
    "organization": [
      "University of Manchester",
      "Alan Turing Institute"
    ],
    "publicationDate": "2018-05-22",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 32000000.0,
    "trainingComputeFlop": 7.296e+16,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1805.08574",
    "reference": "Breaking the Activation Function Bottleneck through Adaptive Parameterization",
    "citations": "12.0",
    "organizationCategorization": [
      "Academia",
      "Government"
    ],
    "countryOfOrganization": "United Kingdom",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "University of Manchester",
    "department": null,
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2018,
    "yearMonth": "2018-05"
  },
  {
    "model": "Dropout-LSTM+Noise(Bernoulli) (WT2)",
    "organization": [
      "Columbia University",
      "New York University (NYU)",
      "Princeton University"
    ],
    "publicationDate": "2018-05-03",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 51000000.0,
    "trainingComputeFlop": 1.27e+17,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/1805.01500",
    "reference": "Noisin: Unbiased Regularization for Recurrent Neural Networks",
    "citations": "26.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Columbia University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2018,
    "yearMonth": "2018-05"
  },
  {
    "model": "ResNeXt-101 32x48d",
    "organization": "Facebook",
    "publicationDate": "2018-05-02",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 829000000.0,
    "trainingComputeFlop": 8.74395e+21,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1805.00932",
    "reference": "Exploring the Limits of Weakly Supervised Pretraining",
    "citations": "1422.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "frontierModel": "True",
    "company": "Meta",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-05"
  },
  {
    "model": "YOLOv3",
    "organization": "University of Washington",
    "publicationDate": "2018-04-08",
    "domain": "Vision",
    "task": "Object detection",
    "parameters": 56933216.0,
    "trainingComputeFlop": 1.3416380824e+19,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/1804.02767",
    "reference": "YOLOv3: An Incremental Improvement",
    "citations": "23662.0",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA M40",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "University of Washington",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2018,
    "yearMonth": "2018-04"
  },
  {
    "model": "4 layer QRNN (h=2500)",
    "organization": "Salesforce Research",
    "publicationDate": "2018-03-22",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 151000000.0,
    "trainingComputeFlop": 5.9158815e+17,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/1803.08240",
    "reference": "An Analysis of Neural Language Modeling at Multiple Scales",
    "citations": "183.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Quadro GP100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Salesforce Research",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-03"
  },
  {
    "model": "Chinese - English translation",
    "organization": "Microsoft",
    "publicationDate": "2018-03-01",
    "domain": "Language",
    "task": "Translation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://www.microsoft.com/en-us/research/publication/achieving-human-parity-on-automatic-chinese-to-english-news-translation/",
    "reference": "Achieving Human Parity on Automatic Chinese to English News Translation",
    "citations": "622.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": null,
    "frontierModel": null,
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Unknown",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-03"
  },
  {
    "model": "TCN (P-MNIST)",
    "organization": [
      "Carnegie Mellon University (CMU)",
      "Intel Labs"
    ],
    "publicationDate": "2018-02-15",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 42000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "link": "https://openreview.net/forum?id=rk8wKk-R-",
    "reference": "Convolutional Sequence Modeling Revisited",
    "citations": "5659.0",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": null,
    "frontierModel": null,
    "company": "Carnegie Mellon University (CMU)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Unknown",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2018,
    "yearMonth": "2018-02"
  },
  {
    "model": "DeepLabV3+",
    "organization": "Google",
    "publicationDate": "2018-02-07",
    "domain": "Vision",
    "task": "Semantic segmentation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "link": "https://arxiv.org/abs/1802.02611v3",
    "reference": "Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation",
    "citations": "14950.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": null,
    "frontierModel": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Unknown",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-02"
  },
  {
    "model": "IMPALA",
    "publicationDate": "2018-02-05",
    "domain": "Games",
    "task": "Atari",
    "parameters": 1600000.0,
    "trainingComputeFlop": 1.68e+20,
    "confidence": "Confident",
    "link": "https://arxiv.org/abs/1802.01561",
    "reference": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures",
    "citations": "1717.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA P100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-02"
  },
  {
    "model": "QRNN",
    "organization": "Salesforce Research",
    "publicationDate": "2018-02-01",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 135000000.0,
    "trainingComputeFlop": 6.8866472e+17,
    "confidence": "Likely",
    "link": "https://mlsys.org/Conferences/doc/2018/50.pdf",
    "reference": "Scalable Language Modeling: WikiText-103 on a Single GPU in 12 hours",
    "citations": "5.0",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Salesforce Research",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-02"
  },
  {
    "model": "ELMo",
    "organization": [
      "University of Washington",
      "Allen Institute for AI"
    ],
    "publicationDate": "2018-02-01",
    "domain": "Language",
    "task": "Question answering",
    "parameters": 94000000.0,
    "trainingComputeFlop": 3300100000000010.0,
    "confidence": "Speculative",
    "link": "https://arxiv.org/abs/1802.05365",
    "reference": "Deep contextualized word representations",
    "citations": "11894.0",
    "organizationCategorization": [
      "Academia",
      "Research collective"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": null,
    "frontierModel": null,
    "company": "University of Washington",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Unknown",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2018,
    "yearMonth": "2018-02"
  },
  {
    "model": "T-DMCA",
    "publicationDate": "2018-01-30",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "link": "https://arxiv.org/abs/1801.10198",
    "reference": "Generating Wikipedia by Summarizing Long Sequences",
    "citations": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA P100 PCIe 16GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "frontierModel": null,
    "company": "Google",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-01"
  }
]