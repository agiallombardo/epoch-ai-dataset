[
  {
    "model": "Claude Opus 4.5",
    "organization": "Anthropic",
    "publicationDate": "2025-11-24",
    "domain": "Language",
    "task": "Code generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-11"
  },
  {
    "model": "Gemini 3 Pro",
    "publicationDate": "2025-11-18",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "Google TPU v7 Ironwood",
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-11"
  },
  {
    "model": "GPT-5.1",
    "organization": "OpenAI",
    "publicationDate": "2025-11-13",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-11"
  },
  {
    "model": "Kimi K2 Thinking",
    "organization": "Moonshot",
    "publicationDate": "2025-11-06",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 1000000000000.0,
    "trainingComputeFlop": 4.2e+24,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Kimi K2",
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Moonshot",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Massive (≥1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-11"
  },
  {
    "model": "Tongyi DeepResearch",
    "organization": "Alibaba",
    "publicationDate": "2025-10-28",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 30500000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Qwen3-30B-A3B",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-10"
  },
  {
    "model": "MiniMax-M2",
    "organization": "MiniMax",
    "publicationDate": "2025-10-27",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 229000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "MiniMax",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-10"
  },
  {
    "model": "Veo 3.1",
    "publicationDate": "2025-10-15",
    "domain": "Video",
    "task": "Image-to-video",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-10"
  },
  {
    "model": "Claude Haiku 4.5",
    "organization": "Anthropic",
    "publicationDate": "2025-10-15",
    "domain": "Language",
    "task": "Chat",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-10"
  },
  {
    "model": "Ling-1T",
    "organization": "Ant Group",
    "publicationDate": "2025-10-10",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 1000000000000.0,
    "trainingComputeFlop": 6.000001e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Ant Group",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Massive (≥1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-10"
  },
  {
    "model": "GPT-5 Pro",
    "organization": "OpenAI",
    "publicationDate": "2025-10-07",
    "domain": "Multimodal",
    "task": null,
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-10"
  },
  {
    "model": "Sora 2.0",
    "organization": "OpenAI",
    "publicationDate": "2025-09-30",
    "domain": "Video",
    "task": "Video generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-09"
  },
  {
    "model": "GLM 4.6",
    "organization": [
      "Zhipu AI",
      "Tsinghua University"
    ],
    "publicationDate": "2025-09-30",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 357000000000.0,
    "trainingComputeFlop": 4.42e+24,
    "confidence": "Likely",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Zhipu AI",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2025,
    "yearMonth": "2025-09"
  },
  {
    "model": "Claude Sonnet 4.5",
    "organization": "Anthropic",
    "publicationDate": "2025-09-29",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-09"
  },
  {
    "model": "Gemini Robotics-ER 1.5",
    "publicationDate": "2025-09-25",
    "domain": "Vision",
    "task": "Instruction interpretation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-09"
  },
  {
    "model": "Qwen3-Omni-30B-A3B",
    "organization": "Alibaba",
    "publicationDate": "2025-09-22",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": 35300000000.0,
    "trainingComputeFlop": 3.6e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-09"
  },
  {
    "model": "AgentFounder-30B",
    "organization": "Alibaba",
    "publicationDate": "2025-09-16",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 30000000000.0,
    "trainingComputeFlop": 6.5367e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Qwen3-30B-A3B",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-09"
  },
  {
    "model": "Qwen3-Max",
    "organization": "Alibaba",
    "publicationDate": "2025-09-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 1000000000000.0,
    "trainingComputeFlop": 1.512e+25,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Massive (≥1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-09"
  },
  {
    "model": "LongCat-Flash",
    "organization": "Meituan Inc",
    "publicationDate": "2025-09-01",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 560000000000.0,
    "trainingComputeFlop": 3.726e+24,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meituan Inc",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-09"
  },
  {
    "model": "Gemini 2.5 Flash Image (Nano Banana)",
    "organization": "Google",
    "publicationDate": "2025-08-26",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": null,
    "baseModel": "Gemini 2.5 Flash",
    "modelAccessibility": "API access",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-08"
  },
  {
    "model": "GPT-5 nano",
    "organization": "OpenAI",
    "publicationDate": "2025-08-07",
    "domain": "Multimodal",
    "task": null,
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-08"
  },
  {
    "model": "GPT-5 mini",
    "organization": "OpenAI",
    "publicationDate": "2025-08-07",
    "domain": "Multimodal",
    "task": null,
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-08"
  },
  {
    "model": "GPT-5",
    "organization": "OpenAI",
    "publicationDate": "2025-08-07",
    "domain": "Multimodal",
    "task": null,
    "parameters": null,
    "trainingComputeFlop": 6.6e+25,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-08"
  },
  {
    "model": "gpt-oss-120b",
    "organization": "OpenAI",
    "publicationDate": "2025-08-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 116830000000.0,
    "trainingComputeFlop": 4.94e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-08"
  },
  {
    "model": "gpt-oss-20b",
    "organization": "OpenAI",
    "publicationDate": "2025-08-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 20910000000.0,
    "trainingComputeFlop": 5.49e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-08"
  },
  {
    "model": "GLM 4.5",
    "organization": [
      "Zhipu AI",
      "Tsinghua University"
    ],
    "publicationDate": "2025-08-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 355000000000.0,
    "trainingComputeFlop": 4.42e+24,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Zhipu AI",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2025,
    "yearMonth": "2025-08"
  },
  {
    "model": "Claude Opus 4.1",
    "organization": "Anthropic",
    "publicationDate": "2025-08-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-08"
  },
  {
    "model": "Qwen Image",
    "organization": "Alibaba",
    "publicationDate": "2025-08-04",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 27000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Qwen2.5-VL-7B",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-08"
  },
  {
    "model": "Hierarchical Reasoning Model (HPM)",
    "organization": "Sapient Intelligence",
    "publicationDate": "2025-08-04",
    "domain": "Language",
    "task": "Visual puzzles",
    "parameters": 27000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": null,
    "countryOfOrganization": "Singapore",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Sapient Intelligence",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2025,
    "yearMonth": "2025-08"
  },
  {
    "model": "MindLink-72B",
    "organization": "Kunlun Inc.",
    "publicationDate": "2025-08-01",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 72000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A800 PCIe 80 GB",
    "baseModel": "Qwen2.5-72B",
    "modelAccessibility": null,
    "company": "Kunlun Inc.",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Unknown",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-08"
  },
  {
    "model": "Gemini 2.5 Deep Think",
    "publicationDate": "2025-08-01",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-08"
  },
  {
    "model": "Qwen3-Coder-480B-A35B",
    "organization": "Alibaba",
    "publicationDate": "2025-07-22",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 480000000000.0,
    "trainingComputeFlop": 1.575e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-07"
  },
  {
    "model": "EXAONE 4.0 (32B)",
    "organization": "LG AI Research",
    "publicationDate": "2025-07-15",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 32000000000.0,
    "trainingComputeFlop": 2.69000000000001e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Korea (Republic of)",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H200 SXM",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "LG AI Research",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-07"
  },
  {
    "model": "Gemini Embedding",
    "publicationDate": "2025-07-14",
    "domain": "Language",
    "task": "Semantic embedding",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-07"
  },
  {
    "model": "Kimi K2",
    "organization": "Moonshot",
    "publicationDate": "2025-07-11",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 1000000000000.0,
    "trainingComputeFlop": 2.976e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H800 SXM5",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Moonshot",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Massive (≥1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-07"
  },
  {
    "model": "Grok 4 Heavy",
    "organization": "xAI",
    "publicationDate": "2025-07-10",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "xAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-07"
  },
  {
    "model": "Grok 4",
    "organization": "xAI",
    "publicationDate": "2025-07-09",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": 5.0000000000001e+26,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "xAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-07"
  },
  {
    "model": "EXAONE Path 2.0",
    "organization": "LG AI Research",
    "publicationDate": "2025-07-09",
    "domain": "Vision",
    "task": "Cancer diagnosis",
    "parameters": 175000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Korea (Republic of)",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "LG AI Research",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-07"
  },
  {
    "model": "dots.llm1",
    "organization": "Rednote",
    "publicationDate": "2025-07-06",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 142000000000.0,
    "trainingComputeFlop": 1.2164856e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA H800 SXM5",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Rednote",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-07"
  },
  {
    "model": "FGN",
    "publicationDate": "2025-06-12",
    "domain": "Earth science",
    "task": "Weather forecasting",
    "parameters": 720000000.0,
    "trainingComputeFlop": 9.618950880000001e+21,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v5p",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-06"
  },
  {
    "model": "Seed-1.6-Thinking",
    "organization": "ByteDance",
    "publicationDate": "2025-06-11",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": 230000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "ByteDance",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-06"
  },
  {
    "model": "Qwen3 Embedding",
    "organization": "Alibaba",
    "publicationDate": "2025-06-05",
    "domain": "Language",
    "task": "Semantic embedding",
    "parameters": 8000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Qwen3-8B",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-06"
  },
  {
    "model": "Claude Sonnet 4",
    "organization": "Anthropic",
    "publicationDate": "2025-05-22",
    "domain": "Language",
    "task": "Code generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-05"
  },
  {
    "model": "Claude Opus 4",
    "organization": "Anthropic",
    "publicationDate": "2025-05-22",
    "domain": "Language",
    "task": "Code generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-05"
  },
  {
    "model": "Veo 3",
    "publicationDate": "2025-05-21",
    "domain": "Video",
    "task": "Video generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-05"
  },
  {
    "model": "Imagen 4",
    "organization": "Google",
    "publicationDate": "2025-05-20",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-05"
  },
  {
    "model": "Seed1.5-VL",
    "organization": "ByteDance",
    "publicationDate": "2025-05-11",
    "domain": "Vision",
    "task": "Visual question answering",
    "parameters": null,
    "trainingComputeFlop": 1.388556e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA H800 SXM5",
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "ByteDance",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-05"
  },
  {
    "model": "Qwen3-235B-A22B",
    "organization": "Alibaba",
    "publicationDate": "2025-04-29",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 235000000000.0,
    "trainingComputeFlop": 4.752e+24,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-04"
  },
  {
    "model": "gpt-image-1",
    "organization": "OpenAI",
    "publicationDate": "2025-04-23",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-04"
  },
  {
    "model": "Pangu Ultra",
    "organization": "Huawei",
    "publicationDate": "2025-04-10",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 135000000000.0,
    "trainingComputeFlop": 1.0692e+25,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Huawei Ascend 910B",
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "Huawei",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-04"
  },
  {
    "model": "Llama 4 Scout",
    "publicationDate": "2025-04-05",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 109000000000.0,
    "trainingComputeFlop": 4.08e+24,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-04"
  },
  {
    "model": "Llama 4 Maverick",
    "publicationDate": "2025-04-05",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 400000000000.0,
    "trainingComputeFlop": 2.244000000001e+24,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-04"
  },
  {
    "model": "Llama 4 Behemoth (preview)",
    "publicationDate": "2025-04-05",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 2000000000000.0,
    "trainingComputeFlop": 5.18400000000001e+25,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Massive (≥1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-04"
  },
  {
    "model": "Gemini 2.5 Pro",
    "publicationDate": "2025-03-25",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-03"
  },
  {
    "model": "Diffusion Renderer",
    "organization": [
      "NVIDIA",
      "University of Toronto",
      "Vector Institute",
      "University of Illinois Urbana-Champaign (UIUC)"
    ],
    "publicationDate": "2025-03-22",
    "domain": "Video",
    "task": "Video editing",
    "parameters": 1100000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Canada"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": "Stable Video Diffusion",
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2025,
    "yearMonth": "2025-03"
  },
  {
    "model": "EXAONE Deep 32B",
    "organization": "LG AI Research",
    "publicationDate": "2025-03-16",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 32000000000.0,
    "trainingComputeFlop": 1.26e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Korea (Republic of)",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": "EXAONE 3.5 32B",
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "LG AI Research",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-03"
  },
  {
    "model": "ERNIE-4.5-VL-424B-A47B (文心大模型4.5)",
    "organization": "Baidu",
    "publicationDate": "2025-03-16",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": 424000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "ERNIE-4.5-300B-A47B",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Baidu",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-03"
  },
  {
    "model": "Hunyuan-TurboS",
    "organization": "Tencent",
    "publicationDate": "2025-03-11",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 560000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Tencent",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-03"
  },
  {
    "model": "QwQ-32B",
    "organization": "Alibaba",
    "publicationDate": "2025-03-06",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 32500000000.0,
    "trainingComputeFlop": 3.51e+24,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Qwen2.5-Coder (32B)",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-03"
  },
  {
    "model": "Mistral OCR",
    "organization": "Mistral AI",
    "publicationDate": "2025-03-06",
    "domain": "Multimodal",
    "task": "Character recognition (OCR)",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "France",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Mistral AI",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-03"
  },
  {
    "model": "Mercury",
    "organization": "Inception Labs",
    "publicationDate": "2025-02-27",
    "domain": "Language",
    "task": "Code generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Inception Labs",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-02"
  },
  {
    "model": "GPT-4.5",
    "organization": "OpenAI",
    "publicationDate": "2025-02-27",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": 3.8e+26,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-02"
  },
  {
    "model": "Claude 3.7 Sonnet",
    "organization": "Anthropic",
    "publicationDate": "2025-02-24",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": 3.35e+25,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-02"
  },
  {
    "model": "Grok 3",
    "organization": "xAI",
    "publicationDate": "2025-02-17",
    "domain": "Language",
    "task": "Chat",
    "parameters": null,
    "trainingComputeFlop": 3.5e+26,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "xAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-02"
  },
  {
    "model": "Eurus-2-7B-PRIME",
    "organization": [
      "Tsinghua University",
      "University of Illinois Urbana-Champaign (UIUC)",
      "Shanghai AI Lab",
      "Peking University",
      "Shanghai Jiao Tong University",
      "CUHK Shenzhen Research Institute"
    ],
    "publicationDate": "2025-02-03",
    "domain": "Mathematics",
    "task": "Mathematical reasoning",
    "parameters": 7000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "China",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Qwen2.5-Math-7B-Base",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2025,
    "yearMonth": "2025-02"
  },
  {
    "model": "o3-mini",
    "organization": "OpenAI",
    "publicationDate": "2025-01-31",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-01"
  },
  {
    "model": "Computer-Using Agent (CUA)",
    "organization": "OpenAI",
    "publicationDate": "2025-01-23",
    "domain": "Vision",
    "task": "Instruction interpretation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-01"
  },
  {
    "model": "Kimi k1.5",
    "organization": "Moonshot",
    "publicationDate": "2025-01-22",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Moonshot",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-01"
  },
  {
    "model": "Doubao-1.5-pro",
    "organization": "ByteDance",
    "publicationDate": "2025-01-22",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "ByteDance",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-01"
  },
  {
    "model": "Eagle 2",
    "organization": [
      "NVIDIA",
      "Nanjing University",
      "Tsinghua University",
      "Hong Kong Polytechnic University",
      "Johns Hopkins University",
      "New York University (NYU)"
    ],
    "publicationDate": "2025-01-20",
    "domain": "Vision",
    "task": null,
    "parameters": 8930000000.0,
    "trainingComputeFlop": 4.7156e+22,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "China",
      "Hong Kong"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": "Qwen2.5-7B",
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2025,
    "yearMonth": "2025-01"
  },
  {
    "model": "DeepSeek-R1",
    "organization": "DeepSeek",
    "publicationDate": "2025-01-20",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 671000000000.0,
    "trainingComputeFlop": 4.020010000000001e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": "DeepSeek-V3",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "DeepSeek",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-01"
  },
  {
    "model": "INTELLECT-MATH",
    "organization": "Prime Intellect",
    "publicationDate": "2025-01-17",
    "domain": "Mathematics",
    "task": "Mathematical reasoning",
    "parameters": 7000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Qwen2.5-Math-7B-Base",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Prime Intellect",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2025,
    "yearMonth": "2025-01"
  },
  {
    "model": "Stable Point Aware 3D (SPAR3D)",
    "organization": [
      "Stability AI",
      "University of Illinois Urbana-Champaign (UIUC)"
    ],
    "publicationDate": "2025-01-08",
    "domain": "3D modeling",
    "task": "3D reconstruction",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United Kingdom",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Stability AI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2025,
    "yearMonth": "2025-01"
  },
  {
    "model": "STORM-B/8",
    "organization": [
      "University of Southern California",
      "Georgia Institute of Technology",
      "Stanford University",
      "NVIDIA"
    ],
    "publicationDate": "2024-12-31",
    "domain": "3D modeling",
    "task": "3D reconstruction",
    "parameters": 100598707.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": null,
    "company": "University of Southern California",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Unknown",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "DeepSeek-V3",
    "organization": "DeepSeek",
    "publicationDate": "2024-12-24",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 671000000000.0,
    "trainingComputeFlop": 3.4078e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H800 SXM5",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "DeepSeek",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "o3",
    "organization": "OpenAI",
    "publicationDate": "2024-12-20",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "Veo 2",
    "publicationDate": "2024-12-16",
    "domain": "Video",
    "task": "Video generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "Apollo 7B",
    "publicationDate": "2024-12-13",
    "domain": "Video",
    "task": "Video description",
    "parameters": 7000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": [
      "Qwen2.5-7B",
      "SigLIP 400M"
    ],
    "modelAccessibility": "Unreleased",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "Gemini 2.0 Pro",
    "publicationDate": "2024-12-11",
    "domain": "Language",
    "task": "Code generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "Sora Turbo",
    "organization": "OpenAI",
    "publicationDate": "2024-12-09",
    "domain": "Video",
    "task": "Video generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "EXAONE 3.5 32B",
    "organization": "LG AI Research",
    "publicationDate": "2024-12-09",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 32000000000.0,
    "trainingComputeFlop": 1.25e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Korea (Republic of)",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "LG AI Research",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "Llama 3.3 70B",
    "publicationDate": "2024-12-06",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 70000000000.0,
    "trainingComputeFlop": 6.8649768e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "o1",
    "organization": "OpenAI",
    "publicationDate": "2024-12-05",
    "domain": "Language",
    "task": "Code generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "NVILA 15B",
    "organization": [
      "NVIDIA",
      "Massachusetts Institute of Technology (MIT)",
      "University of California (UC) Berkeley",
      "University of California San Diego",
      "University of Washington",
      "Tsinghua University"
    ],
    "publicationDate": "2024-12-05",
    "domain": "Vision",
    "task": "Visual question answering",
    "parameters": 15000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "China"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "Infinity",
    "organization": "ByteDance",
    "publicationDate": "2024-12-05",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 2000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "ByteDance",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "Amazon Nova Pro",
    "organization": "Amazon",
    "publicationDate": "2024-12-03",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": 6.000010000000001e+24,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Amazon Trainium1",
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Amazon",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-12"
  },
  {
    "model": "Fugatto 1",
    "organization": "NVIDIA",
    "publicationDate": "2024-11-25",
    "domain": "Multimodal",
    "task": "Audio generation",
    "parameters": 2500000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-11"
  },
  {
    "model": "Suno v4",
    "organization": "Suno",
    "publicationDate": "2024-11-19",
    "domain": "Audio",
    "task": "Audio generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "Suno",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-11"
  },
  {
    "model": "Pixtral Large",
    "organization": "Mistral AI",
    "publicationDate": "2024-11-18",
    "domain": "Multimodal",
    "task": "Vision-language generation",
    "parameters": 124000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "France",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": "Mistral Large 2",
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Mistral AI",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-11"
  },
  {
    "model": "k0-math",
    "organization": "Moonshot",
    "publicationDate": "2024-11-16",
    "domain": "Mathematics",
    "task": "Mathematical reasoning",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Moonshot",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-11"
  },
  {
    "model": "Gemini-Exp-1114",
    "publicationDate": "2024-11-15",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-11"
  },
  {
    "model": "SeedEdit",
    "organization": "ByteDance",
    "publicationDate": "2024-11-11",
    "domain": "Vision",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "ByteDance",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-11"
  },
  {
    "model": "Hunyuan-Large",
    "organization": "Tencent",
    "publicationDate": "2024-11-06",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 389000000000.0,
    "trainingComputeFlop": 3.49237e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Tencent",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-11"
  },
  {
    "model": "Doubao-pro",
    "organization": "ByteDance",
    "publicationDate": "2024-10-28",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 500000000000.0,
    "trainingComputeFlop": 2.505e+25,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "ByteDance",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-10"
  },
  {
    "model": "NVLM-X 72B",
    "organization": "NVIDIA",
    "publicationDate": "2024-10-22",
    "domain": "Vision",
    "task": "Language modeling/generation",
    "parameters": 72000000000.0,
    "trainingComputeFlop": 3.0398181e+24,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": [
      "Qwen2-72B",
      "InternViT-6B"
    ],
    "modelAccessibility": "Unreleased",
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-10"
  },
  {
    "model": "NVLM-H 72B",
    "organization": "NVIDIA",
    "publicationDate": "2024-10-22",
    "domain": "Vision",
    "task": "Language modeling/generation",
    "parameters": 72000000000.0,
    "trainingComputeFlop": 3.02e+24,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": [
      "Qwen2-72B",
      "InternViT-6B"
    ],
    "modelAccessibility": "Unreleased",
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-10"
  },
  {
    "model": "NVLM-D 72B",
    "organization": "NVIDIA",
    "publicationDate": "2024-10-22",
    "domain": "Vision",
    "task": "Language modeling/generation",
    "parameters": 72000000000.0,
    "trainingComputeFlop": 3.02e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": [
      "Qwen2-72B",
      "InternViT-6B"
    ],
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-10"
  },
  {
    "model": "Yi-Lightning",
    "organization": "01.AI",
    "publicationDate": "2024-10-18",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": 1.5e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "01.AI",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-10"
  },
  {
    "model": "CHAI-1",
    "organization": "Chai discovery",
    "publicationDate": "2024-10-15",
    "domain": "Biology",
    "task": "Protein folding prediction",
    "parameters": null,
    "trainingComputeFlop": 7.7605724e+21,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Chai discovery",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-10"
  },
  {
    "model": "RDT-1B",
    "organization": "Tsinghua University",
    "publicationDate": "2024-10-10",
    "domain": "Robotics",
    "task": "Robotic manipulation",
    "parameters": 1200000000.0,
    "trainingComputeFlop": 4.06e+22,
    "confidence": "Likely",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA H100 PCIe",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2024,
    "yearMonth": "2024-10"
  },
  {
    "model": "Palmyra X 004",
    "organization": "Writer",
    "publicationDate": "2024-10-09",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 150000000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Writer",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-10"
  },
  {
    "model": "GR-2",
    "organization": "ByteDance",
    "publicationDate": "2024-10-08",
    "domain": "Robotics",
    "task": "Video",
    "parameters": 230000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "ByteDance",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-10"
  },
  {
    "model": "Movie Gen Video",
    "publicationDate": "2024-10-04",
    "domain": "Video",
    "task": "Video generation",
    "parameters": 30000000000.0,
    "trainingComputeFlop": 1.65e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-10"
  },
  {
    "model": "PixelDance",
    "organization": "ByteDance",
    "publicationDate": "2024-09-24",
    "domain": "Video",
    "task": "Video generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "ByteDance",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "Llama 3.2 11B",
    "publicationDate": "2024-09-24",
    "domain": "Multimodal",
    "task": "Visual question answering",
    "parameters": 10600000000.0,
    "trainingComputeFlop": 5.79e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": "Llama 3.1-8B",
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "Telechat2-115B",
    "organization": "China Telecom",
    "publicationDate": "2024-09-20",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 115000000000.0,
    "trainingComputeFlop": 6.9e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "China Telecom",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "Qwen2.5-72B",
    "organization": "Alibaba",
    "publicationDate": "2024-09-19",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 72700000000.0,
    "trainingComputeFlop": 7.8e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "Qwen2.5 Instruct (72B)",
    "organization": "Alibaba",
    "publicationDate": "2024-09-19",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 72700000000.0,
    "trainingComputeFlop": 7.8516e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": "Qwen2.5-72B",
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "Oryx 34B",
    "organization": [
      "Tsinghua University",
      "Tencent",
      "Nanyang Technological University"
    ],
    "publicationDate": "2024-09-19",
    "domain": "Multimodal",
    "task": "Visual question answering",
    "parameters": 34000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "China",
      "Singapore"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A800 SXM",
    "baseModel": "Yi-1.5-34B",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "Qwen2.5-32B",
    "organization": "Alibaba",
    "publicationDate": "2024-09-17",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 32500000000.0,
    "trainingComputeFlop": 3.51e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "o1-preview",
    "organization": "OpenAI",
    "publicationDate": "2024-09-12",
    "domain": "Language",
    "task": "Code generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "o1-mini",
    "organization": "OpenAI",
    "publicationDate": "2024-09-12",
    "domain": "Language",
    "task": "Code generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "DeepSeek-V2.5",
    "organization": "DeepSeek",
    "publicationDate": "2024-09-06",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 236000000000.0,
    "trainingComputeFlop": 1.7892e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "DeepSeek",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "Hunyuan Turbo",
    "organization": "Tencent",
    "publicationDate": "2024-09-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Tencent",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "Harrison.rad.1",
    "organization": "Harrison.ai",
    "publicationDate": "2024-09-05",
    "domain": "Vision",
    "task": "Visual question answering",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Australia",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Harrison.ai",
    "department": null,
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "AlphaProteo",
    "publicationDate": "2024-09-05",
    "domain": "Biology",
    "task": "Protein generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-09"
  },
  {
    "model": "Hairuo",
    "organization": "Inspur",
    "publicationDate": "2024-08-29",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "Inspur",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-08"
  },
  {
    "model": "GLM-4-Plus",
    "organization": "Zhipu AI",
    "publicationDate": "2024-08-29",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Zhipu AI",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-08"
  },
  {
    "model": "Jamba 1.5-Large",
    "organization": "AI21 Labs",
    "publicationDate": "2024-08-22",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 398000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Israel",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "AI21 Labs",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-08"
  },
  {
    "model": "Grok-2",
    "organization": "xAI",
    "publicationDate": "2024-08-13",
    "domain": "Language",
    "task": "Chat",
    "parameters": null,
    "trainingComputeFlop": 2.96e+25,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "xAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-08"
  },
  {
    "model": "Table Tennis Agent",
    "publicationDate": "2024-08-07",
    "domain": "Robotics",
    "task": "Sports",
    "parameters": 185000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-08"
  },
  {
    "model": "LLaVA-OV-72B",
    "organization": [
      "ByteDance",
      "Nanyang Technological University",
      "Chinese University of Hong Kong (CUHK)",
      "Hong Kong University of Science and Technology (HKUST)"
    ],
    "publicationDate": "2024-08-06",
    "domain": "Multimodal",
    "task": "Image captioning",
    "parameters": 72000000000.0,
    "trainingComputeFlop": 3.036551985824e+24,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "China",
      "Singapore",
      "Hong Kong"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Qwen2-72B",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "ByteDance",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-08"
  },
  {
    "model": "AFM-server",
    "organization": "Apple",
    "publicationDate": "2024-07-29",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": 4.3e+24,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "Apple",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-07"
  },
  {
    "model": "AFM-on-device",
    "organization": "Apple",
    "publicationDate": "2024-07-29",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 2730000000.0,
    "trainingComputeFlop": 4.5126e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "Google TPU v5p",
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "Apple",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-07"
  },
  {
    "model": "Mistral Large 2",
    "organization": "Mistral AI",
    "publicationDate": "2024-07-24",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 123000000000.0,
    "trainingComputeFlop": 2.13e+25,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "France",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Mistral AI",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-07"
  },
  {
    "model": "Llama 3.1-405B",
    "publicationDate": "2024-07-23",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 405000000000.0,
    "trainingComputeFlop": 3.8e+25,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-07"
  },
  {
    "model": "GPT-4o mini",
    "organization": "OpenAI",
    "publicationDate": "2024-07-18",
    "domain": "Language",
    "task": "Chat",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-07"
  },
  {
    "model": "Mathstral",
    "organization": "Mistral AI",
    "publicationDate": "2024-07-16",
    "domain": "Mathematics",
    "task": "Mathematical reasoning",
    "parameters": 7000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "France",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Mistral 7B",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Mistral AI",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-07"
  },
  {
    "model": "DeepL LLM",
    "organization": "DeepL",
    "publicationDate": "2024-07-16",
    "domain": "Language",
    "task": "Translation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Germany",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "DeepL",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-07"
  },
  {
    "model": "SenseChat 5.5",
    "organization": "SenseTime",
    "publicationDate": "2024-07-06",
    "domain": "Multimodal",
    "task": "Vision-language generation",
    "parameters": 600000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Hong Kong",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "SenseTime",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-07"
  },
  {
    "model": "Ernie 4.0 Turbo",
    "organization": "Baidu",
    "publicationDate": "2024-06-28",
    "domain": "Multimodal",
    "task": "Vision-language generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Baidu",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-06"
  },
  {
    "model": "ESM3 (98B)",
    "organization": [
      "EvolutionaryScale",
      "University of California (UC) Berkeley"
    ],
    "publicationDate": "2024-06-25",
    "domain": "Biology",
    "task": "Protein generation",
    "parameters": 98500000000.0,
    "trainingComputeFlop": 1.07e+24,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "EvolutionaryScale",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-06"
  },
  {
    "model": "Cambrian-1-34B",
    "organization": "New York University (NYU)",
    "publicationDate": "2024-06-24",
    "domain": "Multimodal",
    "task": "Image captioning",
    "parameters": 34000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": "Yi-34B",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "New York University (NYU)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Public",
    "year": 2024,
    "yearMonth": "2024-06"
  },
  {
    "model": "Claude 3.5 Sonnet",
    "organization": "Anthropic",
    "publicationDate": "2024-06-20",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": null,
    "trainingComputeFlop": 2.700000000000001e+25,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-06"
  },
  {
    "model": "DeepSeek-Coder-V2 236B",
    "organization": "DeepSeek",
    "publicationDate": "2024-06-17",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 236000000000.0,
    "trainingComputeFlop": 1.2852e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "DeepSeek-V2 (MoE-236B)",
    "modelAccessibility": "Open weights (restricted use)",
    "company": "DeepSeek",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-06"
  },
  {
    "model": "Nemotron-4 340B",
    "organization": "NVIDIA",
    "publicationDate": "2024-06-14",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 340000000000.0,
    "trainingComputeFlop": 1.8e+25,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-06"
  },
  {
    "model": "OpenVLA",
    "publicationDate": "2024-06-13",
    "domain": "Robotics",
    "task": "Robotic manipulation",
    "parameters": 7188100000.0,
    "trainingComputeFlop": 1.1e+23,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": "Llama 2-7B",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Stanford University",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-06"
  },
  {
    "model": "Llama-3.1-Nemotron-70B-Instruct",
    "publicationDate": "2024-06-12",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": 7.929e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Llama 3.1-70B",
    "modelAccessibility": "Open weights (restricted use)",
    "company": "NVIDIA",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-06"
  },
  {
    "model": "Qwen2-72B",
    "organization": "Alibaba",
    "publicationDate": "2024-06-07",
    "domain": "Language",
    "task": "Chat",
    "parameters": 72710000000.0,
    "trainingComputeFlop": 3.02e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-06"
  },
  {
    "model": "ALLaM adapted 70B",
    "organization": "Saudi Data and Artificial Intelligence Authority",
    "publicationDate": "2024-05-21",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 70000000000.0,
    "trainingComputeFlop": 1.062e+24,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Government"
    ],
    "countryOfOrganization": "Saudi Arabia",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": "Llama 2-70B",
    "modelAccessibility": "Unreleased",
    "company": "Saudi Data and Artificial Intelligence Authority",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-05"
  },
  {
    "model": "ALLaM 7B",
    "organization": "Saudi Data and Artificial Intelligence Authority",
    "publicationDate": "2024-05-21",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 7000000000.0,
    "trainingComputeFlop": 9.04e+22,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Government"
    ],
    "countryOfOrganization": "Saudi Arabia",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": "LLaMA-7B",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Saudi Data and Artificial Intelligence Authority",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-05"
  },
  {
    "model": "Octo-Base",
    "publicationDate": "2024-05-20",
    "domain": "Robotics",
    "task": "Robotic manipulation",
    "parameters": 93000000.0,
    "trainingComputeFlop": 5.85e+20,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "University of California (UC) Berkeley",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-05"
  },
  {
    "model": "GLM-4 (0520)",
    "organization": "Zhipu AI",
    "publicationDate": "2024-05-20",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": "GLM-4 (0116)",
    "modelAccessibility": "API access",
    "company": "Zhipu AI",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-05"
  },
  {
    "model": "Yi-Large",
    "organization": "01.AI",
    "publicationDate": "2024-05-13",
    "domain": "Language",
    "task": "Chat",
    "parameters": 100000000000.0,
    "trainingComputeFlop": 1.8e+24,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "01.AI",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-05"
  },
  {
    "model": "GPT-4o",
    "organization": "OpenAI",
    "publicationDate": "2024-05-13",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-05"
  },
  {
    "model": "AlphaFold 3",
    "publicationDate": "2024-05-08",
    "domain": "Biology",
    "task": "Protein folding prediction",
    "parameters": null,
    "trainingComputeFlop": 4.1405645e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": [
      "United States",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-05"
  },
  {
    "model": "VILA1.5-13B",
    "organization": [
      "NVIDIA",
      "Massachusetts Institute of Technology (MIT)"
    ],
    "publicationDate": "2024-05-03",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 13493916736.0,
    "trainingComputeFlop": 2.3003136e+21,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": [
      "SigLIP 400M",
      "Vicuna-13B-v1.5"
    ],
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-05"
  },
  {
    "model": "GenCast",
    "publicationDate": "2024-05-01",
    "domain": "Earth science",
    "task": "Weather forecasting",
    "parameters": null,
    "trainingComputeFlop": 8.169984e+20,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v5e",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-05"
  },
  {
    "model": "Llama 3-70B",
    "publicationDate": "2024-04-18",
    "domain": "Language",
    "task": "Chat",
    "parameters": 70000000000.0,
    "trainingComputeFlop": 7.861e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-04"
  },
  {
    "model": "Reka Core",
    "organization": "Reka AI",
    "publicationDate": "2024-04-15",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 67000000000.0,
    "trainingComputeFlop": 8.400010000000001e+24,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Reka AI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-04"
  },
  {
    "model": "ReALM",
    "organization": "Apple",
    "publicationDate": "2024-03-29",
    "domain": "Language",
    "task": "Named entity recognition (NER)",
    "parameters": 3000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Flan-T5 11B",
    "modelAccessibility": "Unreleased",
    "company": "Apple",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-03"
  },
  {
    "model": "DBRX",
    "organization": "Databricks",
    "publicationDate": "2024-03-27",
    "domain": "Language",
    "task": "Chat",
    "parameters": 132000000000.0,
    "trainingComputeFlop": 2.6e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Databricks",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-03"
  },
  {
    "model": "MM1-30B",
    "organization": "Apple",
    "publicationDate": "2024-03-14",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 30000000000.0,
    "trainingComputeFlop": 4.86e+23,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Apple",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-03"
  },
  {
    "model": "ManiGaussian",
    "organization": [
      "Tsinghua University",
      "Nanyang Technological University",
      "Carnegie Mellon University (CMU)"
    ],
    "publicationDate": "2024-03-13",
    "domain": "Robotics",
    "task": "Robotic manipulation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "China",
      "Singapore",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA GeForce RTX 4090",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2024,
    "yearMonth": "2024-03"
  },
  {
    "model": "Inflection-2.5",
    "organization": "Inflection AI",
    "publicationDate": "2024-03-07",
    "domain": "Language",
    "task": "Chat",
    "parameters": null,
    "trainingComputeFlop": 8.000001e+24,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "Inflection AI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-03"
  },
  {
    "model": "Claude 3 Sonnet",
    "organization": "Anthropic",
    "publicationDate": "2024-03-04",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-03"
  },
  {
    "model": "Claude 3 Opus",
    "organization": "Anthropic",
    "publicationDate": "2024-03-04",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-03"
  },
  {
    "model": "Aramco Metabrain AI",
    "organization": "Saudi Aramco",
    "publicationDate": "2024-03-04",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 250000000000.0,
    "trainingComputeFlop": 1.05e+25,
    "confidence": "Likely",
    "organizationCategorization": [
      "Industry",
      "Government"
    ],
    "countryOfOrganization": "Saudi Arabia",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Saudi Aramco",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-03"
  },
  {
    "model": "Mistral Large",
    "organization": "Mistral AI",
    "publicationDate": "2024-02-26",
    "domain": "Language",
    "task": "Chat",
    "parameters": null,
    "trainingComputeFlop": 1.12e+25,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "France",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Mistral AI",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-02"
  },
  {
    "model": "MegaScale (Production)",
    "organization": [
      "ByteDance",
      "Peking University"
    ],
    "publicationDate": "2024-02-23",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 530000000000.0,
    "trainingComputeFlop": 3.9e+24,
    "confidence": "Speculative",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "ByteDance",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-02"
  },
  {
    "model": "Stable Diffusion 3",
    "organization": "Stability AI",
    "publicationDate": "2024-02-22",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 8000000000.0,
    "trainingComputeFlop": 5.0000000000000004e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United Kingdom",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Stability AI",
    "department": null,
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-02"
  },
  {
    "model": "Sora",
    "organization": "OpenAI",
    "publicationDate": "2024-02-15",
    "domain": "Video",
    "task": "Video generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-02"
  },
  {
    "model": "Gemini 1.5 Pro",
    "publicationDate": "2024-02-15",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-02"
  },
  {
    "model": "Aya",
    "organization": [
      "Cohere for AI",
      "Brown University",
      "Cohere",
      "Carnegie Mellon University (CMU)",
      "Massachusetts Institute of Technology (MIT)"
    ],
    "publicationDate": "2024-02-12",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 13000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "Canada",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": "mT5-XXL",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Cohere for AI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-02"
  },
  {
    "model": "Qwen1.5-72B",
    "organization": "Alibaba",
    "publicationDate": "2024-02-04",
    "domain": "Language",
    "task": "Chat",
    "parameters": 72000000000.0,
    "trainingComputeFlop": 1.3e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-02"
  },
  {
    "model": "Qwen-VL-Max",
    "organization": "Alibaba",
    "publicationDate": "2024-01-25",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 7000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-01"
  },
  {
    "model": "AlphaGeometry",
    "publicationDate": "2024-01-17",
    "domain": "Mathematics",
    "task": "Geometry",
    "parameters": 151000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2024,
    "yearMonth": "2024-01"
  },
  {
    "model": "Palmyra X 003",
    "organization": "Writer",
    "publicationDate": "2024-01-01",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 72000000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Writer",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-01"
  },
  {
    "model": "Kimi Explorer",
    "organization": "Moonshot",
    "publicationDate": "2024-01-01",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "Moonshot",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2024,
    "yearMonth": "2024-01"
  },
  {
    "model": "CoRe",
    "organization": "Tsinghua University",
    "publicationDate": "2023-12-29",
    "domain": "Mathematics",
    "task": "Quantitative reasoning",
    "parameters": 12400000000.0,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": "GPT-J-6B",
    "modelAccessibility": "Unreleased",
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "GQA-8-XXL",
    "publicationDate": "2023-12-23",
    "domain": "Language",
    "task": "Text summarization",
    "parameters": 11000000000.0,
    "trainingComputeFlop": 3.4912896e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "Google TPU v3",
    "baseModel": "T5-11B",
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "nekomata-14b",
    "organization": "rinna",
    "publicationDate": "2023-12-21",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 14200000000.0,
    "trainingComputeFlop": 2.5562e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Japan",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Amazon Trainium1",
    "baseModel": "Qwen-14B",
    "modelAccessibility": "Open weights (restricted use)",
    "company": "rinna",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "Gemini Nano-2",
    "publicationDate": "2023-12-19",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 3250000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "Google TPU v5e",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "Gemini Nano-1",
    "publicationDate": "2023-12-19",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 1800000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "Google TPU v5e",
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "FunSearch",
    "publicationDate": "2023-12-14",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 15000000000.0,
    "trainingComputeFlop": 3.87e+23,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "PaLM 2",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "CogAgent",
    "organization": [
      "Tsinghua University",
      "Zhipu AI"
    ],
    "publicationDate": "2023-12-14",
    "domain": "Vision",
    "task": "Instruction interpretation",
    "parameters": 18000000000.0,
    "trainingComputeFlop": 6.707e+22,
    "confidence": "Likely",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "CogVLM-17B",
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "VILA-13B",
    "organization": [
      "NVIDIA",
      "Massachusetts Institute of Technology (MIT)"
    ],
    "publicationDate": "2023-12-12",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 13350839296.0,
    "trainingComputeFlop": 2.3003136e+21,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": [
      "Llama 2-13B",
      "CLIP (ViT L/14@336px)"
    ],
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "W.A.L.T",
    "publicationDate": "2023-12-11",
    "domain": "Video",
    "task": "Video generation",
    "parameters": 4719000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Stanford University",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "Mixtral 8x7B",
    "organization": "Mistral AI",
    "publicationDate": "2023-12-11",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 46700000000.0,
    "trainingComputeFlop": 7.74e+23,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "France",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Mistral AI",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "SeamlessM4T",
    "organization": [
      "Facebook",
      "INRIA",
      "University of California (UC) Berkeley"
    ],
    "publicationDate": "2023-12-08",
    "domain": "Speech",
    "task": "Translation",
    "parameters": 2300000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": "W2v-BERT",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "Llama Guard",
    "publicationDate": "2023-12-07",
    "domain": "Language",
    "task": "Chat",
    "parameters": 7000000000.0,
    "trainingComputeFlop": 1.6e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": "Llama 2-7B",
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "Gemini 1.0 Ultra",
    "publicationDate": "2023-12-06",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": 5.0000000001e+25,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "Gemini 1.0 Pro",
    "publicationDate": "2023-12-06",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "Mamba-24M (SC09)",
    "organization": [
      "Carnegie Mellon University (CMU)",
      "Princeton University"
    ],
    "publicationDate": "2023-12-01",
    "domain": "Speech",
    "task": "Audio generation",
    "parameters": 23400000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Carnegie Mellon University (CMU)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-12"
  },
  {
    "model": "Qwen-72B",
    "organization": "Alibaba",
    "publicationDate": "2023-11-30",
    "domain": "Language",
    "task": "Chat",
    "parameters": 72000000000.0,
    "trainingComputeFlop": 1.3e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "PPLX-70B-Online",
    "organization": "Perplexity",
    "publicationDate": "2023-11-29",
    "domain": "Language",
    "task": "Question answering",
    "parameters": 70000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": "Llama 2-70B",
    "modelAccessibility": "API access",
    "company": "Perplexity",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "GNoME for crystal discovery",
    "publicationDate": "2023-11-29",
    "domain": "Materials science",
    "task": "Crystal discovery",
    "parameters": 16240000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "Inflection-2",
    "organization": "Inflection AI",
    "publicationDate": "2023-11-22",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": 1.001e+25,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "NVIDIA H100 SXM5 80GB",
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "Inflection AI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "Claude 2.1",
    "organization": "Anthropic",
    "publicationDate": "2023-11-21",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": "Claude 2",
    "modelAccessibility": "API access",
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "AndesGPT",
    "organization": "Oppo Mobile Telecommunications",
    "publicationDate": "2023-11-16",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "Oppo Mobile Telecommunications",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "Nemotron-3-8B",
    "organization": "NVIDIA",
    "publicationDate": "2023-11-15",
    "domain": "Language",
    "task": "Chat",
    "parameters": 8000000000.0,
    "trainingComputeFlop": 1.8e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "Qwen-Audio-Chat",
    "organization": "Alibaba",
    "publicationDate": "2023-11-14",
    "domain": "Language",
    "task": "Audio question answering",
    "parameters": 8460000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "GraphCast",
    "publicationDate": "2023-11-14",
    "domain": "Earth science",
    "task": "Weather forecasting",
    "parameters": null,
    "trainingComputeFlop": 2.1000000000000002e+22,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "Volcano 13B",
    "organization": [
      "Korea University",
      "Korea Advanced Institute of Science and Technology (KAIST)",
      "LG"
    ],
    "publicationDate": "2023-11-13",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 13000000000.0,
    "trainingComputeFlop": 4.56e+22,
    "confidence": "Likely",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "Korea (Republic of)",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": "LLaVA 1.5",
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Korea University",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "SPHINX (Llama 2 13B)",
    "organization": [
      "Shanghai AI Lab",
      "Chinese University of Hong Kong (CUHK)",
      "ShanghaiTech University"
    ],
    "publicationDate": "2023-11-13",
    "domain": "Vision",
    "task": "Visual question answering",
    "parameters": 19900000000.0,
    "trainingComputeFlop": 3.04e+22,
    "confidence": "Likely",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "China",
      "Hong Kong"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": "Llama 2-13B",
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Shanghai AI Lab",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "RoFormer",
    "organization": "Zhuiyi Technology",
    "publicationDate": "2023-11-08",
    "domain": "Language",
    "task": "Text classification",
    "parameters": 110000000.0,
    "trainingComputeFlop": 2.162688e+18,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Zhuiyi Technology",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "MultiBand Diffusion",
    "publicationDate": "2023-11-08",
    "domain": "Audio",
    "task": "Audio generation",
    "parameters": null,
    "trainingComputeFlop": 2.6e+19,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Israel",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "OmniVec",
    "organization": "TensorTour",
    "publicationDate": "2023-11-07",
    "domain": "Multimodal",
    "task": "Image classification",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "BERT-Large",
    "modelAccessibility": "Unreleased",
    "company": "TensorTour",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "mPLUG-Owl2",
    "organization": "Alibaba",
    "publicationDate": "2023-11-07",
    "domain": "Vision",
    "task": "Visual question answering",
    "parameters": 7120000000.0,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Llama 2-7B",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "GPT-4 Turbo",
    "organization": "OpenAI",
    "publicationDate": "2023-11-06",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "CogVLM-17B",
    "organization": [
      "Tsinghua University",
      "Zhipu AI",
      "Beihang University"
    ],
    "publicationDate": "2023-11-06",
    "domain": "Multimodal",
    "task": "Image captioning",
    "parameters": 17000000000.0,
    "trainingComputeFlop": 6.331e+22,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Vicuna-7B v0",
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "LLaVA 1.5",
    "organization": [
      "University of Wisconsin Madison",
      "Microsoft Research"
    ],
    "publicationDate": "2023-11-05",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 13000000000.0,
    "trainingComputeFlop": 7.807e+22,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": "Vicuna-13B v0",
    "modelAccessibility": "Open weights (restricted use)",
    "company": "University of Wisconsin Madison",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "Grok-1",
    "organization": "xAI",
    "publicationDate": "2023-11-04",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 314000000000.0,
    "trainingComputeFlop": 2.90000000001e+24,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "xAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "RT-Trajectory",
    "publicationDate": "2023-11-03",
    "domain": "Robotics",
    "task": "Robotic manipulation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "BLUUMI",
    "organization": [
      "University of Turku",
      "Hugging Face"
    ],
    "publicationDate": "2023-11-03",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 176000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "Finland",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "AMD Radeon Instinct MI250X",
    "baseModel": "BLOOM-176B",
    "modelAccessibility": "Open weights (restricted use)",
    "company": "University of Turku",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "Yi-34B",
    "organization": "01.AI",
    "publicationDate": "2023-11-02",
    "domain": "Language",
    "task": "Chat",
    "parameters": 34000000000.0,
    "trainingComputeFlop": 6.1e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "01.AI",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "Cohere Embed",
    "organization": "Cohere",
    "publicationDate": "2023-11-02",
    "domain": "Language",
    "task": "Semantic embedding",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Canada",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Cohere",
    "department": null,
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-11"
  },
  {
    "model": "Skywork-13B",
    "organization": "Kunlun Inc.",
    "publicationDate": "2023-10-30",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 13000000000.0,
    "trainingComputeFlop": 2.5e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A800 PCIe 40 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Kunlun Inc.",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-10"
  },
  {
    "model": "ChatGLM3-6B",
    "organization": "Zhipu AI",
    "publicationDate": "2023-10-27",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 6000000000.0,
    "trainingComputeFlop": 5.04e+22,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Zhipu AI",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-10"
  },
  {
    "model": "DiT-XL/2 + CADS",
    "organization": [
      "ETH Zurich",
      "Disney Research"
    ],
    "publicationDate": "2023-10-26",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 675000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "Switzerland",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "DiT-XL/2",
    "modelAccessibility": "Unreleased",
    "company": "ETH Zurich",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-10"
  },
  {
    "model": "CODEFUSION (Python)",
    "organization": [
      "Microsoft",
      "Microsoft Research"
    ],
    "publicationDate": "2023-10-26",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 75000000.0,
    "trainingComputeFlop": 7.92e+18,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-10"
  },
  {
    "model": "DALL·E 3",
    "organization": "OpenAI",
    "publicationDate": "2023-10-19",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-10"
  },
  {
    "model": "PaLI-3",
    "publicationDate": "2023-10-17",
    "domain": "Multimodal",
    "task": "Visual question answering",
    "parameters": 5000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": [
      "United States",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "UL2",
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": [
      "DeepMind",
      "Google Research"
    ],
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-10"
  },
  {
    "model": "ERNIE 4.0",
    "organization": "Baidu",
    "publicationDate": "2023-10-17",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Baidu",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-10"
  },
  {
    "model": "RT-2-X",
    "publicationDate": "2023-10-13",
    "domain": "Robotics",
    "task": "Robotic manipulation",
    "parameters": 55000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "RT-2",
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-10"
  },
  {
    "model": "Ferret (13B)",
    "organization": [
      "Columbia University",
      "Apple"
    ],
    "publicationDate": "2023-10-11",
    "domain": "Multimodal",
    "task": "Object recognition",
    "parameters": 13000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": "Vicuna-13B v0",
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Columbia University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-10"
  },
  {
    "model": "RoseTTAFold All-Atom (RFAA)",
    "organization": [
      "University of Washington",
      "Seoul National University",
      "University of Sheffield"
    ],
    "publicationDate": "2023-10-09",
    "domain": "Biology",
    "task": "Protein folding prediction",
    "parameters": null,
    "trainingComputeFlop": 2.14e+20,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "United States",
      "Korea (Republic of)",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA RTX A6000",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "University of Washington",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-10"
  },
  {
    "model": "FinGPT-13B",
    "organization": [
      "University of California Los Angeles (UCLA)",
      "Columbia University",
      "New York University (NYU)"
    ],
    "publicationDate": "2023-10-07",
    "domain": "Language",
    "task": "Named entity recognition (NER)",
    "parameters": 13000000000.0,
    "trainingComputeFlop": 1.6e+23,
    "confidence": "Likely",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA GeForce RTX 3090",
    "baseModel": "Llama 2-13B",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "University of California Los Angeles (UCLA)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-10"
  },
  {
    "model": "CTM (CIFAR-10)",
    "organization": [
      "Stanford University",
      "Sony"
    ],
    "publicationDate": "2023-10-01",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "Japan"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Stanford University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-10"
  },
  {
    "model": "Amazon Titan",
    "organization": "Amazon",
    "publicationDate": "2023-09-28",
    "domain": "Language",
    "task": "Semantic search",
    "parameters": 200000000000.0,
    "trainingComputeFlop": 4.8e+24,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Amazon",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-09"
  },
  {
    "model": "Show-1",
    "organization": "National University of Singapore",
    "publicationDate": "2023-09-27",
    "domain": "Video",
    "task": "Video generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "Singapore",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "National University of Singapore",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-09"
  },
  {
    "model": "GPT-4V",
    "organization": "OpenAI",
    "publicationDate": "2023-09-25",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": "GPT-4",
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-09"
  },
  {
    "model": "AlphaMissense",
    "publicationDate": "2023-09-22",
    "domain": "Biology",
    "task": "Protein pathogenicity prediction",
    "parameters": 93000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "AlphaFold 2",
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-09"
  },
  {
    "model": "Robot Parkour",
    "organization": [
      "Shanghai Qi Zhi institute",
      "Stanford University",
      "Carnegie Mellon University (CMU)",
      "Tsinghua University"
    ],
    "publicationDate": "2023-09-12",
    "domain": "Robotics",
    "task": "Animal (human/non-human) imitation",
    "parameters": 500000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "China",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA GeForce RTX 3090",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Shanghai Qi Zhi institute",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-09"
  },
  {
    "model": "Falcon-180B",
    "organization": "Technology Innovation Institute",
    "publicationDate": "2023-09-06",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 180000000000.0,
    "trainingComputeFlop": 3.76e+24,
    "confidence": "Confident",
    "organizationCategorization": "Government",
    "countryOfOrganization": "United Arab Emirates",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Technology Innovation Institute",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-09"
  },
  {
    "model": "Swift",
    "organization": "Intel Labs",
    "publicationDate": "2023-08-30",
    "domain": "Robotics",
    "task": "Helicopter driving",
    "parameters": 56804.0,
    "trainingComputeFlop": 5.337e+16,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA GeForce RTX 3090",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Intel Labs",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-08"
  },
  {
    "model": "Jais",
    "organization": [
      "Cerebras Systems",
      "Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)",
      "Inception G42"
    ],
    "publicationDate": "2023-08-29",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 13000000000.0,
    "trainingComputeFlop": 4.8946763e+22,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "United Arab Emirates"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Cerebras CS-2",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Cerebras Systems",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-08"
  },
  {
    "model": "PeptideBERT",
    "organization": "Carnegie Mellon University (CMU)",
    "publicationDate": "2023-08-28",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": null,
    "trainingComputeFlop": 4.9e+16,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA GeForce GTX 1080 Ti",
    "baseModel": "ProtBERT-UniRef",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Carnegie Mellon University (CMU)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-08"
  },
  {
    "model": "Qwen-VL",
    "organization": "Alibaba",
    "publicationDate": "2023-08-24",
    "domain": "Multimodal",
    "task": "Image captioning",
    "parameters": 9600000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Qwen-7B",
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-08"
  },
  {
    "model": "GGNN",
    "organization": [
      "Westlake University",
      "Tsinghua University",
      "Toyota Technological Institute at Chicago"
    ],
    "publicationDate": "2023-08-05",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": null,
    "trainingComputeFlop": 7.56e+21,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "China",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": "ESM2-650M",
    "modelAccessibility": "Unreleased",
    "company": "Westlake University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-08"
  },
  {
    "model": "RT-2",
    "publicationDate": "2023-07-28",
    "domain": "Robotics",
    "task": "Robotic manipulation",
    "parameters": 55000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "PaLI-X",
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-07"
  },
  {
    "model": "AudioLM",
    "publicationDate": "2023-07-26",
    "domain": "Audio",
    "task": "Audio generation",
    "parameters": 1500000000.0,
    "trainingComputeFlop": 3.9e+18,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-07"
  },
  {
    "model": "Llama 2-70B",
    "publicationDate": "2023-07-18",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 70000000000.0,
    "trainingComputeFlop": 8.1e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-07"
  },
  {
    "model": "Llama 2-7B",
    "publicationDate": "2023-07-18",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 7000000000.0,
    "trainingComputeFlop": 8.4e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-07"
  },
  {
    "model": "GPT3-2.7B (FlashAttention-2)",
    "organization": [
      "Stanford University",
      "Princeton University"
    ],
    "publicationDate": "2023-07-18",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 2700000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Stanford University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-07"
  },
  {
    "model": "Claude 2",
    "organization": "Anthropic",
    "publicationDate": "2023-07-11",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": 3.866e+24,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-07"
  },
  {
    "model": "xTrimoPGLM -100B",
    "organization": [
      "Tsinghua University",
      "BioMap Research"
    ],
    "publicationDate": "2023-07-06",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 100000000000.0,
    "trainingComputeFlop": 6.2e+23,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-07"
  },
  {
    "model": "InternLM",
    "organization": [
      "Shanghai AI Lab",
      "SenseTime"
    ],
    "publicationDate": "2023-07-06",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 104000000000.0,
    "trainingComputeFlop": 9.984e+23,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "China",
      "Hong Kong"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Shanghai AI Lab",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-07"
  },
  {
    "model": "Pangu-Weather",
    "organization": "Huawei",
    "publicationDate": "2023-07-05",
    "domain": "Earth science",
    "task": "Weather forecasting",
    "parameters": 256000000.0,
    "trainingComputeFlop": 3.98e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Huawei",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-07"
  },
  {
    "model": "Stable Diffusion XL (SDXL)",
    "organization": "Stability AI",
    "publicationDate": "2023-07-04",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 3400000000.0,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United Kingdom",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Stability AI",
    "department": null,
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-07"
  },
  {
    "model": "HyenaDNA",
    "organization": [
      "Stanford University",
      "Harvard University",
      "Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",
      "University of Montreal / Université de Montréal"
    ],
    "publicationDate": "2023-06-27",
    "domain": "Biology",
    "task": "Protein or nucleotide language model (pLM/nLM)",
    "parameters": 6600000.0,
    "trainingComputeFlop": 1.811e+21,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "United States",
      "Canada"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Stanford University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-06"
  },
  {
    "model": "ERNIE 3.5",
    "organization": "Baidu",
    "publicationDate": "2023-06-27",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "Baidu",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-06"
  },
  {
    "model": "MusicGen",
    "publicationDate": "2023-06-08",
    "domain": "Audio",
    "task": "Audio generation",
    "parameters": 3359000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-06"
  },
  {
    "model": "LTM-1",
    "organization": "Magic",
    "publicationDate": "2023-06-06",
    "domain": "Language",
    "task": "Code generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Magic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-06"
  },
  {
    "model": "PaLI-X",
    "publicationDate": "2023-05-29",
    "domain": "Multimodal",
    "task": "Image captioning",
    "parameters": 55000000000.0,
    "trainingComputeFlop": 5.6e+23,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "UL2",
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-05"
  },
  {
    "model": "DPO on Pythia-2.8B",
    "organization": [
      "Stanford University",
      "CZ Biohub Network"
    ],
    "publicationDate": "2023-05-29",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 2800000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": null,
    "baseModel": "Pythia-2.8b",
    "modelAccessibility": "Unreleased",
    "company": "Stanford University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-05"
  },
  {
    "model": "Goat-7B",
    "organization": "National University of Singapore",
    "publicationDate": "2023-05-23",
    "domain": "Language",
    "task": "Quantitative reasoning",
    "parameters": 7000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "Singapore",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A10 PCIe",
    "baseModel": "LLaMA-7B",
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "National University of Singapore",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-05"
  },
  {
    "model": "CodeT5+",
    "organization": "Salesforce",
    "publicationDate": "2023-05-20",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 16000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Salesforce",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-05"
  },
  {
    "model": "ONE-PEACE",
    "organization": [
      "Alibaba",
      "Huazhong University of Science and Technology"
    ],
    "publicationDate": "2023-05-18",
    "domain": "Multimodal",
    "task": "Image classification",
    "parameters": 4000000000.0,
    "trainingComputeFlop": 1.8e+20,
    "confidence": "Speculative",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-05"
  },
  {
    "model": "CoEdiT-xxl",
    "organization": [
      "University of Minnesota",
      "Grammarly"
    ],
    "publicationDate": "2023-05-17",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 11000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": "Flan-T5 11B",
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "University of Minnesota",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-05"
  },
  {
    "model": "Med-PaLM 2",
    "publicationDate": "2023-05-16",
    "domain": "Medicine",
    "task": "Question answering",
    "parameters": 340000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": [
      "United States",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "PaLM 2",
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": [
      "Google Research",
      "DeepMind"
    ],
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-05"
  },
  {
    "model": "InstructBLIP",
    "organization": [
      "Salesforce Research",
      "Hong Kong University of Science and Technology (HKUST)",
      "Nanyang Technological University"
    ],
    "publicationDate": "2023-05-11",
    "domain": "Multimodal",
    "task": "Visual question answering",
    "parameters": 13000000000.0,
    "trainingComputeFlop": 1.94e+20,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Hong Kong",
      "Singapore"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": "Vicuna-13B v0",
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Salesforce Research",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-05"
  },
  {
    "model": "PaLM 2",
    "organization": "Google",
    "publicationDate": "2023-05-10",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 340000000000.0,
    "trainingComputeFlop": 7.34e+24,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-05"
  },
  {
    "model": "StarCoder",
    "organization": [
      "Hugging Face",
      "ServiceNow",
      "Northeastern University",
      "Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",
      "Carnegie Mellon University (CMU)",
      "Johns Hopkins University",
      "Leipzig University",
      "ScaDS.AI",
      "Queen Mary University of London",
      "Roblox",
      "Sea AI Lab",
      "Technion - Israel Institute of Technology",
      "Monash University",
      "CSIRO",
      "Data61",
      "McGill University",
      "Saama",
      "University of British Columbia (UBC)",
      "Massachusetts Institute of Technology (MIT)",
      "Technical University of Munich",
      "IBM",
      "University of Vermont",
      "UnfoldML",
      "SAP",
      "University of Notre Dame",
      "Columbia University",
      "New York University (NYU)",
      "University of Allahabad",
      "Discover Dollar",
      "Toloka",
      "Telefonica",
      "Stanford University",
      "Weizmann Institute of Science",
      "Alan Turing Institute",
      "Wellesley College",
      "EleutherAI",
      "Forschungszentrum Julich"
    ],
    "publicationDate": "2023-05-09",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 15500000000.0,
    "trainingComputeFlop": 8.46e+22,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia",
      "Government",
      "Research collective"
    ],
    "countryOfOrganization": [
      "United States",
      "Canada",
      "Germany",
      "United Kingdom",
      "Singapore",
      "Israel",
      "Australia",
      "Sweden",
      "India",
      "Netherlands",
      "Spain"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Hugging Face",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-05"
  },
  {
    "model": "ImageBind",
    "publicationDate": "2023-05-09",
    "domain": "Multimodal",
    "task": "Image classification",
    "parameters": 932000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": "ViT-Huge/14",
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-05"
  },
  {
    "model": "Agile Soccer Robot",
    "publicationDate": "2023-04-26",
    "domain": "Robotics",
    "task": "Animal (human/non-human) imitation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-04"
  },
  {
    "model": "LLaVA",
    "organization": [
      "University of Wisconsin Madison",
      "Microsoft Research",
      "Columbia University"
    ],
    "publicationDate": "2023-04-17",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 13000000000.0,
    "trainingComputeFlop": 7.8049e+22,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": "Vicuna-13B v0",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "University of Wisconsin Madison",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-04"
  },
  {
    "model": "DINOv2",
    "publicationDate": "2023-04-14",
    "domain": "Vision",
    "task": "Image representation",
    "parameters": 1140000000.0,
    "trainingComputeFlop": 7.41851136e+21,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-04"
  },
  {
    "model": "Incoder-6.7B",
    "publicationDate": "2023-04-09",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 6700000000.0,
    "trainingComputeFlop": 3.00001e+21,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-04"
  },
  {
    "model": "Segment Anything Model",
    "publicationDate": "2023-04-05",
    "domain": "Vision",
    "task": "Image segmentation",
    "parameters": 636000000.0,
    "trainingComputeFlop": 7.8e+21,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA A100",
    "baseModel": "ViT-Huge/14",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-04"
  },
  {
    "model": "BloombergGPT",
    "organization": [
      "Bloomberg",
      "Johns Hopkins University"
    ],
    "publicationDate": "2023-03-30",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 50558868480.0,
    "trainingComputeFlop": 2.36e+23,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Bloomberg",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "VideoMAE V2",
    "organization": [
      "Nanjing University",
      "Shenzhen Institute of Advanced Technology",
      "Shanghai AI Lab"
    ],
    "publicationDate": "2023-03-29",
    "domain": "Video",
    "task": "Action recognition",
    "parameters": 1000000000.0,
    "trainingComputeFlop": 9.7e+21,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": "ViT-G/14",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Nanjing University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "SigLIP 400M",
    "publicationDate": "2023-03-27",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 400000000.0,
    "trainingComputeFlop": 4.9467301e+21,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "Firefly",
    "organization": "Adobe",
    "publicationDate": "2023-03-21",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "Adobe",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "PanGu-Σ",
    "organization": "Huawei Noah's Ark Lab",
    "publicationDate": "2023-03-20",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 1085000000000.0,
    "trainingComputeFlop": 4.67e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Huawei Ascend 910",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Huawei Noah's Ark Lab",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Massive (≥1T)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "Gen-2",
    "organization": "Runway",
    "publicationDate": "2023-03-20",
    "domain": "Video",
    "task": "Video generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Runway",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "LEP-AD",
    "organization": [
      "King Abdullah University of Science and Technology (KAUST)",
      "Karolinska Institute"
    ],
    "publicationDate": "2023-03-15",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 3007381000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "Saudi Arabia",
      "Sweden"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "ESM2-3B",
    "modelAccessibility": "Unreleased",
    "company": "King Abdullah University of Science and Technology (KAUST)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "GPT-4",
    "organization": "OpenAI",
    "publicationDate": "2023-03-15",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": 1800000000000.0,
    "trainingComputeFlop": 2.1e+25,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Massive (≥1T)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "Falcon-40B",
    "organization": "Technology Innovation Institute",
    "publicationDate": "2023-03-15",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 40000000000.0,
    "trainingComputeFlop": 2.4e+23,
    "confidence": "Confident",
    "organizationCategorization": "Government",
    "countryOfOrganization": "United Arab Emirates",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Technology Innovation Institute",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "Claude",
    "organization": "Anthropic",
    "publicationDate": "2023-03-14",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Anthropic",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "PaLM-E",
    "organization": [
      "Google",
      "TU Berlin"
    ],
    "publicationDate": "2023-03-06",
    "domain": "Robotics",
    "task": "Visual question answering",
    "parameters": 562000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Germany"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "PaLM (540B)",
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "AudioGen",
    "publicationDate": "2023-03-05",
    "domain": "Audio",
    "task": "Audio generation",
    "parameters": 1000000000.0,
    "trainingComputeFlop": 9.5e+21,
    "confidence": "Likely",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Israel"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "DiT-XL/2",
    "organization": [
      "New York University (NYU)",
      "University of California (UC) Berkeley"
    ],
    "publicationDate": "2023-03-02",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 675000000.0,
    "trainingComputeFlop": 6e+20,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": "Stable Diffusion (LDM-KL-8-G)",
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "New York University (NYU)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-03"
  },
  {
    "model": "LLaMA-65B",
    "publicationDate": "2023-02-24",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 65200000000.0,
    "trainingComputeFlop": 5.5e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-02"
  },
  {
    "model": "BASIC-L + Lion",
    "organization": [
      "Google",
      "University of California Los Angeles (UCLA)"
    ],
    "publicationDate": "2023-02-13",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 3070000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-02"
  },
  {
    "model": "ViT-22B",
    "organization": "Google",
    "publicationDate": "2023-02-10",
    "domain": "Vision",
    "task": "Object detection",
    "parameters": 21743000000.0,
    "trainingComputeFlop": 1.93248e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-02"
  },
  {
    "model": "ProteinDT",
    "organization": [
      "University of California (UC) Berkeley",
      "California Institute of Technology",
      "University of Toronto",
      "University of Wisconsin Madison",
      "Texas A&M",
      "NVIDIA",
      "Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)"
    ],
    "publicationDate": "2023-02-09",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "Canada"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "SciBERT",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "University of California (UC) Berkeley",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-02"
  },
  {
    "model": "Gen-1",
    "organization": "Runway",
    "publicationDate": "2023-02-06",
    "domain": "Video",
    "task": "Video generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Runway",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-02"
  },
  {
    "model": "BLIP-2 (Q-Former)",
    "organization": "Salesforce Research",
    "publicationDate": "2023-01-30",
    "domain": "Vision",
    "task": "Visual question answering",
    "parameters": 1480000000.0,
    "trainingComputeFlop": 1.20000000001e+21,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Salesforce Research",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-01"
  },
  {
    "model": "DDPM-IP (CelebA)",
    "organization": "Utrecht University",
    "publicationDate": "2023-01-27",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 295000000.0,
    "trainingComputeFlop": 3.5e+20,
    "confidence": "Likely",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "Netherlands",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Utrecht University",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-01"
  },
  {
    "model": "MusicLM",
    "organization": "Google",
    "publicationDate": "2023-01-26",
    "domain": "Audio",
    "task": "Audio generation",
    "parameters": 860000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "W2v-BERT",
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-01"
  },
  {
    "model": "Ankh_large",
    "organization": [
      "Technical University of Munich",
      "Columbia University"
    ],
    "publicationDate": "2023-01-16",
    "domain": "Biology",
    "task": "Protein generation",
    "parameters": 1900000000.0,
    "trainingComputeFlop": 6.5e+21,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "Germany",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Technical University of Munich",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2023,
    "yearMonth": "2023-01"
  },
  {
    "model": "Nucleotide Transformer",
    "organization": [
      "NVIDIA",
      "Technical University of Munich",
      "InstaDeep"
    ],
    "publicationDate": "2023-01-15",
    "domain": "Biology",
    "task": "Protein or nucleotide language model (pLM/nLM)",
    "parameters": 2500000000.0,
    "trainingComputeFlop": 8.08e+21,
    "confidence": "Likely",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Germany",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-01"
  },
  {
    "model": "DreamerV3",
    "publicationDate": "2023-01-10",
    "domain": "Games",
    "task": "Open ended play",
    "parameters": 200000000.0,
    "trainingComputeFlop": 2.2032e+20,
    "confidence": "Likely",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United Kingdom",
      "Canada"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2023,
    "yearMonth": "2023-01"
  },
  {
    "model": "VALL-E",
    "organization": "Microsoft",
    "publicationDate": "2023-01-05",
    "domain": "Audio",
    "task": "Speech synthesis",
    "parameters": 353000000.0,
    "trainingComputeFlop": 1.01e+19,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2023,
    "yearMonth": "2023-01"
  },
  {
    "model": "Hybrid H3-2.7B",
    "organization": [
      "Stanford University",
      "University at Buffalo"
    ],
    "publicationDate": "2022-12-28",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 2700000000.0,
    "trainingComputeFlop": 6.48e+21,
    "confidence": "Likely",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Stanford University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-12"
  },
  {
    "model": "CaLM",
    "organization": "University of Oxford",
    "publicationDate": "2022-12-19",
    "domain": "Biology",
    "task": "Protein or nucleotide language model (pLM/nLM)",
    "parameters": 86000000.0,
    "trainingComputeFlop": 2.9e+19,
    "confidence": "Likely",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United Kingdom",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Quadro RTX 4000",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "University of Oxford",
    "department": null,
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-12"
  },
  {
    "model": "RT-1",
    "organization": "Google",
    "publicationDate": "2022-12-13",
    "domain": "Robotics",
    "task": "Robotic manipulation",
    "parameters": 35000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-12"
  },
  {
    "model": "TranceptEve",
    "organization": [
      "University of Oxford",
      "Harvard Medical School"
    ],
    "publicationDate": "2022-12-10",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "United Kingdom",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Tranception",
    "modelAccessibility": "Unreleased",
    "company": "University of Oxford",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-12"
  },
  {
    "model": "Vega v2",
    "organization": [
      "Wuhan University",
      "JD Explore Academy",
      "Shanghai AI Lab",
      "Nanyang Technological University",
      "Washington University in St Louis",
      "Chongqing University of Posts and Telecommunications",
      "University of Sydney"
    ],
    "publicationDate": "2022-12-04",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 6000000000.0,
    "trainingComputeFlop": 7.76e+22,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "China",
      "Singapore",
      "United States",
      "Australia"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Wuhan University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-12"
  },
  {
    "model": "DeepNash",
    "publicationDate": "2022-12-01",
    "domain": "Games",
    "task": "Stratego",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-12"
  },
  {
    "model": "GPT-3.5 Turbo",
    "organization": "OpenAI",
    "publicationDate": "2022-11-30",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 20000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "GPT-3.5",
    "organization": "OpenAI",
    "publicationDate": "2022-11-28",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": 2.578e+24,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "DiT-XL/2 + Discriminator Guidance",
    "organization": [
      "Korea Advanced Institute of Science and Technology (KAIST)",
      "NAVER"
    ],
    "publicationDate": "2022-11-28",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "Korea (Republic of)",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": "DiT-XL/2",
    "modelAccessibility": "Unreleased",
    "company": "Korea Advanced Institute of Science and Technology (KAIST)",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "Discriminator Guidance",
    "organization": [
      "Korea Advanced Institute of Science and Technology (KAIST)",
      "NAVER"
    ],
    "publicationDate": "2022-11-28",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": 2.1570000001e+20,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "Korea (Republic of)",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 PCIe",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Korea Advanced Institute of Science and Technology (KAIST)",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "ALM 1.0",
    "organization": "Beijing Academy of Artificial Intelligence / BAAI",
    "publicationDate": "2022-11-28",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 335000000.0,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "Beijing Academy of Artificial Intelligence / BAAI",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "CICERO",
    "publicationDate": "2022-11-22",
    "domain": "Games",
    "task": "Diplomacy",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "AR-LDM",
    "organization": [
      "Alibaba",
      "University of Waterloo",
      "Vector Institute"
    ],
    "publicationDate": "2022-11-20",
    "domain": "Image generation",
    "task": "Text-to-image",
    "parameters": 1500000000.0,
    "trainingComputeFlop": 5.1e+20,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "China",
      "Canada"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": "Stable Diffusion (LDM-KL-8-G)",
    "modelAccessibility": "Unreleased",
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "Fusion in Encoder",
    "organization": "Samsung",
    "publicationDate": "2022-11-18",
    "domain": "Language",
    "task": "Question answering",
    "parameters": 330000000.0,
    "trainingComputeFlop": 1.3e+20,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Korea (Republic of)",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Samsung",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "Galactica",
    "publicationDate": "2022-11-16",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 120000000000.0,
    "trainingComputeFlop": 3.24e+23,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "EVA-01",
    "organization": [
      "Beijing Academy of Artificial Intelligence / BAAI",
      "Huazhong University of Science and Technology",
      "Zhejiang University (ZJU)",
      "Beijing Institute of Technology"
    ],
    "publicationDate": "2022-11-14",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 1011000000.0,
    "trainingComputeFlop": 1.501e+22,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Beijing Academy of Artificial Intelligence / BAAI",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "AltCLIP_M9",
    "organization": "Beijing Academy of Artificial Intelligence / BAAI",
    "publicationDate": "2022-11-12",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "CLIP (ViT L/14@336px)",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Beijing Academy of Artificial Intelligence / BAAI",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "InternImage",
    "organization": [
      "Shanghai AI Lab",
      "Tsinghua University",
      "Nanjing University",
      "SenseTime",
      "Chinese University of Hong Kong (CUHK)"
    ],
    "publicationDate": "2022-11-10",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 1080000000.0,
    "trainingComputeFlop": 2.408e+21,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "China",
      "Hong Kong"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Shanghai AI Lab",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "mT0-13B",
    "organization": [
      "Hugging Face",
      "BigScience"
    ],
    "publicationDate": "2022-11-03",
    "domain": "Language",
    "task": "Translation",
    "parameters": 13000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Research collective"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "mT5-XXL",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Hugging Face",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "Mogrifier RLSTM (WT2)",
    "publicationDate": "2022-11-03",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 35000000.0,
    "trainingComputeFlop": 1.4e+17,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "eDiff-I",
    "organization": "NVIDIA",
    "publicationDate": "2022-11-02",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 9100000000.0,
    "trainingComputeFlop": 5.46e+19,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-11"
  },
  {
    "model": "EnCodec",
    "publicationDate": "2022-10-24",
    "domain": "Audio",
    "task": "Audio generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-10"
  },
  {
    "model": "U-PaLM (540B)",
    "organization": "Google",
    "publicationDate": "2022-10-20",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 540000000000.0,
    "trainingComputeFlop": 2.53e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": "PaLM (540B)",
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-10"
  },
  {
    "model": "LMSI-Palm",
    "organization": [
      "Google",
      "University of Illinois Urbana-Champaign (UIUC)"
    ],
    "publicationDate": "2022-10-20",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 540000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "PaLM (540B)",
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-10"
  },
  {
    "model": "Flan-PaLM 540B",
    "organization": "Google",
    "publicationDate": "2022-10-20",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 540000000000.0,
    "trainingComputeFlop": 2.540000000001e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v4",
    "baseModel": "PaLM (540B)",
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-10"
  },
  {
    "model": "GenSLM",
    "organization": [
      "University of Chicago",
      "NVIDIA",
      "Harvard University",
      "Cerebras Systems",
      "Technical University of Munich",
      "California Institute of Technology"
    ],
    "publicationDate": "2022-10-11",
    "domain": "Biology",
    "task": "Protein or nucleotide language model (pLM/nLM)",
    "parameters": 25000000000.0,
    "trainingComputeFlop": 1.42e+21,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "Germany"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "University of Chicago",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-10"
  },
  {
    "model": "Diplodocus",
    "publicationDate": "2022-10-11",
    "domain": "Games",
    "task": "Diplomacy",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-10"
  },
  {
    "model": "Phenaki",
    "publicationDate": "2022-10-05",
    "domain": "Video",
    "task": "Video generation",
    "parameters": 1800000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United Kingdom",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "University College London (UCL)",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-10"
  },
  {
    "model": "AlphaTensor",
    "publicationDate": "2022-10-05",
    "domain": "Other",
    "task": null,
    "parameters": null,
    "trainingComputeFlop": 7.1414784e+20,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-10"
  },
  {
    "model": "DiffDock",
    "organization": "Massachusetts Institute of Technology (MIT)",
    "publicationDate": "2022-10-04",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 20240000.0,
    "trainingComputeFlop": 7.2e+19,
    "confidence": "Likely",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA RTX A6000",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Massachusetts Institute of Technology (MIT)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-10"
  },
  {
    "model": "Make-A-Video",
    "publicationDate": "2022-09-29",
    "domain": "Video",
    "task": "Video generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-09"
  },
  {
    "model": "Whisper",
    "organization": "OpenAI",
    "publicationDate": "2022-09-21",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": 1550000000.0,
    "trainingComputeFlop": 4.2072663e+21,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-09"
  },
  {
    "model": "SauTech",
    "organization": [
      "Saudi Data and Artificial Intelligence Authority",
      "Saudi Company for Artificial Intelligence"
    ],
    "publicationDate": "2022-09-14",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": [
      "Industry",
      "Government"
    ],
    "countryOfOrganization": "Saudi Arabia",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "Saudi Data and Artificial Intelligence Authority",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-09"
  },
  {
    "model": "PaLI",
    "organization": "Google",
    "publicationDate": "2022-09-14",
    "domain": "Language",
    "task": "Visual question answering",
    "parameters": 16900000000.0,
    "trainingComputeFlop": 1.69e+23,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-09"
  },
  {
    "model": "BEIT-3",
    "organization": "Microsoft",
    "publicationDate": "2022-08-22",
    "domain": "Multimodal",
    "task": "Object detection",
    "parameters": 1900000000.0,
    "trainingComputeFlop": 7e+19,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-08"
  },
  {
    "model": "BlenderBot 3",
    "publicationDate": "2022-08-10",
    "domain": "Language",
    "task": "Chat",
    "parameters": 175000000000.0,
    "trainingComputeFlop": 4.3e+23,
    "confidence": "Likely",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "Canada",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": "OPT-175B",
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "McGill University",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-08"
  },
  {
    "model": "GLM-130B",
    "organization": "Tsinghua University",
    "publicationDate": "2022-08-04",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 130000000000.0,
    "trainingComputeFlop": 3.5490054945e+23,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-08"
  },
  {
    "model": "AlexaTM 20B",
    "organization": "Amazon",
    "publicationDate": "2022-08-02",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 19750000000.0,
    "trainingComputeFlop": 2.04374016e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Amazon",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-08"
  },
  {
    "model": "OmegaPLM",
    "organization": [
      "Massachusetts Institute of Technology (MIT)",
      "Westlake University"
    ],
    "publicationDate": "2022-07-22",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 670000000.0,
    "trainingComputeFlop": 1.03514112e+22,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "United States",
      "China"
    ],
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Massachusetts Institute of Technology (MIT)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-07"
  },
  {
    "model": "ESM2-15B",
    "publicationDate": "2022-07-21",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 15000000000.0,
    "trainingComputeFlop": 7.35000000001e+22,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-07"
  },
  {
    "model": "BLOOM-176B",
    "organization": [
      "Hugging Face",
      "BigScience"
    ],
    "publicationDate": "2022-07-11",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 176247271424.0,
    "trainingComputeFlop": 3.65664e+23,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Research collective"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "Hugging Face",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-07"
  },
  {
    "model": "NLLB",
    "publicationDate": "2022-07-06",
    "domain": "Language",
    "task": "Translation",
    "parameters": 54500000000.0,
    "trainingComputeFlop": 1.751113728e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-07"
  },
  {
    "model": "CodeT5-large",
    "organization": "Salesforce",
    "publicationDate": "2022-07-05",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 770000000.0,
    "trainingComputeFlop": 2.72e+21,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Salesforce",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-07"
  },
  {
    "model": "Minerva (540B)",
    "organization": "Google",
    "publicationDate": "2022-06-29",
    "domain": "Language",
    "task": "Quantitative reasoning",
    "parameters": 540350000000.0,
    "trainingComputeFlop": 2.7415e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": "PaLM (540B)",
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-06"
  },
  {
    "model": "ProGen2-xlarge",
    "organization": [
      "Salesforce Research",
      "Columbia University",
      "Johns Hopkins University"
    ],
    "publicationDate": "2022-06-27",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 6400000000.0,
    "trainingComputeFlop": 1.35e+22,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Salesforce Research",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-06"
  },
  {
    "model": "Parti",
    "publicationDate": "2022-06-22",
    "domain": "Image generation",
    "task": "Text-to-image",
    "parameters": 20000000000.0,
    "trainingComputeFlop": 5.09607936e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-06"
  },
  {
    "model": "CoCa",
    "publicationDate": "2022-06-14",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 2100000000.0,
    "trainingComputeFlop": 7.3e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-06"
  },
  {
    "model": "MetaLM",
    "organization": "Microsoft Research",
    "publicationDate": "2022-06-13",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Microsoft Research",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-06"
  },
  {
    "model": "DITTO",
    "organization": [
      "Tsinghua University",
      "Apple",
      "Westlake University",
      "Chinese University of Hong Kong (CUHK)"
    ],
    "publicationDate": "2022-06-06",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 750000000.0,
    "trainingComputeFlop": 3.31776e+18,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "China",
      "United States",
      "Hong Kong"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-06"
  },
  {
    "model": "Diffusion-GAN",
    "organization": [
      "UT Austin",
      "Microsoft"
    ],
    "publicationDate": "2022-06-05",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "UT Austin",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-06"
  },
  {
    "model": "CogVideo",
    "organization": [
      "Tsinghua University",
      "Beijing Academy of Artificial Intelligence / BAAI"
    ],
    "publicationDate": "2022-05-29",
    "domain": "Video",
    "task": "Video generation",
    "parameters": 9400000000.0,
    "trainingComputeFlop": null,
    "confidence": "Speculative",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": null,
    "baseModel": "CogView2",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-05"
  },
  {
    "model": "Tranception",
    "organization": [
      "University of Oxford",
      "Harvard Medical School",
      "Cohere"
    ],
    "publicationDate": "2022-05-27",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 700000000.0,
    "trainingComputeFlop": 7.24e+21,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United Kingdom",
      "United States",
      "Canada"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "University of Oxford",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-05"
  },
  {
    "model": "GPT-2 Medium (FlashAttention)",
    "organization": [
      "Stanford University",
      "University at Buffalo"
    ],
    "publicationDate": "2022-05-27",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 355000000.0,
    "trainingComputeFlop": 8.9280922e+20,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Stanford University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-05"
  },
  {
    "model": "Imagen",
    "publicationDate": "2022-05-23",
    "domain": "Image generation",
    "task": "Text-to-image",
    "parameters": 7762000000.0,
    "trainingComputeFlop": 1.46e+22,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Google",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-05"
  },
  {
    "model": "SimCSE",
    "organization": [
      "Princeton University",
      "Tsinghua University"
    ],
    "publicationDate": "2022-05-18",
    "domain": "Language",
    "task": "Semantic embedding",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "United States",
      "China"
    ],
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": "RoBERTa Large",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Princeton University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-05"
  },
  {
    "model": "Gato",
    "publicationDate": "2022-05-12",
    "domain": "Multimodal",
    "task": "Atari",
    "parameters": 1180000000.0,
    "trainingComputeFlop": 4.02e+21,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-05"
  },
  {
    "model": "UL2",
    "publicationDate": "2022-05-10",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 20000000000.0,
    "trainingComputeFlop": 1.2e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Google",
    "department": [
      "Google Research",
      "Google Brain"
    ],
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-05"
  },
  {
    "model": "DeBERTaV3large + KEAR",
    "organization": "Microsoft",
    "publicationDate": "2022-05-04",
    "domain": "Language",
    "task": "Question answering",
    "parameters": 418000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "DeBERTaV3large",
    "modelAccessibility": "Unreleased",
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-05"
  },
  {
    "model": "OPT-175B",
    "publicationDate": "2022-05-02",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 175000000000.0,
    "trainingComputeFlop": 4.3e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-05"
  },
  {
    "model": "Flamingo",
    "publicationDate": "2022-04-29",
    "domain": "Multimodal",
    "task": "Visual question answering",
    "parameters": 80000000000.0,
    "trainingComputeFlop": 2.18972000000001e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": "Chinchilla",
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-04"
  },
  {
    "model": "XMC-GAN",
    "publicationDate": "2022-04-14",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-04"
  },
  {
    "model": "Sparse all-MLP",
    "publicationDate": "2022-04-14",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 9410000000.0,
    "trainingComputeFlop": 5.32224e+20,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-04"
  },
  {
    "model": "Stable Diffusion (LDM-KL-8-G)",
    "organization": [
      "Runway",
      "Ludwig Maximilian University of Munich",
      "Heidelberg University"
    ],
    "publicationDate": "2022-04-13",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 1450000000.0,
    "trainingComputeFlop": 5.0000000000000004e+22,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Germany"
    ],
    "notabilityCriteria": "Significant use",
    "trainingHardware": "NVIDIA A100",
    "baseModel": "Stable Diffusion 1.2",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Runway",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-04"
  },
  {
    "model": "BERT-RBP",
    "organization": "Waseda University",
    "publicationDate": "2022-04-07",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 110000000.0,
    "trainingComputeFlop": 1.4e+20,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "Japan",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "DNABERT",
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Waseda University",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-04"
  },
  {
    "model": "DALL·E 2",
    "organization": "OpenAI",
    "publicationDate": "2022-04-06",
    "domain": "Image generation",
    "task": "Text-to-image",
    "parameters": 3500000000.0,
    "trainingComputeFlop": 3.3695784e+23,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-04"
  },
  {
    "model": "PaLM (540B)",
    "publicationDate": "2022-04-04",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 540350000000.0,
    "trainingComputeFlop": 2.5272e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-04"
  },
  {
    "model": "Chinchilla",
    "publicationDate": "2022-03-29",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 70000000000.0,
    "trainingComputeFlop": 5.76e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-03"
  },
  {
    "model": "Make-A-Scene",
    "publicationDate": "2022-03-24",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 4000000000.0,
    "trainingComputeFlop": 6.4172851e+21,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-03"
  },
  {
    "model": [
      "Segatron-XL large",
      "M=384 + HCP"
    ],
    "organization": [
      "Microsoft Research",
      "University of Waterloo"
    ],
    "publicationDate": "2022-03-21",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 256999999.99999997,
    "trainingComputeFlop": 2.65e+19,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Canada"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Microsoft Research",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-03"
  },
  {
    "model": "ViT-G (model soup)",
    "publicationDate": "2022-03-10",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 1843000000.0,
    "trainingComputeFlop": 3.4e+21,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "Israel"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "University of Washington",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-03"
  },
  {
    "model": "MegaSyn",
    "organization": "Collaborations Pharmaceuticals",
    "publicationDate": "2022-03-07",
    "domain": "Medicine",
    "task": "Drug discovery",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "Collaborations Pharmaceuticals",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-03"
  },
  {
    "model": "Statement Curriculum Learning",
    "organization": "OpenAI",
    "publicationDate": "2022-03-02",
    "domain": "Language",
    "task": "Automated theorem proving",
    "parameters": 774000000.0,
    "trainingComputeFlop": 1.7901648e+22,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-03"
  },
  {
    "model": "DeepNet",
    "organization": "Microsoft Research",
    "publicationDate": "2022-03-01",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 3200000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Microsoft Research",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-03"
  },
  {
    "model": "PolyCoder",
    "organization": "Carnegie Mellon University (CMU)",
    "publicationDate": "2022-02-26",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 2700000000.0,
    "trainingComputeFlop": 1.1e+21,
    "confidence": "Likely",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Quadro RTX 8000",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Carnegie Mellon University (CMU)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-02"
  },
  {
    "model": "FourCastNet",
    "organization": [
      "NVIDIA",
      "NERSC",
      "Lawrence Berkeley National Laboratory",
      "University of Michigan",
      "Rice University",
      "California Institute of Technology",
      "Purdue University"
    ],
    "publicationDate": "2022-02-22",
    "domain": "Earth science",
    "task": "Weather forecasting",
    "parameters": null,
    "trainingComputeFlop": 3.4504704e+20,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Government",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-02"
  },
  {
    "model": "ST-MoE",
    "publicationDate": "2022-02-17",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 269000000000.0,
    "trainingComputeFlop": 2.9e+23,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": [
      "Google Brain",
      "Google Research"
    ],
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-02"
  },
  {
    "model": "Midjourney V1",
    "organization": "Midjourney",
    "publicationDate": "2022-02-15",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "Midjourney",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-02"
  },
  {
    "model": "ProteinBERT",
    "organization": [
      "Hebrew University of Jerusalem",
      "Ben-Gurion University of the Negev",
      "Deep Trading"
    ],
    "publicationDate": "2022-02-10",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 16000000.0,
    "trainingComputeFlop": 6.5e+19,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "Israel",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Quadro RTX 5000",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Hebrew University of Jerusalem",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-02"
  },
  {
    "model": "LaMDA",
    "organization": "Google",
    "publicationDate": "2022-02-10",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 137000000000.0,
    "trainingComputeFlop": 3.55e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-02"
  },
  {
    "model": "GPT-NeoX-20B",
    "organization": "EleutherAI",
    "publicationDate": "2022-02-09",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 20000000000.0,
    "trainingComputeFlop": 9.31627008e+22,
    "confidence": "Confident",
    "organizationCategorization": "Research collective",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "EleutherAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-02"
  },
  {
    "model": "MaskGIT (ImageNet)",
    "publicationDate": "2022-02-08",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 227000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": null,
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Unknown",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-02"
  },
  {
    "model": "RETRO-7B",
    "publicationDate": "2022-02-07",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 7500000000.0,
    "trainingComputeFlop": 1.68e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-02"
  },
  {
    "model": "AlphaCode",
    "publicationDate": "2022-02-02",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 41100000000.0,
    "trainingComputeFlop": 2.38010000000001e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-02"
  },
  {
    "model": "InstructGPT 175B",
    "organization": "OpenAI",
    "publicationDate": "2022-01-27",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 175000000000.0,
    "trainingComputeFlop": 3.19181e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": null,
    "baseModel": "GPT-3 175B (davinci)",
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-01"
  },
  {
    "model": "InstructGPT 6B",
    "organization": "OpenAI",
    "publicationDate": "2022-01-27",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 6000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": null,
    "baseModel": "GPT-3 6.7B",
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-01"
  },
  {
    "model": "InstructGPT 1.3B",
    "organization": "OpenAI",
    "publicationDate": "2022-01-27",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 1300000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": null,
    "baseModel": "GPT-3 XL",
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-01"
  },
  {
    "model": "OntoProtein",
    "organization": "Zhejiang University (ZJU)",
    "publicationDate": "2022-01-23",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 420000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "ProtBERT-BFD",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Zhejiang University (ZJU)",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-01"
  },
  {
    "model": "AbLang (heavy sequences)",
    "organization": "University of Oxford",
    "publicationDate": "2022-01-22",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 355000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United Kingdom",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "University of Oxford",
    "department": null,
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2022,
    "yearMonth": "2022-01"
  },
  {
    "model": "data2vec (vision)",
    "publicationDate": "2022-01-20",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 705134592.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-01"
  },
  {
    "model": "data2vec (speech)",
    "publicationDate": "2022-01-20",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": 705134592.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-01"
  },
  {
    "model": "data2vec (language)",
    "publicationDate": "2022-01-20",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 705134592.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2022,
    "yearMonth": "2022-01"
  },
  {
    "model": "Detic",
    "publicationDate": "2022-01-07",
    "domain": "Vision",
    "task": "Object detection",
    "parameters": 88000000.0,
    "trainingComputeFlop": 2.34399744e+19,
    "confidence": "Speculative",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2022,
    "yearMonth": "2022-01"
  },
  {
    "model": "ERNIE-ViLG",
    "organization": "Baidu",
    "publicationDate": "2021-12-31",
    "domain": "Multimodal",
    "task": "Vision-language generation",
    "parameters": 10000000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Baidu",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-12"
  },
  {
    "model": "ERNIE 3.0 Titan",
    "organization": [
      "Baidu",
      "Peng Cheng Laboratory"
    ],
    "publicationDate": "2021-12-23",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 260000000000.0,
    "trainingComputeFlop": 1.0421e+24,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 32 GB",
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "Baidu",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-12"
  },
  {
    "model": "XGLM-7.5B",
    "publicationDate": "2021-12-20",
    "domain": "Language",
    "task": "Translation",
    "parameters": 7500000000.0,
    "trainingComputeFlop": 2.25e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Meta",
    "department": [
      "Meta AI",
      "Facebook AI Research"
    ],
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-12"
  },
  {
    "model": "LDM-1.45B",
    "organization": [
      "Heidelberg University",
      "Runway"
    ],
    "publicationDate": "2021-12-20",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 1450000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "Germany",
      "United States"
    ],
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Heidelberg University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-12"
  },
  {
    "model": "Contriever",
    "publicationDate": "2021-12-16",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 110000000.0,
    "trainingComputeFlop": 1.57e+20,
    "confidence": "Likely",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "United Kingdom",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "BERT-Large",
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-12"
  },
  {
    "model": "LongT5",
    "publicationDate": "2021-12-15",
    "domain": "Language",
    "task": "Text summarization",
    "parameters": 3000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-12"
  },
  {
    "model": "GLaM",
    "organization": "Google",
    "publicationDate": "2021-12-13",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 1200000000000.0,
    "trainingComputeFlop": 3.6363112434e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Massive (≥1T)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-12"
  },
  {
    "model": "Gopher (280B)",
    "publicationDate": "2021-12-08",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 280000000000.0,
    "trainingComputeFlop": 6.31e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-12"
  },
  {
    "model": "Student of Games",
    "publicationDate": "2021-12-06",
    "domain": "Games",
    "task": "Chess",
    "parameters": null,
    "trainingComputeFlop": 3.6679273004682866e+22,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-12"
  },
  {
    "model": "T-NLRv5 XXL",
    "organization": "Microsoft",
    "publicationDate": "2021-12-03",
    "domain": "Language",
    "task": null,
    "parameters": 5400000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-12"
  },
  {
    "model": "NÜWA",
    "organization": [
      "Microsoft Research",
      "Peking University"
    ],
    "publicationDate": "2021-11-24",
    "domain": "Multimodal",
    "task": "Image generation",
    "parameters": 870000000.0,
    "trainingComputeFlop": 7.24598784e+21,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "China"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Microsoft Research",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-11"
  },
  {
    "model": "Florence",
    "organization": "Microsoft",
    "publicationDate": "2021-11-22",
    "domain": "Vision",
    "task": "Image captioning",
    "parameters": 893000000.0,
    "trainingComputeFlop": 4.831e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-11"
  },
  {
    "model": "BASIC-L",
    "organization": "Google",
    "publicationDate": "2021-11-19",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 3070000000.0,
    "trainingComputeFlop": 4.12e+22,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v4",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-11"
  },
  {
    "model": "Swin Transformer V2 (SwinV2-G)",
    "organization": "Microsoft Research Asia",
    "publicationDate": "2021-11-18",
    "domain": "Vision",
    "task": "Action recognition",
    "parameters": 3000000000.0,
    "trainingComputeFlop": 1.1e+21,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 40 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Microsoft Research Asia",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-11"
  },
  {
    "model": "ViT-G/14 (LiT)",
    "publicationDate": "2021-11-15",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 3005000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": "ViT-G/14",
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-11"
  },
  {
    "model": "Masked Autoencoders ViT-H",
    "publicationDate": "2021-11-11",
    "domain": "Vision",
    "task": "Semantic segmentation",
    "parameters": 632000000.0,
    "trainingComputeFlop": 4.6e+20,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": "ViT-Huge/14",
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-11"
  },
  {
    "model": "Projected GAN",
    "organization": "Heidelberg University",
    "publicationDate": "2021-11-01",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": 1.05e+19,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "Germany",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Heidelberg University",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2021,
    "yearMonth": "2021-11"
  },
  {
    "model": "CodeT5-base",
    "organization": [
      "Salesforce",
      "Nanyang Technological University"
    ],
    "publicationDate": "2021-11-01",
    "domain": "Language",
    "task": "Code generation",
    "parameters": 220000000.0,
    "trainingComputeFlop": 1.56e+21,
    "confidence": "Likely",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Singapore"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Salesforce",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-11"
  },
  {
    "model": "S4",
    "organization": "Stanford University",
    "publicationDate": "2021-10-31",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 249000000.00000003,
    "trainingComputeFlop": 7.8328627e+19,
    "confidence": "Likely",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Stanford University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2021,
    "yearMonth": "2021-10"
  },
  {
    "model": "EfficientZero",
    "organization": [
      "Tsinghua University",
      "University of California (UC) Berkeley",
      "Shanghai Qi Zhi institute"
    ],
    "publicationDate": "2021-10-30",
    "domain": "Games",
    "task": "Atari",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "China",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2021,
    "yearMonth": "2021-10"
  },
  {
    "model": "Eve",
    "organization": [
      "Harvard Medical School",
      "University of Oxford"
    ],
    "publicationDate": "2021-10-27",
    "domain": "Biology",
    "task": "Protein pathogenicity prediction",
    "parameters": 15010300.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "United States",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Harvard Medical School",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2021,
    "yearMonth": "2021-10"
  },
  {
    "model": "base LM+GNN+kNN",
    "organization": [
      "Shannon.AI",
      "Nanjing University",
      "Nanyang Technological University",
      "Zhejiang University (ZJU)"
    ],
    "publicationDate": "2021-10-17",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 274000000.0,
    "trainingComputeFlop": 5.2587456e+19,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "China",
      "Singapore"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Transformer (Adaptive Input Embeddings) WT103",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Shannon.AI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-10"
  },
  {
    "model": "Yuan 1.0",
    "organization": "Inspur",
    "publicationDate": "2021-10-12",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 245730000000.0,
    "trainingComputeFlop": 3.5380000000001e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "Inspur",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-10"
  },
  {
    "model": "Megatron-Turing NLG 530B",
    "organization": [
      "Microsoft",
      "NVIDIA"
    ],
    "publicationDate": "2021-10-11",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 530000000000.0,
    "trainingComputeFlop": 8.586e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100 SXM4 80 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-10"
  },
  {
    "model": "AlphaFold-Multimer",
    "publicationDate": "2021-10-04",
    "domain": "Biology",
    "task": "Protein folding prediction",
    "parameters": null,
    "trainingComputeFlop": 4.35e+21,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": "AlphaFold 2",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-10"
  },
  {
    "model": "Turing ULRv5",
    "organization": "Microsoft",
    "publicationDate": "2021-09-28",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 2200000000.0,
    "trainingComputeFlop": 2.8983951e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-09"
  },
  {
    "model": "TrOCR",
    "organization": [
      "Beihang University",
      "Microsoft Research Asia"
    ],
    "publicationDate": "2021-09-21",
    "domain": "Vision",
    "task": "Character recognition (OCR)",
    "parameters": 558000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Beihang University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-09"
  },
  {
    "model": "PLATO-XL",
    "organization": "Baidu",
    "publicationDate": "2021-09-20",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 11000000000.0,
    "trainingComputeFlop": 9.9e+21,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Baidu",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-09"
  },
  {
    "model": "HyperCLOVA 204B",
    "organization": "NAVER",
    "publicationDate": "2021-09-10",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 204000000000.0,
    "trainingComputeFlop": 2.0000000001e+23,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Korea (Republic of)",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "NAVER",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-09"
  },
  {
    "model": "PermuteFormer",
    "organization": "Peking University",
    "publicationDate": "2021-09-06",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 149697024.0,
    "trainingComputeFlop": 2.775e+18,
    "confidence": "Speculative",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Peking University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2021,
    "yearMonth": "2021-09"
  },
  {
    "model": "MEB",
    "organization": "Microsoft",
    "publicationDate": "2021-09-04",
    "domain": "Search",
    "task": "Search",
    "parameters": 135000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-09"
  },
  {
    "model": "FLAN 137B",
    "publicationDate": "2021-09-03",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 137000000000.0,
    "trainingComputeFlop": 2.047e+24,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": "LaMDA",
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-09"
  },
  {
    "model": "XLMR-XXL",
    "publicationDate": "2021-08-17",
    "domain": "Language",
    "task": "Translation",
    "parameters": 10700000000.0,
    "trainingComputeFlop": 3.366e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-08"
  },
  {
    "model": "DNABERT",
    "organization": "Northeastern University",
    "publicationDate": "2021-08-15",
    "domain": "Biology",
    "task": "Protein or nucleotide language model (pLM/nLM)",
    "parameters": 110000000.0,
    "trainingComputeFlop": 1.07e+20,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA GeForce RTX 2080 Ti 11GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Northeastern University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2021,
    "yearMonth": "2021-08"
  },
  {
    "model": "Zidong Taichu",
    "organization": [
      "Chinese Academy of Sciences",
      "Wuhan AI Computing Center"
    ],
    "publicationDate": "2021-08-11",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": 3200000000.0,
    "trainingComputeFlop": 8.016e+20,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Government"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Chinese Academy of Sciences",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-08"
  },
  {
    "model": "Jurassic-1-Jumbo",
    "organization": "AI21 Labs",
    "publicationDate": "2021-08-11",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 178000000000.0,
    "trainingComputeFlop": 3.7e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Israel",
    "notabilityCriteria": "Training cost",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "AI21 Labs",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-08"
  },
  {
    "model": "W2v-BERT",
    "publicationDate": "2021-08-07",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": 1000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-08"
  },
  {
    "model": "YOLOX-X",
    "organization": "Megvii Inc",
    "publicationDate": "2021-08-06",
    "domain": "Vision",
    "task": "Object detection",
    "parameters": 99100000.0,
    "trainingComputeFlop": 6.34275e+20,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Megvii Inc",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-08"
  },
  {
    "model": "6-Act Tether",
    "publicationDate": "2021-08-03",
    "domain": "Robotics",
    "task": "Object detection",
    "parameters": 5000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-08"
  },
  {
    "model": "SEER",
    "publicationDate": "2021-07-29",
    "domain": "Vision",
    "task": "Image embedding",
    "parameters": 1300000000.0,
    "trainingComputeFlop": 1.8e+22,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-07"
  },
  {
    "model": "HuBERT",
    "publicationDate": "2021-07-27",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": 1000000000.0,
    "trainingComputeFlop": 5.54e+21,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-07"
  },
  {
    "model": "GOAT",
    "publicationDate": "2021-07-27",
    "domain": "Games",
    "task": "Open ended play",
    "parameters": 3472816.0,
    "trainingComputeFlop": 2.412e+22,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-07"
  },
  {
    "model": "Codex",
    "organization": "OpenAI",
    "publicationDate": "2021-07-07",
    "domain": "Language",
    "task": "Code autocompletion",
    "parameters": 12000000000.0,
    "trainingComputeFlop": 7.344e+22,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": "GPT-3 13B",
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-07"
  },
  {
    "model": "ERNIE 3.0",
    "organization": "Baidu",
    "publicationDate": "2021-07-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 10000000000.0,
    "trainingComputeFlop": 2.25e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Baidu",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-07"
  },
  {
    "model": "Adaptive Input Transformer + RD",
    "organization": [
      "Microsoft Research Asia",
      "Soochow University"
    ],
    "publicationDate": "2021-06-28",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 247000000.00000003,
    "trainingComputeFlop": 8.58e+19,
    "confidence": "Likely",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "China",
      "Taiwan"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Microsoft Research Asia",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-06"
  },
  {
    "model": "Fold2Seq",
    "organization": [
      "IBM",
      "Texas A&M"
    ],
    "publicationDate": "2021-06-24",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 12427904.0,
    "trainingComputeFlop": 1.4e+17,
    "confidence": "Speculative",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Tesla K80",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "IBM",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-06"
  },
  {
    "model": "EfficientNetV2-XL",
    "publicationDate": "2021-06-23",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 208000000.0,
    "trainingComputeFlop": 9.56e+19,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Google",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-06"
  },
  {
    "model": "StyleGAN3-T",
    "organization": [
      "NVIDIA",
      "Aalto University"
    ],
    "publicationDate": "2021-06-21",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 2230000.0,
    "trainingComputeFlop": 1.70208e+21,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Finland"
    ],
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-06"
  },
  {
    "model": "StyleGAN3-R",
    "organization": [
      "NVIDIA",
      "Aalto University"
    ],
    "publicationDate": "2021-06-21",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 1580000.0,
    "trainingComputeFlop": 2.42784e+21,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Finland"
    ],
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-06"
  },
  {
    "model": "Denoising Diffusion Probabilistic Models (LSUN Bedroom)",
    "organization": "University of California (UC) Berkeley",
    "publicationDate": "2021-06-11",
    "domain": "Vision",
    "task": "Image generation",
    "parameters": 256000000.0,
    "trainingComputeFlop": 7.840125000000001e+19,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "University of California (UC) Berkeley",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2021,
    "yearMonth": "2021-06"
  },
  {
    "model": "ALIGN",
    "publicationDate": "2021-06-11",
    "domain": "Multimodal",
    "task": "Representation learning",
    "parameters": 820000000.0,
    "trainingComputeFlop": 2.598670000001e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-06"
  },
  {
    "model": "DeBERTa",
    "organization": "Microsoft",
    "publicationDate": "2021-06-10",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 1500000000.0,
    "trainingComputeFlop": 2.588e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-06"
  },
  {
    "model": "EMDR",
    "publicationDate": "2021-06-09",
    "domain": "Language",
    "task": "Question answering",
    "parameters": 440000000.0,
    "trainingComputeFlop": 1.91e+21,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "Canada",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-06"
  },
  {
    "model": "CoAtNet",
    "publicationDate": "2021-06-09",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 2440000000.0,
    "trainingComputeFlop": 4.27e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": [
      "Google Research",
      "Google Brain"
    ],
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-06"
  },
  {
    "model": "ViT-G/14",
    "publicationDate": "2021-06-08",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 1843000000.0,
    "trainingComputeFlop": 5.85e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": [
      "Google Brain",
      "Google Research"
    ],
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-06"
  },
  {
    "model": "ByT5-XXL",
    "publicationDate": "2021-05-28",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 12900000000.0,
    "trainingComputeFlop": 8.1e+22,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-05"
  },
  {
    "model": "Transformer local-attention (NesT-B)",
    "publicationDate": "2021-05-26",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 90100000.0,
    "trainingComputeFlop": 2.40576e+19,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Google Cloud",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-05"
  },
  {
    "model": "CogView",
    "organization": [
      "Tsinghua University",
      "Alibaba DAMO Academy"
    ],
    "publicationDate": "2021-05-26",
    "domain": "Image generation",
    "task": "Text-to-image",
    "parameters": 4000000000.0,
    "trainingComputeFlop": 2.68e+22,
    "confidence": "Likely",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 16 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-05"
  },
  {
    "model": "MedBERT",
    "organization": [
      "Peng Cheng Laboratory",
      "University of Texas at Houston"
    ],
    "publicationDate": "2021-05-20",
    "domain": "Medicine",
    "task": "Medical diagnosis",
    "parameters": 17000000.0,
    "trainingComputeFlop": 9.47e+18,
    "confidence": "Likely",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "China",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 32 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Peng Cheng Laboratory",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2021,
    "yearMonth": "2021-05"
  },
  {
    "model": "ADM",
    "organization": "OpenAI",
    "publicationDate": "2021-05-11",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 559000000.0,
    "trainingComputeFlop": 6.2e+21,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-05"
  },
  {
    "model": "ProtT5-XL-U50",
    "organization": [
      "Technical University of Munich",
      "Med AI Technology",
      "NVIDIA",
      "Oak Ridge National Laboratory",
      "Google",
      "Seoul National University"
    ],
    "publicationDate": "2021-05-04",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 3000000000.0,
    "trainingComputeFlop": 1.8704498688e+22,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry",
      "Government"
    ],
    "countryOfOrganization": [
      "Germany",
      "China",
      "United States",
      "Korea (Republic of)"
    ],
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Technical University of Munich",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-05"
  },
  {
    "model": "ProtBERT-BFD",
    "organization": [
      "Technical University of Munich",
      "NVIDIA",
      "Seoul National University",
      "Google",
      "Oak Ridge National Laboratory",
      "Med AI Technology"
    ],
    "publicationDate": "2021-05-04",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 420000000.0,
    "trainingComputeFlop": 3.9e+22,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry",
      "Government"
    ],
    "countryOfOrganization": [
      "Germany",
      "United States",
      "Korea (Republic of)",
      "China"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Technical University of Munich",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-05"
  },
  {
    "model": "PLUG",
    "organization": "Alibaba",
    "publicationDate": "2021-04-19",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 27000000000.0,
    "trainingComputeFlop": 3.5997696e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA A100",
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-04"
  },
  {
    "model": "Unicorn",
    "organization": "Allen Institute for AI",
    "publicationDate": "2021-03-24",
    "domain": "Language",
    "task": "Question answering",
    "parameters": 11000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Research collective",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "T5-11B",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Allen Institute for AI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-03"
  },
  {
    "model": "M6-T",
    "organization": "Alibaba",
    "publicationDate": "2021-03-05",
    "domain": "Multimodal",
    "task": "Chat",
    "parameters": 1002700000000.0,
    "trainingComputeFlop": 5.5e+21,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 32 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Alibaba",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Massive (≥1T)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-03"
  },
  {
    "model": "Generative BST",
    "publicationDate": "2021-03-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 9431810048.0,
    "trainingComputeFlop": 1.449e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-03"
  },
  {
    "model": "Meta Pseudo Labels",
    "publicationDate": "2021-03-01",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 480000000.0,
    "trainingComputeFlop": 4.79e+22,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-03"
  },
  {
    "model": "SRU++ Large",
    "organization": "ASAPP",
    "publicationDate": "2021-02-24",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 234000000.0,
    "trainingComputeFlop": 2.1173704e+19,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "ASAPP",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-02"
  },
  {
    "model": "Rational DQN Average",
    "organization": "TU Darmstadt",
    "publicationDate": "2021-02-18",
    "domain": "Games",
    "task": "Atari",
    "parameters": 1683456.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "organizationCategorization": "Academia",
    "countryOfOrganization": "Germany",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "TU Darmstadt",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2021,
    "yearMonth": "2021-02"
  },
  {
    "model": "MSA Transformer",
    "publicationDate": "2021-02-13",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 100000000.0,
    "trainingComputeFlop": 5.49e+21,
    "confidence": "Likely",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 32 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-02"
  },
  {
    "model": "top-down frozen classifier",
    "organization": [
      "University of Edinburgh",
      "Toshiba Cambridge Research Laboratory"
    ],
    "publicationDate": "2021-02-09",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United Kingdom",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "University of Edinburgh",
    "department": null,
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-02"
  },
  {
    "model": "DLWP",
    "organization": [
      "University of Washington",
      "Microsoft Research"
    ],
    "publicationDate": "2021-02-09",
    "domain": "Earth science",
    "task": "Weather forecasting",
    "parameters": 2676376.0,
    "trainingComputeFlop": 5.6845152e+18,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 32 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "University of Washington",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-02"
  },
  {
    "model": "DeiT-B",
    "publicationDate": "2021-01-15",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 86000000.0,
    "trainingComputeFlop": 7.884e+19,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": "Meta AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2021,
    "yearMonth": "2021-01"
  },
  {
    "model": "Switch",
    "organization": "Google",
    "publicationDate": "2021-01-11",
    "domain": "Language",
    "task": "Text autocompletion",
    "parameters": 1571000000000.0,
    "trainingComputeFlop": 8.22e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Massive (≥1T)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-01"
  },
  {
    "model": "BigSSL",
    "organization": [
      "Google",
      "Apple"
    ],
    "publicationDate": "2021-01-10",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": 8000000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-01"
  },
  {
    "model": "DALL-E",
    "organization": "OpenAI",
    "publicationDate": "2021-01-05",
    "domain": "Image generation",
    "task": "Text-to-image",
    "parameters": 12000000000.0,
    "trainingComputeFlop": 4.7e+22,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 16 GB",
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-01"
  },
  {
    "model": "CLIP (ViT L/14@336px)",
    "organization": "OpenAI",
    "publicationDate": "2021-01-05",
    "domain": "Multimodal",
    "task": "Zero-shot image classification",
    "parameters": 370000000.0,
    "trainingComputeFlop": 1.05e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-01"
  },
  {
    "model": "CLIP (ResNet-50)",
    "organization": "OpenAI",
    "publicationDate": "2021-01-05",
    "domain": "Multimodal",
    "task": "Zero-shot image classification",
    "parameters": 88600000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2021,
    "yearMonth": "2021-01"
  },
  {
    "model": "ERNIE-Doc (247M)",
    "organization": "Baidu",
    "publicationDate": "2020-12-31",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 247000000.00000003,
    "trainingComputeFlop": 3.0302798e+19,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Baidu",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-12"
  },
  {
    "model": "CT-MoS (WT2)",
    "organization": [
      "Google",
      "National Tsing Hua University"
    ],
    "publicationDate": "2020-12-25",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 45000000.0,
    "trainingComputeFlop": 5.4e+17,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Taiwan"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA GeForce GTX 1080 Ti",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-12"
  },
  {
    "model": "DensePhrases",
    "organization": [
      "Korea University",
      "Princeton University"
    ],
    "publicationDate": "2020-12-23",
    "domain": "Language",
    "task": "Question answering",
    "parameters": null,
    "trainingComputeFlop": 2.09952e+18,
    "confidence": "Speculative",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "Korea (Republic of)",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA TITAN Xp",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Korea University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2020,
    "yearMonth": "2020-12"
  },
  {
    "model": "VQGAN + CLIP",
    "organization": "Heidelberg University",
    "publicationDate": "2020-12-17",
    "domain": "Image generation",
    "task": "Text-to-image",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "Germany",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Heidelberg University",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2020,
    "yearMonth": "2020-12"
  },
  {
    "model": "ESM1b",
    "publicationDate": "2020-12-15",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 652400000.0,
    "trainingComputeFlop": 5.1e+21,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-12"
  },
  {
    "model": "CPM-Large",
    "organization": [
      "Tsinghua University",
      "Beijing Academy of Artificial Intelligence / BAAI"
    ],
    "publicationDate": "2020-12-01",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 2600000000.0,
    "trainingComputeFlop": 2.6052e+20,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Public",
    "year": 2020,
    "yearMonth": "2020-12"
  },
  {
    "model": "AlphaFold 2",
    "publicationDate": "2020-11-30",
    "domain": "Biology",
    "task": "Protein folding prediction",
    "parameters": 93000000.0,
    "trainingComputeFlop": 2.99e+21,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-11"
  },
  {
    "model": "KEPLER",
    "organization": [
      "Tsinghua University",
      "Mila - Quebec AI (originally Montreal Institute for Learning Algorithms)",
      "HEC",
      "CIFAR AI Research",
      "Princeton University",
      "University of Montreal / Université de Montréal"
    ],
    "publicationDate": "2020-11-23",
    "domain": "Language",
    "task": "Relation extraction",
    "parameters": 125000000.0,
    "trainingComputeFlop": 1.66e+21,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Research collective"
    ],
    "countryOfOrganization": [
      "China",
      "Canada",
      "France",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "RoBERTa Base",
    "modelAccessibility": "Unreleased",
    "company": "Tsinghua University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-11"
  },
  {
    "model": "wave2vec 2.0 LARGE",
    "organization": "Facebook",
    "publicationDate": "2020-10-22",
    "domain": "Speech",
    "task": "Speech completion",
    "parameters": 317000000.0,
    "trainingComputeFlop": 3.87072e+21,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 32 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-10"
  },
  {
    "model": "ViT-Huge/14",
    "publicationDate": "2020-10-22",
    "domain": "Vision",
    "task": "Image representation",
    "parameters": 632000000.0,
    "trainingComputeFlop": 4.262e+21,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Google",
    "department": [
      "Google Brain",
      "Google Research"
    ],
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-10"
  },
  {
    "model": "ViT-Base/32",
    "publicationDate": "2020-10-22",
    "domain": "Vision",
    "task": "Image representation",
    "parameters": 86000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Google",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-10"
  },
  {
    "model": "German ELECTRA Large",
    "organization": [
      "deepset",
      "Bayerische Staatsbibliothek Muenchen"
    ],
    "publicationDate": "2020-10-21",
    "domain": "Language",
    "task": "Document classification",
    "parameters": 335000000.0,
    "trainingComputeFlop": 1.42829568e+21,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Government"
    ],
    "countryOfOrganization": "Germany",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "deepset",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-10"
  },
  {
    "model": "mT5-XXL",
    "publicationDate": "2020-10-20",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 13000000000.0,
    "trainingComputeFlop": 8.2e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-10"
  },
  {
    "model": "Conformer + Wav2vec 2.0 + Noisy Student",
    "publicationDate": "2020-10-20",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": 1000000000.0,
    "trainingComputeFlop": 7.6e+21,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": [
      "Google Research",
      "Google Brain"
    ],
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-10"
  },
  {
    "model": "LUKE",
    "organization": [
      "University of Washington",
      "National Institute of Informatics"
    ],
    "publicationDate": "2020-10-02",
    "domain": "Language",
    "task": "Question answering",
    "parameters": 483000000.0,
    "trainingComputeFlop": 1.8144e+22,
    "confidence": "Likely",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "United States",
      "Japan"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": "RoBERTa Large",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "University of Washington",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2020,
    "yearMonth": "2020-10"
  },
  {
    "model": "ProBERTa",
    "organization": [
      "University of Illinois Urbana-Champaign (UIUC)",
      "Reed College"
    ],
    "publicationDate": "2020-09-01",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 44000000.0,
    "trainingComputeFlop": 9.72e+18,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "University of Illinois Urbana-Champaign (UIUC)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2020,
    "yearMonth": "2020-09"
  },
  {
    "model": "ERNIE-GEN (large)",
    "organization": "Baidu",
    "publicationDate": "2020-08-06",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 340000000.0,
    "trainingComputeFlop": 2e+20,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Baidu",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-08"
  },
  {
    "model": "DeLighT",
    "publicationDate": "2020-08-03",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 99000000.0,
    "trainingComputeFlop": 3.8016e+18,
    "confidence": "Likely",
    "organizationCategorization": [
      "Academia",
      "Research collective",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "University of Washington",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-08"
  },
  {
    "model": "EfficientDet",
    "publicationDate": "2020-07-27",
    "domain": "Vision",
    "task": "Object detection",
    "parameters": 77000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Google",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-07"
  },
  {
    "model": "Hopfield Networks (2020)",
    "organization": [
      "Johannes Kepler University Linz",
      "Institute of Advanced Research in Artificial Intelligence",
      "University of Oslo"
    ],
    "publicationDate": "2020-07-16",
    "domain": "Biology",
    "task": "Drug discovery",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "Austria",
      "Norway"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Johannes Kepler University Linz",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2020,
    "yearMonth": "2020-07"
  },
  {
    "model": "SemExp",
    "publicationDate": "2020-07-02",
    "domain": "Robotics",
    "task": "Object detection",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Carnegie Mellon University (CMU)",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-07"
  },
  {
    "model": "GShard (dense)",
    "organization": "Google",
    "publicationDate": "2020-06-30",
    "domain": "Language",
    "task": "Translation",
    "parameters": 2300000000.0,
    "trainingComputeFlop": 4.765e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-06"
  },
  {
    "model": "GPT-3 175B (davinci)",
    "organization": "OpenAI",
    "publicationDate": "2020-05-28",
    "domain": "Language",
    "task": "Text autocompletion",
    "parameters": 174600000000.0,
    "trainingComputeFlop": 3.14e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 32 GB",
    "baseModel": null,
    "modelAccessibility": "API access",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-05"
  },
  {
    "model": "DETR",
    "organization": "Facebook",
    "publicationDate": "2020-05-26",
    "domain": "Vision",
    "task": "Object detection",
    "parameters": 60000000.0,
    "trainingComputeFlop": 4e+20,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-05"
  },
  {
    "model": "Retrieval-Augmented Generator",
    "organization": [
      "Facebook",
      "New York University (NYU)",
      "University College London (UCL)"
    ],
    "publicationDate": "2020-05-22",
    "domain": "Language",
    "task": "Question answering",
    "parameters": 626000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "United Kingdom"
    ],
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA Tesla V100 PCIe 32 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-05"
  },
  {
    "model": "Conformer",
    "organization": "Google",
    "publicationDate": "2020-05-16",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": 118800000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-05"
  },
  {
    "model": "ContextNet",
    "organization": "Google",
    "publicationDate": "2020-05-07",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": 112700000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-05"
  },
  {
    "model": "NAS+ESS (23M)",
    "organization": [
      "Northeastern University (China)",
      "NiuTrans Research",
      "Kingsoft"
    ],
    "publicationDate": "2020-05-06",
    "domain": "Language",
    "task": "Neural Architecture Search - NAS",
    "parameters": 23000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA GeForce GTX 1080 Ti",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Northeastern University (China)",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-05"
  },
  {
    "model": "UnifiedQA",
    "organization": [
      "Allen Institute for AI",
      "University of Washington"
    ],
    "publicationDate": "2020-05-02",
    "domain": "Language",
    "task": "Question answering",
    "parameters": 11000000000.0,
    "trainingComputeFlop": 1.65e+19,
    "confidence": "Confident",
    "organizationCategorization": [
      "Research collective",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": "T5-11B",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Allen Institute for AI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-05"
  },
  {
    "model": "Once for All",
    "organization": [
      "MIT-IBM Watson AI Lab",
      "Massachusetts Institute of Technology (MIT)",
      "IBM"
    ],
    "publicationDate": "2020-04-29",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 7700000.0,
    "trainingComputeFlop": 6.237e+20,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "MIT-IBM Watson AI Lab",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-04"
  },
  {
    "model": "Go-explore",
    "organization": [
      "Uber AI",
      "OpenAI"
    ],
    "publicationDate": "2020-04-27",
    "domain": "Games",
    "task": "Atari",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Uber AI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-04"
  },
  {
    "model": "CURL",
    "organization": "University of California (UC) Berkeley",
    "publicationDate": "2020-04-08",
    "domain": "Games",
    "task": "Atari",
    "parameters": 907264.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "University of California (UC) Berkeley",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2020,
    "yearMonth": "2020-04"
  },
  {
    "model": "Agent57",
    "publicationDate": "2020-03-30",
    "domain": "Games",
    "task": "Atari",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-03"
  },
  {
    "model": "MetNet",
    "organization": "Google",
    "publicationDate": "2020-03-24",
    "domain": "Earth science",
    "task": "Weather forecasting",
    "parameters": 225000000.0,
    "trainingComputeFlop": 9.510912e+18,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-03"
  },
  {
    "model": "ELECTRA",
    "publicationDate": "2020-03-23",
    "domain": "Language",
    "task": "Text autocompletion",
    "parameters": 335000000.0,
    "trainingComputeFlop": 3.1e+21,
    "confidence": "Likely",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Stanford University",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-03"
  },
  {
    "model": "Tensor-Transformer(1core)+PN (WT103)",
    "organization": "University of California (UC) Berkeley",
    "publicationDate": "2020-03-17",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 85300000.0,
    "trainingComputeFlop": 1.58e+18,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "University of California (UC) Berkeley",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2020,
    "yearMonth": "2020-03"
  },
  {
    "model": "Routing Transformer (WT-103)",
    "publicationDate": "2020-03-12",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 79500000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Google",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-03"
  },
  {
    "model": "TransformerXL + spectrum control",
    "organization": [
      "University of California Los Angeles (UCLA)",
      "JD.com"
    ],
    "publicationDate": "2020-03-11",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 151000000.0,
    "trainingComputeFlop": 2.6289761e+19,
    "confidence": "Speculative",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "China"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "University of California Los Angeles (UCLA)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-03"
  },
  {
    "model": "TCAN (WT2)",
    "organization": [
      "Nanjing University",
      "Ant Group"
    ],
    "publicationDate": "2020-02-28",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 33000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Nanjing University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-02"
  },
  {
    "model": "Feedback Transformer",
    "publicationDate": "2020-02-21",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 126000000.0,
    "trainingComputeFlop": 7.690547e+18,
    "confidence": "Speculative",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "France",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "LORIA",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-02"
  },
  {
    "model": "FFN SwiGLU",
    "organization": "Google",
    "publicationDate": "2020-02-14",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 220000000.0,
    "trainingComputeFlop": 3.3687317e+19,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "Google TPU v2",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-02"
  },
  {
    "model": "Turing-NLG",
    "organization": "Microsoft",
    "publicationDate": "2020-02-13",
    "domain": "Language",
    "task": "Text autocompletion",
    "parameters": 17000000000.0,
    "trainingComputeFlop": 1.57e+22,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 32 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-02"
  },
  {
    "model": "SimCLR",
    "publicationDate": "2020-02-13",
    "domain": "Vision",
    "task": "Image completion",
    "parameters": 375000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Google",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-02"
  },
  {
    "model": "ALBERT-xxlarge",
    "organization": [
      "Toyota Technological Institute at Chicago",
      "Google"
    ],
    "publicationDate": "2020-02-09",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 235000000.0,
    "trainingComputeFlop": 2.39e+21,
    "confidence": "Speculative",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Toyota Technological Institute at Chicago",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-02"
  },
  {
    "model": "TaLK Convolution",
    "organization": "Carleton University",
    "publicationDate": "2020-02-08",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 240000000.0,
    "trainingComputeFlop": 2.6990346e+19,
    "confidence": "Likely",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "Canada",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA GeForce RTX 2080 Ti 11GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Carleton University",
    "department": null,
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2020,
    "yearMonth": "2020-02"
  },
  {
    "model": "Perceiver IO (optical flow)",
    "publicationDate": "2020-02-08",
    "domain": "Multimodal",
    "task": "Language modeling/generation",
    "parameters": 27900000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-02"
  },
  {
    "model": "Theseus 6/768",
    "organization": [
      "University of California San Diego",
      "Beihang University",
      "Microsoft"
    ],
    "publicationDate": "2020-02-07",
    "domain": "Language",
    "task": "Text autocompletion",
    "parameters": 66000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "China"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": "BERT-Large",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "University of California San Diego",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2020,
    "yearMonth": "2020-02"
  },
  {
    "model": "Meena",
    "publicationDate": "2020-01-28",
    "domain": "Language",
    "task": "Text autocompletion",
    "parameters": 2600000000.0,
    "trainingComputeFlop": 1.12e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-01"
  },
  {
    "model": "ContextNet + Noisy Student",
    "organization": "Google",
    "publicationDate": "2020-01-19",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": null,
    "trainingComputeFlop": 8.16e+21,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-01"
  },
  {
    "model": "AlphaFold",
    "publicationDate": "2020-01-15",
    "domain": "Biology",
    "task": "Protein folding prediction",
    "parameters": 16340840.0,
    "trainingComputeFlop": 1e+20,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2020,
    "yearMonth": "2020-01"
  },
  {
    "model": "Big Transfer (BiT-L)",
    "publicationDate": "2019-12-24",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 928000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-12"
  },
  {
    "model": "DD-PPO",
    "publicationDate": "2019-12-19",
    "domain": "Robotics",
    "task": "Object detection",
    "parameters": null,
    "trainingComputeFlop": 7.8e+20,
    "confidence": "Likely",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "France",
      "Canada"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Georgia Institute of Technology",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-12"
  },
  {
    "model": "OpenAI Five Rerun",
    "organization": "OpenAI",
    "publicationDate": "2019-12-13",
    "domain": "Games",
    "task": "Dota 2",
    "parameters": 159000000.0,
    "trainingComputeFlop": 1.3e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA P100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-12"
  },
  {
    "model": "OpenAI Five",
    "organization": "OpenAI",
    "publicationDate": "2019-12-13",
    "domain": "Games",
    "task": "Dota 2",
    "parameters": 159000000.0,
    "trainingComputeFlop": 6.7e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA P100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-12"
  },
  {
    "model": "MMLSTM (WT-2)",
    "organization": [
      "Beijing University of Posts and Telecommunications",
      "University of West London"
    ],
    "publicationDate": "2019-12-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 32300000.0,
    "trainingComputeFlop": 1.938e+17,
    "confidence": "Likely",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "China",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Beijing University of Posts and Telecommunications",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2019,
    "yearMonth": "2019-12"
  },
  {
    "model": "MMLSTM (PTB)",
    "organization": [
      "Beijing University of Posts and Telecommunications",
      "University of West London"
    ],
    "publicationDate": "2019-12-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 21300000.0,
    "trainingComputeFlop": 5.8298782e+16,
    "confidence": "Likely",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "China",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Beijing University of Posts and Telecommunications",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2019,
    "yearMonth": "2019-12"
  },
  {
    "model": "StarGAN v2",
    "organization": [
      "NAVER",
      "Yonsei University",
      "Swiss Federal Institute of Technology"
    ],
    "publicationDate": "2019-12-04",
    "domain": "Vision",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "Korea (Republic of)",
      "Switzerland"
    ],
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "NAVER",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-12"
  },
  {
    "model": "StyleGAN2",
    "organization": [
      "NVIDIA",
      "Aalto University"
    ],
    "publicationDate": "2019-12-03",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 30000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Finland"
    ],
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (restricted use)",
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-12"
  },
  {
    "model": "Transformer-XL DeFINE (141M)",
    "organization": [
      "University of Washington",
      "Allen Institute for AI"
    ],
    "publicationDate": "2019-11-27",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 141000000.0,
    "trainingComputeFlop": 1.74276e+18,
    "confidence": "Speculative",
    "organizationCategorization": [
      "Academia",
      "Research collective"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "University of Washington",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-11"
  },
  {
    "model": "Photo-Geometric Autoencoder",
    "organization": "University of Oxford",
    "publicationDate": "2019-11-25",
    "domain": "3D modeling",
    "task": "3D reconstruction",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United Kingdom",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "University of Oxford",
    "department": null,
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2019,
    "yearMonth": "2019-11"
  },
  {
    "model": "Transformer - LibriVox + Decoding/Rescoring",
    "organization": "Facebook",
    "publicationDate": "2019-11-19",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": 296000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-11"
  },
  {
    "model": "MuZero",
    "publicationDate": "2019-11-19",
    "domain": "Games",
    "task": "Atari",
    "parameters": 36864000.0,
    "trainingComputeFlop": 4.8e+19,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-11"
  },
  {
    "model": "MoCo",
    "publicationDate": "2019-11-13",
    "domain": "Vision",
    "task": "Image completion",
    "parameters": 375000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Meta",
    "department": "Facebook AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-11"
  },
  {
    "model": "Noisy Student (L2)",
    "organization": [
      "Carnegie Mellon University (CMU)",
      "Google"
    ],
    "publicationDate": "2019-11-11",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 480000000.0,
    "trainingComputeFlop": 2.612e+22,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Carnegie Mellon University (CMU)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-11"
  },
  {
    "model": "Sandwich Transformer",
    "publicationDate": "2019-11-10",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 209000000.0,
    "trainingComputeFlop": 2.3504093e+19,
    "confidence": "Speculative",
    "organizationCategorization": [
      "Research collective",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Allen Institute for AI",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-11"
  },
  {
    "model": "CamemBERT",
    "organization": [
      "Facebook",
      "INRIA",
      "Sorbonne University"
    ],
    "publicationDate": "2019-11-10",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 335000000.0,
    "trainingComputeFlop": 8.3e+20,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-11"
  },
  {
    "model": "XLM-RoBERTa",
    "publicationDate": "2019-11-05",
    "domain": "Language",
    "task": "Named entity recognition (NER)",
    "parameters": 550000000.0,
    "trainingComputeFlop": 2.076e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 32 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Meta",
    "department": "Facebook AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-11"
  },
  {
    "model": "Base LM + kNN LM + Continuous Cache",
    "publicationDate": "2019-11-01",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 247000000.00000003,
    "trainingComputeFlop": 3.05292e+19,
    "confidence": "Likely",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Stanford University",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-11"
  },
  {
    "model": "AlphaStar",
    "publicationDate": "2019-10-30",
    "domain": "Games",
    "task": "StarCraft",
    "parameters": 139000000.0,
    "trainingComputeFlop": 1.0773400001e+23,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-10"
  },
  {
    "model": "BART-large",
    "publicationDate": "2019-10-29",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 406291456.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": "Facebook AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-10"
  },
  {
    "model": "T5-11B",
    "organization": "Google",
    "publicationDate": "2019-10-23",
    "domain": "Language",
    "task": "Text autocompletion",
    "parameters": 11000000000.0,
    "trainingComputeFlop": 3.3e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-10"
  },
  {
    "model": "T5-3B",
    "organization": "Google",
    "publicationDate": "2019-10-23",
    "domain": "Language",
    "task": "Text autocompletion",
    "parameters": 2800000000.0,
    "trainingComputeFlop": 9.0000000001e+21,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-10"
  },
  {
    "model": "M4-50B",
    "organization": "Google",
    "publicationDate": "2019-10-11",
    "domain": "Language",
    "task": "Translation",
    "parameters": 50000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Large (10B-100B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-10"
  },
  {
    "model": "DistilBERT",
    "organization": "Hugging Face",
    "publicationDate": "2019-10-02",
    "domain": "Language",
    "task": "Text autocompletion",
    "parameters": 66000000.0,
    "trainingComputeFlop": 1.24416e+19,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 16 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Hugging Face",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-10"
  },
  {
    "model": "AlphaX-1",
    "publicationDate": "2019-10-02",
    "domain": "Vision",
    "task": "Neural architecture search for computer vision",
    "parameters": 5400000.0,
    "trainingComputeFlop": 8.89344e+17,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA GeForce GTX 1080 Ti",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-10"
  },
  {
    "model": "ALBERT",
    "publicationDate": "2019-09-26",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 18000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Toyota Technological Institute at Chicago",
    "department": "Google Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-09"
  },
  {
    "model": "Adaptive Inputs + LayerDrop",
    "publicationDate": "2019-09-25",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 423000000.00000006,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-09"
  },
  {
    "model": "Megatron-LM (8.3B)",
    "organization": "NVIDIA",
    "publicationDate": "2019-09-17",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 8300000000.0,
    "trainingComputeFlop": 9.1e+21,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 32 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-09"
  },
  {
    "model": "Megatron-LM (1.2B)",
    "organization": "NVIDIA",
    "publicationDate": "2019-09-17",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 1200000000.0,
    "trainingComputeFlop": 1.13e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA Tesla V100 SXM3 32 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-09"
  },
  {
    "model": "Megatron-BERT",
    "organization": "NVIDIA",
    "publicationDate": "2019-09-17",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 3900000000.0,
    "trainingComputeFlop": 2.2e+22,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA Tesla V100S PCIe 32 GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-09"
  },
  {
    "model": "UDSMProt",
    "organization": "Fraunhofer Heinrich Hertz Institute",
    "publicationDate": "2019-09-04",
    "domain": "Biology",
    "task": "Proteins",
    "parameters": 28303800.0,
    "trainingComputeFlop": 6.37e+17,
    "confidence": "Likely",
    "organizationCategorization": "Research collective",
    "countryOfOrganization": "Germany",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Fraunhofer Heinrich Hertz Institute",
    "department": null,
    "countryRelationship": "NATO",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-09"
  },
  {
    "model": [
      "Mogrifier (d2",
      "MoS2",
      "MC) + dynamic eval"
    ],
    "publicationDate": "2019-09-04",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 35000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United Kingdom",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-09"
  },
  {
    "model": "trRosetta",
    "organization": [
      "Nankai University",
      "University of Washington",
      "Tianjin University",
      "Harvard University"
    ],
    "publicationDate": "2019-08-22",
    "domain": "Biology",
    "task": "Protein folding prediction",
    "parameters": null,
    "trainingComputeFlop": 3.8047968e+19,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "China",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA TITAN RTX",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Nankai University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2019,
    "yearMonth": "2019-08"
  },
  {
    "model": "EN^2AS with performance reward",
    "organization": [
      "Beijing Institute of Technology",
      "University of Technology Sydney",
      "Monash University"
    ],
    "publicationDate": "2019-07-22",
    "domain": "Language",
    "task": "Neural Architecture Search - NAS",
    "parameters": 23000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "China",
      "Australia"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Beijing Institute of Technology",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2019,
    "yearMonth": "2019-07"
  },
  {
    "model": "Pluribus",
    "publicationDate": "2019-07-11",
    "domain": "Games",
    "task": "Poker",
    "parameters": null,
    "trainingComputeFlop": 6.6e+16,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-07"
  },
  {
    "model": "BigBiGAN",
    "organization": "Google",
    "publicationDate": "2019-07-04",
    "domain": "Vision",
    "task": "Image completion",
    "parameters": 86000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-07"
  },
  {
    "model": "RoBERTa Large",
    "organization": [
      "Facebook",
      "University of Washington"
    ],
    "publicationDate": "2019-07-01",
    "domain": "Language",
    "task": "Question answering",
    "parameters": 355000000.0,
    "trainingComputeFlop": 8.5067e+21,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 32 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-07"
  },
  {
    "model": "Walking Minotaur robot",
    "publicationDate": "2019-06-19",
    "domain": "Robotics",
    "task": "Animal (human/non-human) imitation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "University of California (UC) Berkeley",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-06"
  },
  {
    "model": "LaNet-L (CIFAR-10)",
    "organization": [
      "Brown University",
      "Facebook"
    ],
    "publicationDate": "2019-06-17",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 44100000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Brown University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-06"
  },
  {
    "model": "PG-SWGAN",
    "organization": "ETH Zurich",
    "publicationDate": "2019-06-15",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "Switzerland",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "ETH Zurich",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2019,
    "yearMonth": "2019-06"
  },
  {
    "model": "FixRes ResNeXt-101 WSL",
    "publicationDate": "2019-06-14",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 829000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Meta",
    "department": "Facebook AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-06"
  },
  {
    "model": "Char-CNN-BiLSTM",
    "organization": "Capital One",
    "publicationDate": "2019-06-13",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Capital One",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-06"
  },
  {
    "model": "AWD-LSTM + MoS + Partial Shuffled",
    "organization": "University of Texas at Austin",
    "publicationDate": "2019-06-10",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 35000000.0,
    "trainingComputeFlop": 3.15e+17,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "University of Texas at Austin",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2019,
    "yearMonth": "2019-06"
  },
  {
    "model": "Transformer-XL Large + Phrase Induction",
    "organization": [
      "Massachusetts Institute of Technology (MIT)",
      "University of Illinois Urbana-Champaign (UIUC)"
    ],
    "publicationDate": "2019-06-04",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 256999999.99999997,
    "trainingComputeFlop": 3.7848651e+20,
    "confidence": "Speculative",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "Transformer-XL (257M)",
    "modelAccessibility": "Unreleased",
    "company": "Massachusetts Institute of Technology (MIT)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2019,
    "yearMonth": "2019-06"
  },
  {
    "model": "XLNet",
    "publicationDate": "2019-06-01",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 340000000.0,
    "trainingComputeFlop": 6.19e+21,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Carnegie Mellon University (CMU)",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-06"
  },
  {
    "model": "XLM",
    "organization": "Facebook",
    "publicationDate": "2019-06-01",
    "domain": "Language",
    "task": "Translation",
    "parameters": 665000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Meta",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-06"
  },
  {
    "model": "DLRM-2020",
    "publicationDate": "2019-05-31",
    "domain": "Recommendation",
    "task": "Recommender system",
    "parameters": 100000000000.0,
    "trainingComputeFlop": 4e+18,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Meta",
    "department": "Facebook AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Very Large (100B-1T)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-05"
  },
  {
    "model": "EfficientNet-L2",
    "organization": "Google",
    "publicationDate": "2019-05-28",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 480000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-05"
  },
  {
    "model": "CPC v2",
    "publicationDate": "2019-05-22",
    "domain": "Vision",
    "task": "Image completion",
    "parameters": 303000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United Kingdom",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-05"
  },
  {
    "model": "AWD-LSTM-DRILL + dynamic evaluation† (WT2)",
    "organization": "IDIAP",
    "publicationDate": "2019-05-14",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 34000000.0,
    "trainingComputeFlop": 4.08e+17,
    "confidence": "Likely",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "Switzerland",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "IDIAP",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2019,
    "yearMonth": "2019-05"
  },
  {
    "model": "ResNeXt-101 Billion-scale",
    "publicationDate": "2019-05-02",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 193000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Meta",
    "department": "Facebook AI",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-05"
  },
  {
    "model": "RaptorX-Contact",
    "organization": "Toyota Technological Institute at Chicago",
    "publicationDate": "2019-05-02",
    "domain": "Biology",
    "task": "Protein folding prediction",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Toyota Technological Institute at Chicago",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2019,
    "yearMonth": "2019-05"
  },
  {
    "model": "Neuro-Symbolic Concept Learner",
    "publicationDate": "2019-04-26",
    "domain": "Vision",
    "task": "Visual question answering",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "China",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Massachusetts Institute of Technology (MIT)",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-04"
  },
  {
    "model": "MuseNet",
    "organization": "OpenAI",
    "publicationDate": "2019-04-25",
    "domain": "Audio",
    "task": "Audio generation",
    "parameters": 2038431744.0,
    "trainingComputeFlop": 2.208301056e+20,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Significant use",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-04"
  },
  {
    "model": "BERT-Large-CAS (PTB+WT2+WT103)",
    "organization": "Amazon",
    "publicationDate": "2019-04-20",
    "domain": "Language",
    "task": "Neural Architecture Search - NAS",
    "parameters": 395000000.0,
    "trainingComputeFlop": 1.5405e+20,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Amazon",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-04"
  },
  {
    "model": "Transformer-XL + RMS dynamic eval",
    "organization": "University of Edinburgh",
    "publicationDate": "2019-04-17",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 256999999.99999997,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United Kingdom",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "University of Edinburgh",
    "department": null,
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2019,
    "yearMonth": "2019-04"
  },
  {
    "model": "WeNet (Penn Treebank)",
    "organization": "Amazon",
    "publicationDate": "2019-04-08",
    "domain": "Language",
    "task": "Neural Architecture Search - NAS",
    "parameters": 23000000.0,
    "trainingComputeFlop": 7.30000001e+17,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Amazon",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-04"
  },
  {
    "model": "True-Regularization+Finetune+Dynamic-Eval",
    "organization": [
      "Mobvoi",
      "Williams College"
    ],
    "publicationDate": "2019-04-08",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 7000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "China",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Mobvoi",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-04"
  },
  {
    "model": "Cross-lingual alignment",
    "organization": [
      "Tel Aviv University",
      "Massachusetts Institute of Technology (MIT)"
    ],
    "publicationDate": "2019-04-04",
    "domain": "Language",
    "task": "Translation",
    "parameters": null,
    "trainingComputeFlop": 2.56e+18,
    "confidence": "Speculative",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "Israel",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA GeForce GTX 1080 Ti",
    "baseModel": "ELMo",
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Tel Aviv University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2019,
    "yearMonth": "2019-04"
  },
  {
    "model": "SciBERT",
    "organization": "Allen Institute for AI",
    "publicationDate": "2019-03-26",
    "domain": "Language",
    "task": "Relation extraction",
    "parameters": 110000000.0,
    "trainingComputeFlop": 8.926848e+19,
    "confidence": "Confident",
    "organizationCategorization": "Research collective",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Allen Institute for AI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-03"
  },
  {
    "model": "NMT Transformer 437M",
    "organization": [
      "Google",
      "Bar-Ilan University"
    ],
    "publicationDate": "2019-02-28",
    "domain": "Language",
    "task": "Translation",
    "parameters": 437700000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United States",
      "Israel"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-02"
  },
  {
    "model": "KataGo",
    "organization": "Jane Street",
    "publicationDate": "2019-02-27",
    "domain": "Games",
    "task": "Go",
    "parameters": 2500000.0,
    "trainingComputeFlop": 2.32e+19,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 16 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Jane Street",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-02"
  },
  {
    "model": "GPT-2 (1.5B)",
    "organization": "OpenAI",
    "publicationDate": "2019-02-14",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 1500000000.0,
    "trainingComputeFlop": 1.920000000001e+21,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-02"
  },
  {
    "model": "SDE",
    "publicationDate": "2019-02-09",
    "domain": "Language",
    "task": "Translation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "Australia"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Carnegie Mellon University (CMU)",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-02"
  },
  {
    "model": "Hanabi 4 player",
    "publicationDate": "2019-02-01",
    "domain": "Games",
    "task": "Hanabi",
    "parameters": 764000.0,
    "trainingComputeFlop": 4.3e+18,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": [
      "United Kingdom",
      "United States"
    ],
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": [
      "DeepMind",
      "Google Brain"
    ],
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-02"
  },
  {
    "model": "MT-DNN",
    "organization": "Microsoft",
    "publicationDate": "2019-01-31",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 330000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2019,
    "yearMonth": "2019-01"
  },
  {
    "model": "Transformer-XL (257M)",
    "publicationDate": "2019-01-09",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 256999999.99999997,
    "trainingComputeFlop": 3.7832771e+20,
    "confidence": "Speculative",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Discretionary",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Carnegie Mellon University (CMU)",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-01"
  },
  {
    "model": "Transformer ELMo",
    "organization": [
      "Allen Institute for AI",
      "University of Washington"
    ],
    "publicationDate": "2019-01-01",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 56000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Research collective",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Allen Institute for AI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2019,
    "yearMonth": "2019-01"
  },
  {
    "model": "StyleGAN",
    "organization": "NVIDIA",
    "publicationDate": "2018-12-12",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 26200000.0,
    "trainingComputeFlop": 3.93e+16,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "NVIDIA",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-12"
  },
  {
    "model": "SPN (ImageNet 128)",
    "publicationDate": "2018-12-04",
    "domain": "Image generation",
    "task": "Image generation",
    "parameters": 250000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": [
      "United States",
      "United Kingdom"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v3",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": [
      "Google Brain",
      "DeepMind"
    ],
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-12"
  },
  {
    "model": "GPipe (Transformer)",
    "organization": "Google",
    "publicationDate": "2018-11-16",
    "domain": "Language",
    "task": "Translation",
    "parameters": 6000000000.0,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Unknown",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-11"
  },
  {
    "model": "Multi-cell LSTM",
    "organization": "University of Hyderabad",
    "publicationDate": "2018-11-15",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 7200000.0,
    "trainingComputeFlop": 2006640000000000.0,
    "confidence": "Likely",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "India",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "University of Hyderabad",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2018,
    "yearMonth": "2018-11"
  },
  {
    "model": "Fine-tuned-AWD-LSTM-DOC (fin)",
    "organization": "Samsung R&D Institute Russia",
    "publicationDate": "2018-11-12",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 46000000.0,
    "trainingComputeFlop": 5.188e+16,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "Russia",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": "AWD-LSTM-DOC (fin) (23M)",
    "modelAccessibility": "Unreleased",
    "company": "Samsung R&D Institute Russia",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-11"
  },
  {
    "model": "Mesh-TensorFlow Transformer 4.9B (language)",
    "publicationDate": "2018-11-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 4900000000.0,
    "trainingComputeFlop": 1.617408e+20,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v2",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-11"
  },
  {
    "model": "Mesh-TensorFlow Transformer 2.9B (translation)",
    "publicationDate": "2018-11-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 2900000000.0,
    "trainingComputeFlop": 6.84288e+19,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "Google TPU v2",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Medium (1B-10B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-11"
  },
  {
    "model": "MemoReader",
    "organization": [
      "Samsung",
      "Korea University"
    ],
    "publicationDate": "2018-10-31",
    "domain": "Language",
    "task": "Question answering",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "Korea (Republic of)",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA M40",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Samsung",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2018,
    "yearMonth": "2018-10"
  },
  {
    "model": "TrellisNet",
    "organization": [
      "Carnegie Mellon University (CMU)",
      "Bosch Center for Artificial Intelligence",
      "Intel Labs"
    ],
    "publicationDate": "2018-10-15",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 180000000.0,
    "trainingComputeFlop": 2.78e+18,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "United States",
      "Germany"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Carnegie Mellon University (CMU)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2018,
    "yearMonth": "2018-10"
  },
  {
    "model": "MetaMimic",
    "organization": "Google",
    "publicationDate": "2018-10-11",
    "domain": "Robotics",
    "task": "Robotic manipulation",
    "parameters": 22000000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Unknown",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-10"
  },
  {
    "model": "BERT-Large",
    "organization": "Google",
    "publicationDate": "2018-10-11",
    "domain": "Language",
    "task": "Question answering",
    "parameters": 340000000.0,
    "trainingComputeFlop": 2.85e+20,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "Google TPU v2",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-10"
  },
  {
    "model": "Transformer (Adaptive Input Embeddings) WT103",
    "publicationDate": "2018-09-28",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 247000000.0,
    "trainingComputeFlop": 4.47e+19,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": "Facebook AI Research",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-09"
  },
  {
    "model": "LSTM+NeuralCache",
    "organization": [
      "KU Leuven",
      "ESAT - PSI",
      "Apple"
    ],
    "publicationDate": "2018-09-24",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 2100000.0,
    "trainingComputeFlop": 982800000000000.0,
    "confidence": "Likely",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": [
      "Belgium",
      "United States"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "KU Leuven",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2018,
    "yearMonth": "2018-09"
  },
  {
    "model": [
      "AWD-LSTM-MoS + dynamic evaluation (WT2",
      "2018)"
    ],
    "organization": [
      "Peking University",
      "Microsoft Research Asia"
    ],
    "publicationDate": "2018-09-18",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 35000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "China",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Peking University",
    "department": null,
    "countryRelationship": "Adversaries",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2018,
    "yearMonth": "2018-09"
  },
  {
    "model": "Transformer + Simple Recurrent Unit",
    "organization": [
      "ASAPP",
      "Cornell University",
      "Google",
      "Princeton University"
    ],
    "publicationDate": "2018-09-17",
    "domain": "Language",
    "task": "Translation",
    "parameters": 90000000.0,
    "trainingComputeFlop": 1.1e+19,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "ASAPP",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2018,
    "yearMonth": "2018-09"
  },
  {
    "model": "NetSurfP-2.0",
    "organization": [
      "Technical University of Denmark",
      "University of Copenhagen",
      "Universidad Nacional de San Martín",
      "AIMST University"
    ],
    "publicationDate": "2018-09-10",
    "domain": "Biology",
    "task": "Protein folding prediction",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Academia",
    "countryOfOrganization": [
      "Denmark",
      "Argentina",
      "Malaysia"
    ],
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Hosted access (no API)",
    "company": "Technical University of Denmark",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (API)",
    "modelCapacity": "Unknown",
    "sector": "Public",
    "year": 2018,
    "yearMonth": "2018-09"
  },
  {
    "model": "(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2)",
    "organization": [
      "NTT Communication Science Laboratories",
      "Tohoku University"
    ],
    "publicationDate": "2018-08-30",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 185000000.0,
    "trainingComputeFlop": 6.66e+17,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "Japan",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "NTT Communication Science Laboratories",
    "department": null,
    "countryRelationship": "Close Allies & Partners",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2018,
    "yearMonth": "2018-08"
  },
  {
    "model": "Big Transformer for Back-Translation",
    "publicationDate": "2018-08-28",
    "domain": "Language",
    "task": "Translation",
    "parameters": null,
    "trainingComputeFlop": 4.7808e+20,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": [
      "United States",
      "France"
    ],
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA Tesla V100 DGXS 16 GB",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "Meta",
    "department": [
      "Facebook AI Research",
      "Google Brain"
    ],
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-08"
  },
  {
    "model": "AWD-LSTM-MoS+PDR + dynamic evaluation (WT2)",
    "organization": "IBM",
    "publicationDate": "2018-08-14",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 35000000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "IBM",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-08"
  },
  {
    "model": "Big-Little Net (speech)",
    "organization": "IBM",
    "publicationDate": "2018-07-10",
    "domain": "Speech",
    "task": "Speech recognition (ASR)",
    "parameters": 3320000.0,
    "trainingComputeFlop": 4.290048e+17,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "IBM",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-07"
  },
  {
    "model": "Big-Little Net",
    "organization": "IBM",
    "publicationDate": "2018-07-10",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 77360000.0,
    "trainingComputeFlop": 2.46048e+17,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Tesla K80",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "IBM",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-07"
  },
  {
    "model": "FTW (For The Win)",
    "publicationDate": "2018-07-03",
    "domain": "Games",
    "task": "Capture the flag",
    "parameters": 126001330.0,
    "trainingComputeFlop": 3.49e+19,
    "confidence": "Speculative",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-07"
  },
  {
    "model": "MobileNetV2",
    "organization": "Google",
    "publicationDate": "2018-06-18",
    "domain": "Vision",
    "task": "Object detection",
    "parameters": 3400000.0,
    "trainingComputeFlop": null,
    "confidence": null,
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Unknown",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-06"
  },
  {
    "model": "Relational Memory Core",
    "publicationDate": "2018-06-05",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Industry",
      "Academia"
    ],
    "countryOfOrganization": "United Kingdom",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Other",
    "year": 2018,
    "yearMonth": "2018-06"
  },
  {
    "model": "GPT-1",
    "organization": "OpenAI",
    "publicationDate": "2018-06-01",
    "domain": "Language",
    "task": "Question answering",
    "parameters": 117000000.0,
    "trainingComputeFlop": 1.7578125e+19,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA Quadro P600",
    "baseModel": null,
    "modelAccessibility": "Open weights (unrestricted)",
    "company": "OpenAI",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Unrestricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-06"
  },
  {
    "model": "aLSTM(depth-2)+RecurrentPolicy (WT2)",
    "organization": [
      "University of Manchester",
      "Alan Turing Institute"
    ],
    "publicationDate": "2018-05-22",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 32000000.0,
    "trainingComputeFlop": 7.296e+16,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Government"
    ],
    "countryOfOrganization": "United Kingdom",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "University of Manchester",
    "department": null,
    "countryRelationship": "FEYE",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2018,
    "yearMonth": "2018-05"
  },
  {
    "model": "Dropout-LSTM+Noise(Bernoulli) (WT2)",
    "organization": [
      "Columbia University",
      "New York University (NYU)",
      "Princeton University"
    ],
    "publicationDate": "2018-05-03",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 51000000.0,
    "trainingComputeFlop": 1.27e+17,
    "confidence": "Likely",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Columbia University",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2018,
    "yearMonth": "2018-05"
  },
  {
    "model": "ResNeXt-101 32x48d",
    "organization": "Facebook",
    "publicationDate": "2018-05-02",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 829000000.0,
    "trainingComputeFlop": 8.74395e+21,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Open weights (non-commercial)",
    "company": "Meta",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Open Source (Restricted)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-05"
  },
  {
    "model": "YOLOv3",
    "organization": "University of Washington",
    "publicationDate": "2018-04-08",
    "domain": "Vision",
    "task": "Object detection",
    "parameters": 56933216.0,
    "trainingComputeFlop": 1.3416380824e+19,
    "confidence": "Likely",
    "organizationCategorization": "Academia",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA M40",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "University of Washington",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Public",
    "year": 2018,
    "yearMonth": "2018-04"
  },
  {
    "model": "4 layer QRNN (h=2500)",
    "organization": "Salesforce Research",
    "publicationDate": "2018-03-22",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 151000000.0,
    "trainingComputeFlop": 5.9158815e+17,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA Quadro GP100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Salesforce Research",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-03"
  },
  {
    "model": "Chinese - English translation",
    "organization": "Microsoft",
    "publicationDate": "2018-03-01",
    "domain": "Language",
    "task": "Translation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": null,
    "company": "Microsoft",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Unknown",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-03"
  },
  {
    "model": "TCN (P-MNIST)",
    "organization": [
      "Carnegie Mellon University (CMU)",
      "Intel Labs"
    ],
    "publicationDate": "2018-02-15",
    "domain": "Vision",
    "task": "Image classification",
    "parameters": 42000.0,
    "trainingComputeFlop": null,
    "confidence": "Confident",
    "organizationCategorization": [
      "Academia",
      "Industry"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": null,
    "company": "Carnegie Mellon University (CMU)",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Unknown",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2018,
    "yearMonth": "2018-02"
  },
  {
    "model": "DeepLabV3+",
    "organization": "Google",
    "publicationDate": "2018-02-07",
    "domain": "Vision",
    "task": "Semantic segmentation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Unknown",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": null,
    "company": "Google",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Unknown",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-02"
  },
  {
    "model": "IMPALA",
    "publicationDate": "2018-02-05",
    "domain": "Games",
    "task": "Atari",
    "parameters": 1600000.0,
    "trainingComputeFlop": 1.68e+20,
    "confidence": "Confident",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": "NVIDIA P100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "DeepMind",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-02"
  },
  {
    "model": "QRNN",
    "organization": "Salesforce Research",
    "publicationDate": "2018-02-01",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": 135000000.0,
    "trainingComputeFlop": 6.8866472e+17,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "SOTA improvement",
    "trainingHardware": "NVIDIA V100",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Salesforce Research",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Small (<1B)",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-02"
  },
  {
    "model": "ELMo",
    "organization": [
      "University of Washington",
      "Allen Institute for AI"
    ],
    "publicationDate": "2018-02-01",
    "domain": "Language",
    "task": "Question answering",
    "parameters": 94000000.0,
    "trainingComputeFlop": 3300100000000010.0,
    "confidence": "Speculative",
    "organizationCategorization": [
      "Academia",
      "Research collective"
    ],
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Highly cited",
    "trainingHardware": null,
    "baseModel": null,
    "modelAccessibility": null,
    "company": "University of Washington",
    "department": null,
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Unknown",
    "modelCapacity": "Small (<1B)",
    "sector": "Other",
    "year": 2018,
    "yearMonth": "2018-02"
  },
  {
    "model": "T-DMCA",
    "publicationDate": "2018-01-30",
    "domain": "Language",
    "task": "Language modeling/generation",
    "parameters": null,
    "trainingComputeFlop": null,
    "confidence": "Likely",
    "organizationCategorization": "Industry",
    "countryOfOrganization": "United States",
    "notabilityCriteria": "Historical significance",
    "trainingHardware": "NVIDIA P100 PCIe 16GB",
    "baseModel": null,
    "modelAccessibility": "Unreleased",
    "company": "Google",
    "department": "Google Brain",
    "countryRelationship": "Private Partnership",
    "accessibilityCategory": "Closed Source (Unreleased)",
    "modelCapacity": "Unknown",
    "sector": "Private",
    "year": 2018,
    "yearMonth": "2018-01"
  }
]